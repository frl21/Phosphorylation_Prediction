{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot as mp\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.models import *\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import *\n",
    "from keras.layers import Conv2D, LSTM, Embedding, Bidirectional, Input, merge, multiply, concatenate, add, GlobalAveragePooling1D, Layer, TimeDistributed, Conv1D, Lambda, Add\n",
    "from keras.layers.core import Dense, Activation, Flatten, Dropout, Reshape\n",
    "from keras.callbacks import *\n",
    "from keras.initializers import *\n",
    "from keras.engine.topology import Layer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PELM Dataset, S positive shape:  (1554, 9)\n",
      "PELM Dataset, T positive shape:  (707, 9)\n",
      "PELM Dataset, Y positive shape:  (267, 9)\n",
      "PPA Dataset, S positive shape:  (307, 9)\n",
      "PPA Dataset, T positive shape:  (68, 9)\n",
      "PPA Dataset, Y positive shape:  (51, 9)\n",
      "\n",
      "PELM Dataset, S negative shape:  (1543, 9)\n",
      "PELM Dataset, T negative shape:  (453, 9)\n",
      "PELM Dataset, Y negative shape:  (226, 9)\n",
      "PPA Dataset, S negative shape:  (307, 9)\n",
      "PPA Dataset, T negative shape:  (68, 9)\n",
      "PPA Dataset, Y negative shape:  (51, 9)\n"
     ]
    }
   ],
   "source": [
    "# Read sample from Dataset\n",
    "\n",
    "with open('fixed_sequences_length_9_PELM/Group_Phos_S_pos.fasta', 'r') as f:\n",
    "    PELM_s_positif_txt = f.readlines()\n",
    "with open('fixed_sequences_length_9_PELM/Group_Phos_T_pos.fasta', 'r') as f:\n",
    "    PELM_t_positif_txt = f.readlines()\n",
    "with open('fixed_sequences_length_9_PELM/Group_Phos_Y_pos.fasta', 'r') as f:\n",
    "    PELM_y_positif_txt = f.readlines()\n",
    "with open('fixed_sequences_length_9_PPA/S_IDS_pos.fasta', 'r') as f:\n",
    "    PPA_s_positif_txt = f.readlines()\n",
    "with open('fixed_sequences_length_9_PPA/T_IDS_pos.fasta', 'r') as f:\n",
    "    PPA_t_positif_txt = f.readlines()\n",
    "with open('fixed_sequences_length_9_PPA/Y_IDS_pos.fasta', 'r') as f:\n",
    "    PPA_y_positif_txt = f.readlines()\n",
    "\n",
    "with open('fixed_sequences_length_9_PELM/Group_Phos_S_neg.fasta', 'r') as f:\n",
    "    PELM_s_negatif_txt = f.readlines()\n",
    "with open('fixed_sequences_length_9_PELM/Group_Phos_T_neg.fasta', 'r') as f:\n",
    "    PELM_t_negatif_txt = f.readlines()\n",
    "with open('fixed_sequences_length_9_PELM/Group_Phos_Y_neg.fasta', 'r') as f:\n",
    "    PELM_y_negatif_txt = f.readlines()\n",
    "with open('fixed_sequences_length_9_PPA/S_IDS_neg.fasta', 'r') as f:\n",
    "    PPA_s_negatif_txt = f.readlines()\n",
    "with open('fixed_sequences_length_9_PPA/T_IDS_neg.fasta', 'r') as f:\n",
    "    PPA_t_negatif_txt = f.readlines()\n",
    "with open('fixed_sequences_length_9_PPA/Y_IDS_neg.fasta', 'r') as f:\n",
    "    PPA_y_negatif_txt = f.readlines()\n",
    "\n",
    "# Pick the window 9\n",
    "\n",
    "PELM_s_positif = np.array([])\n",
    "for i in range(1,len(PELM_s_positif_txt),2):\n",
    "    temp = PELM_s_positif_txt[i]\n",
    "    temp1 = temp[0:9]\n",
    "    temp2 = list(temp1)\n",
    "    PELM_s_positif = np.append(PELM_s_positif, temp2)\n",
    "print('PELM Dataset, S positive shape: ', PELM_s_positif.reshape(int(len(PELM_s_positif)/9),9).shape)\n",
    "\n",
    "PELM_t_positif = np.array([])\n",
    "for i in range(1,len(PELM_t_positif_txt),2):\n",
    "    temp = PELM_t_positif_txt[i]\n",
    "    temp1 = temp[0:9]\n",
    "    temp2 = list(temp1)\n",
    "    PELM_t_positif = np.append(PELM_t_positif, temp2)\n",
    "print('PELM Dataset, T positive shape: ', PELM_t_positif.reshape(int(len(PELM_t_positif)/9),9).shape)\n",
    "    \n",
    "PELM_y_positif = np.array([])\n",
    "for i in range(1,len(PELM_y_positif_txt),2):\n",
    "    temp = PELM_y_positif_txt[i]\n",
    "    temp1 = temp[0:9]\n",
    "    temp2 = list(temp1)\n",
    "    PELM_y_positif = np.append(PELM_y_positif, temp2)\n",
    "print('PELM Dataset, Y positive shape: ', PELM_y_positif.reshape(int(len(PELM_y_positif)/9),9).shape)\n",
    "\n",
    "PPA_s_positif = np.array([])\n",
    "for i in range(1,len(PPA_s_positif_txt),2):\n",
    "    temp = PPA_s_positif_txt[i]\n",
    "    temp1 = temp[0:9]\n",
    "    temp2 = list(temp1)\n",
    "    PPA_s_positif = np.append(PPA_s_positif, temp2)\n",
    "print('PPA Dataset, S positive shape: ', PPA_s_positif.reshape(int(len(PPA_s_positif)/9),9).shape)\n",
    "\n",
    "PPA_t_positif = np.array([])\n",
    "for i in range(1,len(PPA_t_positif_txt),2):\n",
    "    temp = PPA_t_positif_txt[i]\n",
    "    temp1 = temp[0:9]\n",
    "    temp2 = list(temp1)\n",
    "    PPA_t_positif = np.append(PPA_t_positif, temp2)\n",
    "print('PPA Dataset, T positive shape: ', PPA_t_positif.reshape(int(len(PPA_t_positif)/9),9).shape)\n",
    "    \n",
    "PPA_y_positif = np.array([])\n",
    "for i in range(1,len(PPA_y_positif_txt),2):\n",
    "    temp = PPA_y_positif_txt[i]\n",
    "    temp1 = temp[0:9]\n",
    "    temp2 = list(temp1)\n",
    "    PPA_y_positif = np.append(PPA_y_positif, temp2)\n",
    "print('PPA Dataset, Y positive shape: ', PPA_y_positif.reshape(int(len(PPA_y_positif)/9),9).shape)\n",
    "\n",
    "print()\n",
    "\n",
    "PELM_s_negatif = np.array([])\n",
    "for i in range(1,len(PELM_s_negatif_txt),2):\n",
    "    temp = PELM_s_negatif_txt[i]\n",
    "    temp1 = temp[0:9]\n",
    "    temp2 = list(temp1)\n",
    "    PELM_s_negatif = np.append(PELM_s_negatif, temp2)\n",
    "print('PELM Dataset, S negative shape: ', PELM_s_negatif.reshape(int(len(PELM_s_negatif)/9),9).shape)\n",
    "\n",
    "PELM_t_negatif = np.array([])\n",
    "for i in range(1,len(PELM_t_negatif_txt),2):\n",
    "    temp = PELM_t_negatif_txt[i]\n",
    "    temp1 = temp[0:9]\n",
    "    temp2 = list(temp1)\n",
    "    PELM_t_negatif = np.append(PELM_t_negatif, temp2)\n",
    "print('PELM Dataset, T negative shape: ', PELM_t_negatif.reshape(int(len(PELM_t_negatif)/9),9).shape)\n",
    "    \n",
    "PELM_y_negatif = np.array([])\n",
    "for i in range(1,len(PELM_y_negatif_txt),2):\n",
    "    temp = PELM_y_negatif_txt[i]\n",
    "    temp1 = temp[0:9]\n",
    "    temp2 = list(temp1)\n",
    "    PELM_y_negatif = np.append(PELM_y_negatif, temp2)\n",
    "print('PELM Dataset, Y negative shape: ', PELM_y_negatif.reshape(int(len(PELM_y_negatif)/9),9).shape)\n",
    "\n",
    "PPA_s_negatif = np.array([])\n",
    "for i in range(1,len(PPA_s_negatif_txt),2):\n",
    "    temp = PPA_s_negatif_txt[i]\n",
    "    temp1 = temp[0:9]\n",
    "    temp2 = list(temp1)\n",
    "    PPA_s_negatif = np.append(PPA_s_negatif, temp2)\n",
    "print('PPA Dataset, S negative shape: ', PPA_s_negatif.reshape(int(len(PPA_s_negatif)/9),9).shape)\n",
    "\n",
    "PPA_t_negatif = np.array([])\n",
    "for i in range(1,len(PPA_t_negatif_txt),2):\n",
    "    temp = PPA_t_negatif_txt[i]\n",
    "    temp1 = temp[0:9]\n",
    "    temp2 = list(temp1)\n",
    "    PPA_t_negatif = np.append(PPA_t_negatif, temp2)\n",
    "print('PPA Dataset, T negative shape: ', PPA_t_negatif.reshape(int(len(PPA_t_negatif)/9),9).shape)\n",
    "    \n",
    "PPA_y_negatif = np.array([])\n",
    "for i in range(1,len(PPA_y_negatif_txt),2):\n",
    "    temp = PPA_y_negatif_txt[i]\n",
    "    temp1 = temp[0:9]\n",
    "    temp2 = list(temp1)\n",
    "    PPA_y_negatif = np.append(PPA_y_negatif, temp2)\n",
    "print('PPA Dataset, Y negative shape: ', PPA_y_negatif.reshape(int(len(PPA_y_negatif)/9),9).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Dataset shape:  (1554, 9)\n",
      "Positive Label shape:  (1554, 1)\n",
      "Negative Dataset shape:  (1543, 9)\n",
      "Negative Label shape:  (1543, 1)\n"
     ]
    }
   ],
   "source": [
    "# Choose Dataset to train, make sure correspond with negative dataset\n",
    "\n",
    "dataset_pos = PELM_s_positif\n",
    "dataset_neg = PELM_s_negatif\n",
    "string_name = 'PELM_s'\n",
    "\n",
    "# Expand dimension, Reshape and Create Label\n",
    "\n",
    "sequenceLP = int(len(dataset_pos)/9)\n",
    "dataset_pos = np.expand_dims(dataset_pos, axis=0)\n",
    "dataset_pos = dataset_pos.reshape(sequenceLP,9)\n",
    "label_pos = np.ones((sequenceLP,), dtype=int)\n",
    "label_pos = np.expand_dims(label_pos, axis=0)\n",
    "label_pos = label_pos.reshape(sequenceLP,1)\n",
    "\n",
    "sequenceLN = int(len(dataset_neg)/9)\n",
    "dataset_neg = np.expand_dims(dataset_neg, axis=0)\n",
    "dataset_neg = dataset_neg.reshape(sequenceLN,9)\n",
    "label_neg = np.zeros((sequenceLN,), dtype=int)\n",
    "label_neg = np.expand_dims(label_neg, axis=0)\n",
    "label_neg = label_neg.reshape(sequenceLN,1)\n",
    "\n",
    "# Validate\n",
    "\n",
    "print('Positive Dataset shape: ', dataset_pos.shape)\n",
    "print('Positive Label shape: ', label_pos.shape)\n",
    "print('Negative Dataset shape: ', dataset_neg.shape)\n",
    "print('Negative Label shape: ', label_neg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main X shape:  (3097, 9)\n",
      "main Y shape:  (3097, 2)\n",
      "train X shape:  (2477, 9)\n",
      "train Y shape:  (2477, 2)\n",
      "valid X shape:  (620, 9)\n",
      "valid Y shape:  (620, 2)\n"
     ]
    }
   ],
   "source": [
    "# Dataset preparation\n",
    "\n",
    "dataset_X = np.concatenate((dataset_pos, dataset_neg), axis=0, out=None)\n",
    "dataset_Y = np.concatenate((label_pos, label_neg), axis=0, out=None)\n",
    "\n",
    "# Tokenizing, Unique character got its own number\n",
    "\n",
    "asam = ['A','R','N','D','C','Q','E','G','H','I','L','K','M','F','P','S','T','W','Y','V']\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(asam)\n",
    "dataset_X_token = []\n",
    "for i in range(len(dataset_X)):\n",
    "    temp = tokenizer.texts_to_sequences(dataset_X[i])\n",
    "    dataset_X_token = np.append(dataset_X_token, temp)\n",
    "\n",
    "dataset_X_token = dataset_X_token-1\n",
    "dataset_X_token = dataset_X_token.reshape(len(dataset_X),9)\n",
    "\n",
    "# Onehot\n",
    "\n",
    "dataset_X_token_onehot = to_categorical(dataset_X_token)\n",
    "dataset_X_token_onehot = np.expand_dims(dataset_X_token_onehot, axis=3)\n",
    "dataset_X_token_onehot = dataset_X_token_onehot.reshape(len(dataset_X),9,20,1)\n",
    "\n",
    "dataset_Y_onehot = to_categorical(dataset_Y)\n",
    "\n",
    "# Shuffle Dataset, devide\n",
    "\n",
    "main_X, main_Y = shuffle(dataset_X_token, dataset_Y_onehot, random_state=13)\n",
    "train_X, valid_X, train_Y, valid_Y = train_test_split(dataset_X_token, dataset_Y_onehot, \n",
    "                                                              test_size=0.2, random_state=13)\n",
    "\n",
    "# Validation\n",
    "\n",
    "print('main X shape: ', main_X.shape)\n",
    "print('main Y shape: ', main_Y.shape)\n",
    "print('train X shape: ', train_X.shape)\n",
    "print('train Y shape: ', train_Y.shape)\n",
    "print('valid X shape: ', valid_X.shape)\n",
    "print('valid Y shape: ', valid_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 9, 8)         160         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 9)            648         embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 9, 1)         0           lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 9, 8)         0           reshape_1[0][0]                  \n",
      "                                                                 reshape_1[0][0]                  \n",
      "                                                                 reshape_1[0][0]                  \n",
      "                                                                 reshape_1[0][0]                  \n",
      "                                                                 reshape_1[0][0]                  \n",
      "                                                                 reshape_1[0][0]                  \n",
      "                                                                 reshape_1[0][0]                  \n",
      "                                                                 reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 9, 8)         0           embedding_1[0][0]                \n",
      "                                                                 concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 9, 20)        2320        multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 9, 20)        3280        lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   (None, 9, 20)        3280        lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   (None, 20)           3280        lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            42          lstm_5[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 13,010\n",
      "Trainable params: 13,010\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#LSTM\n",
    "\n",
    "epochs = 300\n",
    "inp = Input(shape=(9,))\n",
    "emb = Embedding(20, 8, input_length=9)(inp)\n",
    "\n",
    "att = LSTM(9, activation = 'softmax')(emb)\n",
    "att = Reshape(target_shape=(9,1))(att)\n",
    "att = concatenate([att,att,att,att,att,att,att,att], axis=2)\n",
    "emb = multiply([emb, att])\n",
    "\n",
    "i = LSTM(20, return_sequences=True)(emb)\n",
    "i = LSTM(20, return_sequences=True)(i)\n",
    "i = LSTM(20, return_sequences=True)(i)\n",
    "i = LSTM(20, return_sequences=False)(i)\n",
    "\n",
    "out = Dense(2, activation='softmax')(i)\n",
    "\n",
    "model_LSTM = Model(inputs=inp, outputs=out)\n",
    "model_LSTM.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "model_LSTM.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model LSTM\n",
    "\n",
    "model_LSTM_train = model_LSTM.fit(train_X, train_Y, epochs=epochs, batch_size=32, validation_data=(valid_X, valid_Y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training accuracy model LSTM\n",
    "\n",
    "accuracy = model_LSTM_train.history['acc']\n",
    "val_accuracy = model_LSTM_train.history['val_acc']\n",
    "loss = model_LSTM_train.history['loss']\n",
    "val_loss = model_LSTM_train.history['val_loss']\n",
    "epochs = range(len(accuracy))\n",
    "plt.plot(epochs, accuracy, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
    "plt.title('LSTM Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('LSTM Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "y_pred = np.argmax(model_LSTM.predict(valid_X), axis=1)\n",
    "y_true = np.argmax(valid_Y, axis = 1)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "f1 = f1_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
    "auc = roc_auc_score(y_true, y_pred, average='macro', sample_weight=None, max_fpr=None)\n",
    "sensi = tp/(tp+fn)\n",
    "specificity = tn/(tn+fp)\n",
    "accu = (tn + tp)/(tn + tp + fn + fp)\n",
    "mcc = ((tp*tn)-(fp*fn))/np.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n",
    "print('{} Result'.format(string_name))\n",
    "print('Accuracy :', accu)\n",
    "print('AUC :', auc)\n",
    "print('Sensitivity :', sensi)\n",
    "print('Specificity :', specificity)\n",
    "print('F1 :', f1)\n",
    "print('MCC :', mcc)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 9, 8)         160         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_31 (Bidirectional (None, 9)            1296        embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)             (None, 9, 1)         0           bidirectional_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 9, 8)         0           reshape_6[0][0]                  \n",
      "                                                                 reshape_6[0][0]                  \n",
      "                                                                 reshape_6[0][0]                  \n",
      "                                                                 reshape_6[0][0]                  \n",
      "                                                                 reshape_6[0][0]                  \n",
      "                                                                 reshape_6[0][0]                  \n",
      "                                                                 reshape_6[0][0]                  \n",
      "                                                                 reshape_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_6 (Multiply)           (None, 9, 8)         0           embedding_6[0][0]                \n",
      "                                                                 concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_32 (Bidirectional (None, 9, 64)        10496       multiply_6[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 9, 64)        0           bidirectional_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_33 (Bidirectional (None, 9, 64)        24832       dropout_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 9, 64)        0           bidirectional_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_34 (Bidirectional (None, 9, 64)        24832       dropout_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 9, 64)        0           bidirectional_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_35 (Bidirectional (None, 64)           24832       dropout_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 2)            130         bidirectional_35[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 86,578\n",
      "Trainable params: 86,578\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Bidirectional LSTM\n",
    "\n",
    "epochs = 200\n",
    "inp = Input(shape=(9,))\n",
    "emb = Embedding(20, 8, input_length=9)(inp)\n",
    "\n",
    "att = Bidirectional(LSTM(9, activation = 'softmax'), merge_mode='ave', weights=None)(emb)\n",
    "att = Reshape(target_shape=(9,1))(att)\n",
    "att = concatenate([att,att,att,att,att,att,att,att], axis=2)\n",
    "emb = multiply([emb, att])\n",
    "\n",
    "i = Bidirectional(LSTM(32, return_sequences=True), merge_mode='concat', weights=None)(emb)\n",
    "i = Dropout(0.35, noise_shape=None, seed=None)(i)\n",
    "i = Bidirectional(LSTM(32, return_sequences=True), merge_mode='concat', weights=None)(i)\n",
    "i = Dropout(0.35, noise_shape=None, seed=None)(i)\n",
    "i = Bidirectional(LSTM(32, return_sequences=True), merge_mode='concat', weights=None)(i)\n",
    "i = Dropout(0.35, noise_shape=None, seed=None)(i)\n",
    "i = Bidirectional(LSTM(32, return_sequences=False), merge_mode='concat', weights=None)(i)\n",
    "\n",
    "out = Dense(2, activation='softmax')(i)\n",
    "\n",
    "model_BLSTM = Model(inputs=inp, outputs=out)\n",
    "model_BLSTM.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "# Callback\n",
    "\n",
    "def step_decay(epoch):\n",
    "   if (0 <= epoch <= 90):\n",
    "    lrate = 1e-3\n",
    "   elif (90 < epoch <= 150):\n",
    "    lrate = 5e-4\n",
    "   elif (150 < epoch):\n",
    "    lrate = 1e-4\n",
    "\n",
    "   return lrate\n",
    "lrate = LearningRateScheduler(step_decay, verbose=1)\n",
    "callbacks_list = [lrate]\n",
    "\n",
    "model_BLSTM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2477 samples, validate on 620 samples\n",
      "Epoch 1/200\n",
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 19s 8ms/step - loss: 0.6928 - acc: 0.5067 - val_loss: 0.6843 - val_acc: 0.5548\n",
      "Epoch 2/200\n",
      "\n",
      "Epoch 00002: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.3928 - acc: 0.8236 - val_loss: 0.2560 - val_acc: 0.9048\n",
      "Epoch 3/200\n",
      "\n",
      "Epoch 00003: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.2959 - acc: 0.8740 - val_loss: 0.2740 - val_acc: 0.8968\n",
      "Epoch 4/200\n",
      "\n",
      "Epoch 00004: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.2631 - acc: 0.8934 - val_loss: 0.2424 - val_acc: 0.8968\n",
      "Epoch 5/200\n",
      "\n",
      "Epoch 00005: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.2642 - acc: 0.8906 - val_loss: 0.2470 - val_acc: 0.8952\n",
      "Epoch 6/200\n",
      "\n",
      "Epoch 00006: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.2577 - acc: 0.8934 - val_loss: 0.2402 - val_acc: 0.8984\n",
      "Epoch 7/200\n",
      "\n",
      "Epoch 00007: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.2546 - acc: 0.8987 - val_loss: 0.2498 - val_acc: 0.8952\n",
      "Epoch 8/200\n",
      "\n",
      "Epoch 00008: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.2592 - acc: 0.8922 - val_loss: 0.2396 - val_acc: 0.9032\n",
      "Epoch 9/200\n",
      "\n",
      "Epoch 00009: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.2491 - acc: 0.9039 - val_loss: 0.2395 - val_acc: 0.9016\n",
      "Epoch 10/200\n",
      "\n",
      "Epoch 00010: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.2506 - acc: 0.8983 - val_loss: 0.2390 - val_acc: 0.8984\n",
      "Epoch 11/200\n",
      "\n",
      "Epoch 00011: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.2513 - acc: 0.8950 - val_loss: 0.2497 - val_acc: 0.8919\n",
      "Epoch 12/200\n",
      "\n",
      "Epoch 00012: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.2476 - acc: 0.8995 - val_loss: 0.2570 - val_acc: 0.8984\n",
      "Epoch 13/200\n",
      "\n",
      "Epoch 00013: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.2456 - acc: 0.9031 - val_loss: 0.2467 - val_acc: 0.8919\n",
      "Epoch 14/200\n",
      "\n",
      "Epoch 00014: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.2493 - acc: 0.8979 - val_loss: 0.2543 - val_acc: 0.8984\n",
      "Epoch 15/200\n",
      "\n",
      "Epoch 00015: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.2454 - acc: 0.9007 - val_loss: 0.2393 - val_acc: 0.8919\n",
      "Epoch 16/200\n",
      "\n",
      "Epoch 00016: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.2505 - acc: 0.8962 - val_loss: 0.2461 - val_acc: 0.8952\n",
      "Epoch 17/200\n",
      "\n",
      "Epoch 00017: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.2415 - acc: 0.8999 - val_loss: 0.2428 - val_acc: 0.8984\n",
      "Epoch 18/200\n",
      "\n",
      "Epoch 00018: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.2435 - acc: 0.8995 - val_loss: 0.2453 - val_acc: 0.8984\n",
      "Epoch 19/200\n",
      "\n",
      "Epoch 00019: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.2473 - acc: 0.8979 - val_loss: 0.2431 - val_acc: 0.8968\n",
      "Epoch 20/200\n",
      "\n",
      "Epoch 00020: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.2427 - acc: 0.9015 - val_loss: 0.2382 - val_acc: 0.8887\n",
      "Epoch 21/200\n",
      "\n",
      "Epoch 00021: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.2380 - acc: 0.9027 - val_loss: 0.2423 - val_acc: 0.8903\n",
      "Epoch 22/200\n",
      "\n",
      "Epoch 00022: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.2405 - acc: 0.9031 - val_loss: 0.2399 - val_acc: 0.8935\n",
      "Epoch 23/200\n",
      "\n",
      "Epoch 00023: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.2365 - acc: 0.9007 - val_loss: 0.2429 - val_acc: 0.8887\n",
      "Epoch 24/200\n",
      "\n",
      "Epoch 00024: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.2327 - acc: 0.9092 - val_loss: 0.2441 - val_acc: 0.9016\n",
      "Epoch 25/200\n",
      "\n",
      "Epoch 00025: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.2410 - acc: 0.9039 - val_loss: 0.2610 - val_acc: 0.8919\n",
      "Epoch 26/200\n",
      "\n",
      "Epoch 00026: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.2366 - acc: 0.9047 - val_loss: 0.2426 - val_acc: 0.8935\n",
      "Epoch 27/200\n",
      "\n",
      "Epoch 00027: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.2362 - acc: 0.9023 - val_loss: 0.2438 - val_acc: 0.9000\n",
      "Epoch 28/200\n",
      "\n",
      "Epoch 00028: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.2417 - acc: 0.9047 - val_loss: 0.2411 - val_acc: 0.8935\n",
      "Epoch 29/200\n",
      "\n",
      "Epoch 00029: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 8s 3ms/step - loss: 0.2318 - acc: 0.9047 - val_loss: 0.2514 - val_acc: 0.8968\n",
      "Epoch 30/200\n",
      "\n",
      "Epoch 00030: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.2348 - acc: 0.9080 - val_loss: 0.2397 - val_acc: 0.8903\n",
      "Epoch 31/200\n",
      "\n",
      "Epoch 00031: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 8s 3ms/step - loss: 0.2295 - acc: 0.9031 - val_loss: 0.2534 - val_acc: 0.8935\n",
      "Epoch 32/200\n",
      "\n",
      "Epoch 00032: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.2281 - acc: 0.9108 - val_loss: 0.2741 - val_acc: 0.8823\n",
      "Epoch 33/200\n",
      "\n",
      "Epoch 00033: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.2358 - acc: 0.9011 - val_loss: 0.2704 - val_acc: 0.8855\n",
      "Epoch 34/200\n",
      "\n",
      "Epoch 00034: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.2386 - acc: 0.9023 - val_loss: 0.2607 - val_acc: 0.8919\n",
      "Epoch 35/200\n",
      "\n",
      "Epoch 00035: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.2333 - acc: 0.9063 - val_loss: 0.2420 - val_acc: 0.9016\n",
      "Epoch 36/200\n",
      "\n",
      "Epoch 00036: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 8s 3ms/step - loss: 0.2271 - acc: 0.9067 - val_loss: 0.2557 - val_acc: 0.8952\n",
      "Epoch 37/200\n",
      "\n",
      "Epoch 00037: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.2247 - acc: 0.9067 - val_loss: 0.2400 - val_acc: 0.9000\n",
      "Epoch 38/200\n",
      "\n",
      "Epoch 00038: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.2222 - acc: 0.9088 - val_loss: 0.2366 - val_acc: 0.8968\n",
      "Epoch 39/200\n",
      "\n",
      "Epoch 00039: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.2223 - acc: 0.9100 - val_loss: 0.2356 - val_acc: 0.9065\n",
      "Epoch 40/200\n",
      "\n",
      "Epoch 00040: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.2244 - acc: 0.9080 - val_loss: 0.2434 - val_acc: 0.8935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/200\n",
      "\n",
      "Epoch 00041: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.2230 - acc: 0.9096 - val_loss: 0.2364 - val_acc: 0.8968\n",
      "Epoch 42/200\n",
      "\n",
      "Epoch 00042: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.2215 - acc: 0.9080 - val_loss: 0.2441 - val_acc: 0.8919\n",
      "Epoch 43/200\n",
      "\n",
      "Epoch 00043: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.2191 - acc: 0.9096 - val_loss: 0.2382 - val_acc: 0.9081\n",
      "Epoch 44/200\n",
      "\n",
      "Epoch 00044: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.2185 - acc: 0.9112 - val_loss: 0.2425 - val_acc: 0.9000\n",
      "Epoch 45/200\n",
      "\n",
      "Epoch 00045: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.2192 - acc: 0.9108 - val_loss: 0.2433 - val_acc: 0.8952\n",
      "Epoch 46/200\n",
      "\n",
      "Epoch 00046: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.2156 - acc: 0.9160 - val_loss: 0.2477 - val_acc: 0.9032\n",
      "Epoch 47/200\n",
      "\n",
      "Epoch 00047: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.2187 - acc: 0.9120 - val_loss: 0.2359 - val_acc: 0.9065\n",
      "Epoch 48/200\n",
      "\n",
      "Epoch 00048: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.2192 - acc: 0.9100 - val_loss: 0.2386 - val_acc: 0.9016\n",
      "Epoch 49/200\n",
      "\n",
      "Epoch 00049: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.2137 - acc: 0.9144 - val_loss: 0.2429 - val_acc: 0.9016\n",
      "Epoch 50/200\n",
      "\n",
      "Epoch 00050: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.2145 - acc: 0.9136 - val_loss: 0.2341 - val_acc: 0.9032\n",
      "Epoch 51/200\n",
      "\n",
      "Epoch 00051: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.2133 - acc: 0.9156 - val_loss: 0.2369 - val_acc: 0.8984\n",
      "Epoch 52/200\n",
      "\n",
      "Epoch 00052: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.2075 - acc: 0.9168 - val_loss: 0.2423 - val_acc: 0.8968\n",
      "Epoch 53/200\n",
      "\n",
      "Epoch 00053: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.2055 - acc: 0.9209 - val_loss: 0.2798 - val_acc: 0.9048\n",
      "Epoch 54/200\n",
      "\n",
      "Epoch 00054: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.2116 - acc: 0.9172 - val_loss: 0.2298 - val_acc: 0.9145\n",
      "Epoch 55/200\n",
      "\n",
      "Epoch 00055: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.2051 - acc: 0.9225 - val_loss: 0.2300 - val_acc: 0.9113\n",
      "Epoch 56/200\n",
      "\n",
      "Epoch 00056: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.2067 - acc: 0.9209 - val_loss: 0.2340 - val_acc: 0.9032\n",
      "Epoch 57/200\n",
      "\n",
      "Epoch 00057: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 8s 3ms/step - loss: 0.2077 - acc: 0.9164 - val_loss: 0.2307 - val_acc: 0.9129\n",
      "Epoch 58/200\n",
      "\n",
      "Epoch 00058: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.2020 - acc: 0.9197 - val_loss: 0.2341 - val_acc: 0.9113\n",
      "Epoch 59/200\n",
      "\n",
      "Epoch 00059: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 8s 3ms/step - loss: 0.2083 - acc: 0.9197 - val_loss: 0.2324 - val_acc: 0.9113\n",
      "Epoch 60/200\n",
      "\n",
      "Epoch 00060: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.2044 - acc: 0.9201 - val_loss: 0.2285 - val_acc: 0.9113\n",
      "Epoch 61/200\n",
      "\n",
      "Epoch 00061: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1985 - acc: 0.9225 - val_loss: 0.2383 - val_acc: 0.9145\n",
      "Epoch 62/200\n",
      "\n",
      "Epoch 00062: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.2068 - acc: 0.9193 - val_loss: 0.2393 - val_acc: 0.9065\n",
      "Epoch 63/200\n",
      "\n",
      "Epoch 00063: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.2024 - acc: 0.9209 - val_loss: 0.2383 - val_acc: 0.9081\n",
      "Epoch 64/200\n",
      "\n",
      "Epoch 00064: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1982 - acc: 0.9193 - val_loss: 0.2363 - val_acc: 0.9145\n",
      "Epoch 65/200\n",
      "\n",
      "Epoch 00065: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.2000 - acc: 0.9229 - val_loss: 0.2432 - val_acc: 0.9129\n",
      "Epoch 66/200\n",
      "\n",
      "Epoch 00066: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1979 - acc: 0.9213 - val_loss: 0.2306 - val_acc: 0.9145\n",
      "Epoch 67/200\n",
      "\n",
      "Epoch 00067: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1948 - acc: 0.9253 - val_loss: 0.2307 - val_acc: 0.9113\n",
      "Epoch 68/200\n",
      "\n",
      "Epoch 00068: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1949 - acc: 0.9221 - val_loss: 0.2237 - val_acc: 0.9081\n",
      "Epoch 69/200\n",
      "\n",
      "Epoch 00069: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1993 - acc: 0.9164 - val_loss: 0.2323 - val_acc: 0.9065\n",
      "Epoch 70/200\n",
      "\n",
      "Epoch 00070: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1934 - acc: 0.9269 - val_loss: 0.2341 - val_acc: 0.9113\n",
      "Epoch 71/200\n",
      "\n",
      "Epoch 00071: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1939 - acc: 0.9217 - val_loss: 0.2347 - val_acc: 0.9145\n",
      "Epoch 72/200\n",
      "\n",
      "Epoch 00072: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1916 - acc: 0.9289 - val_loss: 0.2275 - val_acc: 0.9161\n",
      "Epoch 73/200\n",
      "\n",
      "Epoch 00073: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1866 - acc: 0.9265 - val_loss: 0.2388 - val_acc: 0.9113\n",
      "Epoch 74/200\n",
      "\n",
      "Epoch 00074: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 8s 3ms/step - loss: 0.1983 - acc: 0.9273 - val_loss: 0.2199 - val_acc: 0.9177\n",
      "Epoch 75/200\n",
      "\n",
      "Epoch 00075: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 8s 3ms/step - loss: 0.1950 - acc: 0.9233 - val_loss: 0.2201 - val_acc: 0.9210\n",
      "Epoch 76/200\n",
      "\n",
      "Epoch 00076: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1850 - acc: 0.9261 - val_loss: 0.2220 - val_acc: 0.9129\n",
      "Epoch 77/200\n",
      "\n",
      "Epoch 00077: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1873 - acc: 0.9253 - val_loss: 0.2211 - val_acc: 0.9113\n",
      "Epoch 78/200\n",
      "\n",
      "Epoch 00078: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1856 - acc: 0.9261 - val_loss: 0.2293 - val_acc: 0.9081\n",
      "Epoch 79/200\n",
      "\n",
      "Epoch 00079: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1877 - acc: 0.9249 - val_loss: 0.2274 - val_acc: 0.9129\n",
      "Epoch 80/200\n",
      "\n",
      "Epoch 00080: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1862 - acc: 0.9277 - val_loss: 0.2405 - val_acc: 0.9097\n",
      "Epoch 81/200\n",
      "\n",
      "Epoch 00081: LearningRateScheduler setting learning rate to 0.001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1874 - acc: 0.9269 - val_loss: 0.2285 - val_acc: 0.9177\n",
      "Epoch 82/200\n",
      "\n",
      "Epoch 00082: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1871 - acc: 0.9277 - val_loss: 0.2230 - val_acc: 0.9177\n",
      "Epoch 83/200\n",
      "\n",
      "Epoch 00083: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1790 - acc: 0.9330 - val_loss: 0.2234 - val_acc: 0.9161\n",
      "Epoch 84/200\n",
      "\n",
      "Epoch 00084: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1801 - acc: 0.9326 - val_loss: 0.2202 - val_acc: 0.9210\n",
      "Epoch 85/200\n",
      "\n",
      "Epoch 00085: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1852 - acc: 0.9298 - val_loss: 0.2166 - val_acc: 0.9177\n",
      "Epoch 86/200\n",
      "\n",
      "Epoch 00086: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1809 - acc: 0.9318 - val_loss: 0.2287 - val_acc: 0.9145\n",
      "Epoch 87/200\n",
      "\n",
      "Epoch 00087: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1761 - acc: 0.9306 - val_loss: 0.2283 - val_acc: 0.9194\n",
      "Epoch 88/200\n",
      "\n",
      "Epoch 00088: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1784 - acc: 0.9273 - val_loss: 0.2235 - val_acc: 0.9145\n",
      "Epoch 89/200\n",
      "\n",
      "Epoch 00089: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1774 - acc: 0.9310 - val_loss: 0.2261 - val_acc: 0.9177\n",
      "Epoch 90/200\n",
      "\n",
      "Epoch 00090: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1767 - acc: 0.9289 - val_loss: 0.2227 - val_acc: 0.9113\n",
      "Epoch 91/200\n",
      "\n",
      "Epoch 00091: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1786 - acc: 0.9294 - val_loss: 0.2379 - val_acc: 0.9145\n",
      "Epoch 92/200\n",
      "\n",
      "Epoch 00092: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1691 - acc: 0.9306 - val_loss: 0.2250 - val_acc: 0.9177\n",
      "Epoch 93/200\n",
      "\n",
      "Epoch 00093: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1682 - acc: 0.9338 - val_loss: 0.2313 - val_acc: 0.9145\n",
      "Epoch 94/200\n",
      "\n",
      "Epoch 00094: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1681 - acc: 0.9318 - val_loss: 0.2352 - val_acc: 0.9161\n",
      "Epoch 95/200\n",
      "\n",
      "Epoch 00095: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1657 - acc: 0.9346 - val_loss: 0.2298 - val_acc: 0.9177\n",
      "Epoch 96/200\n",
      "\n",
      "Epoch 00096: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1618 - acc: 0.9358 - val_loss: 0.2383 - val_acc: 0.9129\n",
      "Epoch 97/200\n",
      "\n",
      "Epoch 00097: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1691 - acc: 0.9358 - val_loss: 0.2319 - val_acc: 0.9145\n",
      "Epoch 98/200\n",
      "\n",
      "Epoch 00098: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1683 - acc: 0.9338 - val_loss: 0.2318 - val_acc: 0.9194\n",
      "Epoch 99/200\n",
      "\n",
      "Epoch 00099: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 8s 3ms/step - loss: 0.1647 - acc: 0.9342 - val_loss: 0.2264 - val_acc: 0.9210\n",
      "Epoch 100/200\n",
      "\n",
      "Epoch 00100: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1611 - acc: 0.9350 - val_loss: 0.2266 - val_acc: 0.9145\n",
      "Epoch 101/200\n",
      "\n",
      "Epoch 00101: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1648 - acc: 0.9342 - val_loss: 0.2374 - val_acc: 0.9194\n",
      "Epoch 102/200\n",
      "\n",
      "Epoch 00102: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1666 - acc: 0.9366 - val_loss: 0.2328 - val_acc: 0.9177\n",
      "Epoch 103/200\n",
      "\n",
      "Epoch 00103: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1667 - acc: 0.9318 - val_loss: 0.2275 - val_acc: 0.9145\n",
      "Epoch 104/200\n",
      "\n",
      "Epoch 00104: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1636 - acc: 0.9346 - val_loss: 0.2336 - val_acc: 0.9129\n",
      "Epoch 105/200\n",
      "\n",
      "Epoch 00105: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1604 - acc: 0.9342 - val_loss: 0.2278 - val_acc: 0.9210\n",
      "Epoch 106/200\n",
      "\n",
      "Epoch 00106: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1650 - acc: 0.9330 - val_loss: 0.2384 - val_acc: 0.9161\n",
      "Epoch 107/200\n",
      "\n",
      "Epoch 00107: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 8s 3ms/step - loss: 0.1616 - acc: 0.9354 - val_loss: 0.2493 - val_acc: 0.9177\n",
      "Epoch 108/200\n",
      "\n",
      "Epoch 00108: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1590 - acc: 0.9326 - val_loss: 0.2401 - val_acc: 0.9210\n",
      "Epoch 109/200\n",
      "\n",
      "Epoch 00109: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1586 - acc: 0.9346 - val_loss: 0.2310 - val_acc: 0.9145\n",
      "Epoch 110/200\n",
      "\n",
      "Epoch 00110: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1598 - acc: 0.9350 - val_loss: 0.2341 - val_acc: 0.9226\n",
      "Epoch 111/200\n",
      "\n",
      "Epoch 00111: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1617 - acc: 0.9338 - val_loss: 0.2337 - val_acc: 0.9161\n",
      "Epoch 112/200\n",
      "\n",
      "Epoch 00112: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1564 - acc: 0.9374 - val_loss: 0.2437 - val_acc: 0.9161\n",
      "Epoch 113/200\n",
      "\n",
      "Epoch 00113: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1578 - acc: 0.9362 - val_loss: 0.2462 - val_acc: 0.9065\n",
      "Epoch 114/200\n",
      "\n",
      "Epoch 00114: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1551 - acc: 0.9423 - val_loss: 0.2375 - val_acc: 0.9210\n",
      "Epoch 115/200\n",
      "\n",
      "Epoch 00115: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1546 - acc: 0.9407 - val_loss: 0.2424 - val_acc: 0.9129\n",
      "Epoch 116/200\n",
      "\n",
      "Epoch 00116: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1534 - acc: 0.9407 - val_loss: 0.2334 - val_acc: 0.9129\n",
      "Epoch 117/200\n",
      "\n",
      "Epoch 00117: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1522 - acc: 0.9394 - val_loss: 0.2479 - val_acc: 0.9129\n",
      "Epoch 118/200\n",
      "\n",
      "Epoch 00118: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1525 - acc: 0.9415 - val_loss: 0.2375 - val_acc: 0.9113\n",
      "Epoch 119/200\n",
      "\n",
      "Epoch 00119: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1644 - acc: 0.9318 - val_loss: 0.2311 - val_acc: 0.9161\n",
      "Epoch 120/200\n",
      "\n",
      "Epoch 00120: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1544 - acc: 0.9398 - val_loss: 0.2424 - val_acc: 0.9226\n",
      "Epoch 121/200\n",
      "\n",
      "Epoch 00121: LearningRateScheduler setting learning rate to 0.0005.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1538 - acc: 0.9407 - val_loss: 0.2414 - val_acc: 0.9113\n",
      "Epoch 122/200\n",
      "\n",
      "Epoch 00122: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1525 - acc: 0.9423 - val_loss: 0.2422 - val_acc: 0.9048\n",
      "Epoch 123/200\n",
      "\n",
      "Epoch 00123: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1516 - acc: 0.9411 - val_loss: 0.2455 - val_acc: 0.9081\n",
      "Epoch 124/200\n",
      "\n",
      "Epoch 00124: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1532 - acc: 0.9390 - val_loss: 0.2487 - val_acc: 0.9048\n",
      "Epoch 125/200\n",
      "\n",
      "Epoch 00125: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1571 - acc: 0.9378 - val_loss: 0.2467 - val_acc: 0.9097\n",
      "Epoch 126/200\n",
      "\n",
      "Epoch 00126: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1499 - acc: 0.9419 - val_loss: 0.2514 - val_acc: 0.9048\n",
      "Epoch 127/200\n",
      "\n",
      "Epoch 00127: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1445 - acc: 0.9455 - val_loss: 0.2443 - val_acc: 0.9129\n",
      "Epoch 128/200\n",
      "\n",
      "Epoch 00128: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1482 - acc: 0.9439 - val_loss: 0.2476 - val_acc: 0.9016\n",
      "Epoch 129/200\n",
      "\n",
      "Epoch 00129: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1537 - acc: 0.9390 - val_loss: 0.2553 - val_acc: 0.9048\n",
      "Epoch 130/200\n",
      "\n",
      "Epoch 00130: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1512 - acc: 0.9415 - val_loss: 0.2490 - val_acc: 0.9065\n",
      "Epoch 131/200\n",
      "\n",
      "Epoch 00131: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1456 - acc: 0.9455 - val_loss: 0.2466 - val_acc: 0.9113\n",
      "Epoch 132/200\n",
      "\n",
      "Epoch 00132: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1508 - acc: 0.9415 - val_loss: 0.2536 - val_acc: 0.9065\n",
      "Epoch 133/200\n",
      "\n",
      "Epoch 00133: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1544 - acc: 0.9378 - val_loss: 0.2499 - val_acc: 0.9129\n",
      "Epoch 134/200\n",
      "\n",
      "Epoch 00134: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1519 - acc: 0.9423 - val_loss: 0.2525 - val_acc: 0.9081\n",
      "Epoch 135/200\n",
      "\n",
      "Epoch 00135: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1424 - acc: 0.9427 - val_loss: 0.2490 - val_acc: 0.9065\n",
      "Epoch 136/200\n",
      "\n",
      "Epoch 00136: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1443 - acc: 0.9411 - val_loss: 0.2454 - val_acc: 0.9081\n",
      "Epoch 137/200\n",
      "\n",
      "Epoch 00137: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1430 - acc: 0.9459 - val_loss: 0.2620 - val_acc: 0.9065\n",
      "Epoch 138/200\n",
      "\n",
      "Epoch 00138: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1463 - acc: 0.9435 - val_loss: 0.2514 - val_acc: 0.9097\n",
      "Epoch 139/200\n",
      "\n",
      "Epoch 00139: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1455 - acc: 0.9451 - val_loss: 0.2437 - val_acc: 0.9177\n",
      "Epoch 140/200\n",
      "\n",
      "Epoch 00140: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1481 - acc: 0.9398 - val_loss: 0.2500 - val_acc: 0.9000\n",
      "Epoch 141/200\n",
      "\n",
      "Epoch 00141: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1413 - acc: 0.9463 - val_loss: 0.2559 - val_acc: 0.8984\n",
      "Epoch 142/200\n",
      "\n",
      "Epoch 00142: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1341 - acc: 0.9475 - val_loss: 0.2542 - val_acc: 0.9048\n",
      "Epoch 143/200\n",
      "\n",
      "Epoch 00143: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1442 - acc: 0.9419 - val_loss: 0.2534 - val_acc: 0.8968\n",
      "Epoch 144/200\n",
      "\n",
      "Epoch 00144: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1376 - acc: 0.9467 - val_loss: 0.2708 - val_acc: 0.9048\n",
      "Epoch 145/200\n",
      "\n",
      "Epoch 00145: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1406 - acc: 0.9475 - val_loss: 0.2572 - val_acc: 0.9065\n",
      "Epoch 146/200\n",
      "\n",
      "Epoch 00146: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1430 - acc: 0.9451 - val_loss: 0.2647 - val_acc: 0.9032\n",
      "Epoch 147/200\n",
      "\n",
      "Epoch 00147: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1373 - acc: 0.9447 - val_loss: 0.2537 - val_acc: 0.9016\n",
      "Epoch 148/200\n",
      "\n",
      "Epoch 00148: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1365 - acc: 0.9455 - val_loss: 0.2758 - val_acc: 0.9000\n",
      "Epoch 149/200\n",
      "\n",
      "Epoch 00149: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1369 - acc: 0.9447 - val_loss: 0.2644 - val_acc: 0.9048\n",
      "Epoch 150/200\n",
      "\n",
      "Epoch 00150: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1336 - acc: 0.9463 - val_loss: 0.2627 - val_acc: 0.9000\n",
      "Epoch 151/200\n",
      "\n",
      "Epoch 00151: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1313 - acc: 0.9512 - val_loss: 0.2661 - val_acc: 0.9016\n",
      "Epoch 152/200\n",
      "\n",
      "Epoch 00152: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1310 - acc: 0.9463 - val_loss: 0.2617 - val_acc: 0.8984\n",
      "Epoch 153/200\n",
      "\n",
      "Epoch 00153: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1258 - acc: 0.9520 - val_loss: 0.2652 - val_acc: 0.8968\n",
      "Epoch 154/200\n",
      "\n",
      "Epoch 00154: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1268 - acc: 0.9503 - val_loss: 0.2671 - val_acc: 0.9000\n",
      "Epoch 155/200\n",
      "\n",
      "Epoch 00155: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1227 - acc: 0.9524 - val_loss: 0.2661 - val_acc: 0.9000\n",
      "Epoch 156/200\n",
      "\n",
      "Epoch 00156: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 8s 3ms/step - loss: 0.1263 - acc: 0.9491 - val_loss: 0.2663 - val_acc: 0.9016\n",
      "Epoch 157/200\n",
      "\n",
      "Epoch 00157: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 8s 3ms/step - loss: 0.1231 - acc: 0.9536 - val_loss: 0.2644 - val_acc: 0.9048\n",
      "Epoch 158/200\n",
      "\n",
      "Epoch 00158: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 8s 3ms/step - loss: 0.1243 - acc: 0.9512 - val_loss: 0.2673 - val_acc: 0.9000\n",
      "Epoch 159/200\n",
      "\n",
      "Epoch 00159: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 8s 3ms/step - loss: 0.1191 - acc: 0.9516 - val_loss: 0.2716 - val_acc: 0.9048\n",
      "Epoch 160/200\n",
      "\n",
      "Epoch 00160: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1258 - acc: 0.9503 - val_loss: 0.2655 - val_acc: 0.9016\n",
      "Epoch 161/200\n",
      "\n",
      "Epoch 00161: LearningRateScheduler setting learning rate to 0.0001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1215 - acc: 0.9548 - val_loss: 0.2701 - val_acc: 0.9000\n",
      "Epoch 162/200\n",
      "\n",
      "Epoch 00162: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1243 - acc: 0.9520 - val_loss: 0.2691 - val_acc: 0.9032\n",
      "Epoch 163/200\n",
      "\n",
      "Epoch 00163: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1243 - acc: 0.9491 - val_loss: 0.2635 - val_acc: 0.9065\n",
      "Epoch 164/200\n",
      "\n",
      "Epoch 00164: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1251 - acc: 0.9512 - val_loss: 0.2687 - val_acc: 0.9032\n",
      "Epoch 165/200\n",
      "\n",
      "Epoch 00165: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1267 - acc: 0.9503 - val_loss: 0.2622 - val_acc: 0.9032\n",
      "Epoch 166/200\n",
      "\n",
      "Epoch 00166: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1217 - acc: 0.9528 - val_loss: 0.2707 - val_acc: 0.9000\n",
      "Epoch 167/200\n",
      "\n",
      "Epoch 00167: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1251 - acc: 0.9507 - val_loss: 0.2728 - val_acc: 0.8984\n",
      "Epoch 168/200\n",
      "\n",
      "Epoch 00168: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1216 - acc: 0.9532 - val_loss: 0.2711 - val_acc: 0.9048\n",
      "Epoch 169/200\n",
      "\n",
      "Epoch 00169: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1248 - acc: 0.9516 - val_loss: 0.2767 - val_acc: 0.8984\n",
      "Epoch 170/200\n",
      "\n",
      "Epoch 00170: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1201 - acc: 0.9532 - val_loss: 0.2732 - val_acc: 0.8984\n",
      "Epoch 171/200\n",
      "\n",
      "Epoch 00171: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1175 - acc: 0.9520 - val_loss: 0.2807 - val_acc: 0.9065\n",
      "Epoch 172/200\n",
      "\n",
      "Epoch 00172: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1185 - acc: 0.9552 - val_loss: 0.2694 - val_acc: 0.9032\n",
      "Epoch 173/200\n",
      "\n",
      "Epoch 00173: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1226 - acc: 0.9524 - val_loss: 0.2677 - val_acc: 0.9048\n",
      "Epoch 174/200\n",
      "\n",
      "Epoch 00174: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1228 - acc: 0.9507 - val_loss: 0.2674 - val_acc: 0.9032\n",
      "Epoch 175/200\n",
      "\n",
      "Epoch 00175: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1175 - acc: 0.9520 - val_loss: 0.2746 - val_acc: 0.9000\n",
      "Epoch 176/200\n",
      "\n",
      "Epoch 00176: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1184 - acc: 0.9560 - val_loss: 0.2731 - val_acc: 0.9016\n",
      "Epoch 177/200\n",
      "\n",
      "Epoch 00177: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1196 - acc: 0.9520 - val_loss: 0.2668 - val_acc: 0.9032\n",
      "Epoch 178/200\n",
      "\n",
      "Epoch 00178: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1211 - acc: 0.9495 - val_loss: 0.2702 - val_acc: 0.9065\n",
      "Epoch 179/200\n",
      "\n",
      "Epoch 00179: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1220 - acc: 0.9524 - val_loss: 0.2718 - val_acc: 0.9048\n",
      "Epoch 180/200\n",
      "\n",
      "Epoch 00180: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1189 - acc: 0.9503 - val_loss: 0.2727 - val_acc: 0.9032\n",
      "Epoch 181/200\n",
      "\n",
      "Epoch 00181: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1223 - acc: 0.9548 - val_loss: 0.2699 - val_acc: 0.9016\n",
      "Epoch 182/200\n",
      "\n",
      "Epoch 00182: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1200 - acc: 0.9512 - val_loss: 0.2706 - val_acc: 0.9032\n",
      "Epoch 183/200\n",
      "\n",
      "Epoch 00183: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1181 - acc: 0.9540 - val_loss: 0.2735 - val_acc: 0.9016\n",
      "Epoch 184/200\n",
      "\n",
      "Epoch 00184: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1229 - acc: 0.9507 - val_loss: 0.2724 - val_acc: 0.9016\n",
      "Epoch 185/200\n",
      "\n",
      "Epoch 00185: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1183 - acc: 0.9540 - val_loss: 0.2769 - val_acc: 0.9000\n",
      "Epoch 186/200\n",
      "\n",
      "Epoch 00186: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1171 - acc: 0.9532 - val_loss: 0.2819 - val_acc: 0.9016\n",
      "Epoch 187/200\n",
      "\n",
      "Epoch 00187: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1179 - acc: 0.9536 - val_loss: 0.2803 - val_acc: 0.9032\n",
      "Epoch 188/200\n",
      "\n",
      "Epoch 00188: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1188 - acc: 0.9520 - val_loss: 0.2784 - val_acc: 0.9000\n",
      "Epoch 189/200\n",
      "\n",
      "Epoch 00189: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1162 - acc: 0.9532 - val_loss: 0.2783 - val_acc: 0.8968\n",
      "Epoch 190/200\n",
      "\n",
      "Epoch 00190: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1222 - acc: 0.9540 - val_loss: 0.2801 - val_acc: 0.9032\n",
      "Epoch 191/200\n",
      "\n",
      "Epoch 00191: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1163 - acc: 0.9532 - val_loss: 0.2826 - val_acc: 0.9016\n",
      "Epoch 192/200\n",
      "\n",
      "Epoch 00192: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1205 - acc: 0.9528 - val_loss: 0.2802 - val_acc: 0.9016\n",
      "Epoch 193/200\n",
      "\n",
      "Epoch 00193: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1127 - acc: 0.9556 - val_loss: 0.2835 - val_acc: 0.9032\n",
      "Epoch 194/200\n",
      "\n",
      "Epoch 00194: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1176 - acc: 0.9520 - val_loss: 0.2747 - val_acc: 0.9016\n",
      "Epoch 195/200\n",
      "\n",
      "Epoch 00195: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1206 - acc: 0.9564 - val_loss: 0.2770 - val_acc: 0.9048\n",
      "Epoch 196/200\n",
      "\n",
      "Epoch 00196: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1168 - acc: 0.9544 - val_loss: 0.2833 - val_acc: 0.9032\n",
      "Epoch 197/200\n",
      "\n",
      "Epoch 00197: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1142 - acc: 0.9556 - val_loss: 0.2810 - val_acc: 0.9000\n",
      "Epoch 198/200\n",
      "\n",
      "Epoch 00198: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1189 - acc: 0.9520 - val_loss: 0.2793 - val_acc: 0.9032\n",
      "Epoch 199/200\n",
      "\n",
      "Epoch 00199: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1143 - acc: 0.9536 - val_loss: 0.2795 - val_acc: 0.9016\n",
      "Epoch 200/200\n",
      "\n",
      "Epoch 00200: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 7s 3ms/step - loss: 0.1150 - acc: 0.9552 - val_loss: 0.2790 - val_acc: 0.9032\n"
     ]
    }
   ],
   "source": [
    "# Train model Bi LSTM\n",
    "\n",
    "model_BLSTM_train = model_BLSTM.fit(train_X, train_Y, epochs=epochs, batch_size=32, validation_data=(valid_X, valid_Y), \n",
    "   callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PELM_s Result\n",
      "Accuracy : 0.9032258064516129\n",
      "AUC : 0.9027054066447122\n",
      "Sensitivity : 0.9161490683229814\n",
      "Specificity : 0.889261744966443\n",
      "F1 : 0.9076923076923078\n",
      "MCC : 0.8061672966195442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Plot the training accuracy model Bi LSTM\n",
    "\n",
    "accuracy = model_BLSTM_train.history['acc']\n",
    "val_accuracy = model_BLSTM_train.history['val_acc']\n",
    "loss = model_BLSTM_train.history['loss']\n",
    "val_loss = model_BLSTM_train.history['val_loss']\n",
    "epochs = range(len(accuracy))\n",
    "plt.plot(epochs, accuracy, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
    "plt.title('BSTM Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('BLSTM Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "y_pred = np.argmax(model_BLSTM.predict(valid_X), axis=1)\n",
    "y_true = np.argmax(valid_Y, axis = 1)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "f1 = f1_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
    "auc = roc_auc_score(y_true, y_pred, average='macro', sample_weight=None, max_fpr=None)\n",
    "sensi = tp/(tp+fn)\n",
    "specificity = tn/(tn+fp)\n",
    "accu = (tn + tp)/(tn + tp + fn + fp)\n",
    "mcc = ((tp*tn)-(fp*fn))/np.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n",
    "print('{} Result'.format(string_name))\n",
    "print('Accuracy :', accu)\n",
    "print('AUC :', auc)\n",
    "print('Sensitivity :', sensi)\n",
    "print('Specificity :', specificity)\n",
    "print('F1 :', f1)\n",
    "print('MCC :', mcc)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dense\n",
    "\n",
    "epochs = 300\n",
    "inp = Input(shape=(9,))\n",
    "emb = Embedding(20, 8, input_length=9)(inp)\n",
    "\n",
    "pos_emb = Embedding(20, 8, trainable=False, weights=[GetPosEncodingMatrix(20, 8)], input_length=9)(inp)\n",
    "emb = add([emb, pos_emb])\n",
    "att = Dense(1, activation = 'softmax')(emb)\n",
    "att = concatenate([att,att,att,att,att,att,att,att], axis=2)\n",
    "emb = multiply([emb, att])\n",
    "\n",
    "i = Dense(32)(emb)\n",
    "i = Dense(32)(i)\n",
    "i = Dense(32)(i)\n",
    "i = Flatten()(i)\n",
    "\n",
    "out = Dense(2, activation='softmax')(i)\n",
    "\n",
    "model_Dense = Model(inputs=inp, outputs=out)\n",
    "model_Dense.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "# Callback\n",
    "\n",
    "def step_decay(epoch):\n",
    "   if (0 <= epoch <= 90):\n",
    "    lrate = 1e-3\n",
    "   elif (90 < epoch <= 150):\n",
    "    lrate = 5e-4\n",
    "   elif (150 < epoch):\n",
    "    lrate = 1e-4\n",
    "\n",
    "   return lrate\n",
    "lrate = LearningRateScheduler(step_decay, verbose=1)\n",
    "callbacks_list = [lrate]\n",
    "\n",
    "model_Dense.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2477 samples, validate on 620 samples\n",
      "Epoch 1/300\n",
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 2s 689us/step - loss: 0.6848 - acc: 0.6233 - val_loss: 0.5657 - val_acc: 0.7065\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 00002: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 148us/step - loss: 0.5513 - acc: 0.7174 - val_loss: 0.4101 - val_acc: 0.8468\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 00003: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 147us/step - loss: 0.3952 - acc: 0.8333 - val_loss: 0.2765 - val_acc: 0.8871\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 00004: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 150us/step - loss: 0.3184 - acc: 0.8623 - val_loss: 0.2726 - val_acc: 0.8855\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 00005: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 150us/step - loss: 0.3073 - acc: 0.8736 - val_loss: 0.2572 - val_acc: 0.8919\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 00006: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 150us/step - loss: 0.2912 - acc: 0.8849 - val_loss: 0.2402 - val_acc: 0.8984\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 00007: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 155us/step - loss: 0.2817 - acc: 0.8837 - val_loss: 0.2342 - val_acc: 0.8984\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 00008: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 151us/step - loss: 0.2787 - acc: 0.8886 - val_loss: 0.2468 - val_acc: 0.9000\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 00009: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 152us/step - loss: 0.2734 - acc: 0.8910 - val_loss: 0.2448 - val_acc: 0.9032\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 00010: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 149us/step - loss: 0.2751 - acc: 0.8890 - val_loss: 0.2381 - val_acc: 0.8968\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 00011: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 153us/step - loss: 0.2679 - acc: 0.8954 - val_loss: 0.2306 - val_acc: 0.9032\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 00012: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 155us/step - loss: 0.2716 - acc: 0.8942 - val_loss: 0.2223 - val_acc: 0.9113\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 00013: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 152us/step - loss: 0.2632 - acc: 0.8934 - val_loss: 0.2387 - val_acc: 0.9016\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 00014: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 152us/step - loss: 0.2588 - acc: 0.8950 - val_loss: 0.2224 - val_acc: 0.9177\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 00015: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 156us/step - loss: 0.2602 - acc: 0.8983 - val_loss: 0.2238 - val_acc: 0.9081\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 00016: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 172us/step - loss: 0.2488 - acc: 0.8987 - val_loss: 0.2234 - val_acc: 0.9129\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 00017: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 168us/step - loss: 0.2455 - acc: 0.9027 - val_loss: 0.2410 - val_acc: 0.9016\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 00018: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 174us/step - loss: 0.2510 - acc: 0.8999 - val_loss: 0.2116 - val_acc: 0.9129\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 00019: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 174us/step - loss: 0.2412 - acc: 0.9108 - val_loss: 0.2029 - val_acc: 0.9290\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 00020: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 171us/step - loss: 0.2463 - acc: 0.9031 - val_loss: 0.2071 - val_acc: 0.9242\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 00021: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 156us/step - loss: 0.2580 - acc: 0.8950 - val_loss: 0.2198 - val_acc: 0.9097\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 00022: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 164us/step - loss: 0.2480 - acc: 0.9075 - val_loss: 0.2102 - val_acc: 0.9161\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 00023: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 157us/step - loss: 0.2576 - acc: 0.8942 - val_loss: 0.2209 - val_acc: 0.9226\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 00024: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 165us/step - loss: 0.2436 - acc: 0.8995 - val_loss: 0.2072 - val_acc: 0.9145\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 00025: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 166us/step - loss: 0.2451 - acc: 0.8991 - val_loss: 0.2137 - val_acc: 0.9210\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 00026: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 160us/step - loss: 0.2498 - acc: 0.8946 - val_loss: 0.2054 - val_acc: 0.9258\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 00027: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 167us/step - loss: 0.2495 - acc: 0.9011 - val_loss: 0.2095 - val_acc: 0.9258\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 00028: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 169us/step - loss: 0.2476 - acc: 0.9007 - val_loss: 0.2133 - val_acc: 0.9258\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 00029: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 156us/step - loss: 0.2366 - acc: 0.9051 - val_loss: 0.2103 - val_acc: 0.9258\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 00030: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 165us/step - loss: 0.2492 - acc: 0.8975 - val_loss: 0.2069 - val_acc: 0.9226\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 00031: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 144us/step - loss: 0.2400 - acc: 0.9084 - val_loss: 0.2167 - val_acc: 0.9161\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 00032: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 147us/step - loss: 0.2436 - acc: 0.9003 - val_loss: 0.2053 - val_acc: 0.9177\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 00033: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 151us/step - loss: 0.2575 - acc: 0.8958 - val_loss: 0.2017 - val_acc: 0.9226\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 00034: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 163us/step - loss: 0.2453 - acc: 0.9019 - val_loss: 0.2155 - val_acc: 0.9242\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 00035: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 166us/step - loss: 0.2540 - acc: 0.8983 - val_loss: 0.2160 - val_acc: 0.9210\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 00036: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 149us/step - loss: 0.2409 - acc: 0.9019 - val_loss: 0.2096 - val_acc: 0.9226\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 00037: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 169us/step - loss: 0.2338 - acc: 0.9092 - val_loss: 0.2276 - val_acc: 0.9258\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 00038: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 151us/step - loss: 0.2399 - acc: 0.9096 - val_loss: 0.2208 - val_acc: 0.9194\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 00039: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 163us/step - loss: 0.2453 - acc: 0.9055 - val_loss: 0.2070 - val_acc: 0.9258\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 00040: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 165us/step - loss: 0.2404 - acc: 0.9071 - val_loss: 0.2027 - val_acc: 0.9258\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 00041: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 167us/step - loss: 0.2337 - acc: 0.9071 - val_loss: 0.2161 - val_acc: 0.9210\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 00042: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 163us/step - loss: 0.2420 - acc: 0.9035 - val_loss: 0.2101 - val_acc: 0.9226\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 00043: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 156us/step - loss: 0.2428 - acc: 0.9092 - val_loss: 0.2102 - val_acc: 0.9177\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 00044: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 169us/step - loss: 0.2401 - acc: 0.9096 - val_loss: 0.2226 - val_acc: 0.9226\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 00045: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 153us/step - loss: 0.2396 - acc: 0.9059 - val_loss: 0.2213 - val_acc: 0.9129\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 00046: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 167us/step - loss: 0.2424 - acc: 0.8999 - val_loss: 0.2264 - val_acc: 0.9177\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 00047: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 156us/step - loss: 0.2393 - acc: 0.9071 - val_loss: 0.2028 - val_acc: 0.9194\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 00048: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 169us/step - loss: 0.2465 - acc: 0.9019 - val_loss: 0.2184 - val_acc: 0.9226\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 00049: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 156us/step - loss: 0.2265 - acc: 0.9096 - val_loss: 0.2124 - val_acc: 0.9210\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 00050: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 163us/step - loss: 0.2455 - acc: 0.9035 - val_loss: 0.1993 - val_acc: 0.9258\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 00051: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 148us/step - loss: 0.2361 - acc: 0.9108 - val_loss: 0.2167 - val_acc: 0.9226\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 00052: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 148us/step - loss: 0.2383 - acc: 0.9043 - val_loss: 0.2101 - val_acc: 0.9226\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 00053: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 151us/step - loss: 0.2432 - acc: 0.9011 - val_loss: 0.2098 - val_acc: 0.9145\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 00054: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 149us/step - loss: 0.2331 - acc: 0.9096 - val_loss: 0.2045 - val_acc: 0.9097\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 00055: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 149us/step - loss: 0.2362 - acc: 0.9132 - val_loss: 0.2096 - val_acc: 0.9194\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 00056: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 163us/step - loss: 0.2428 - acc: 0.9027 - val_loss: 0.2443 - val_acc: 0.9129\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 00057: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 161us/step - loss: 0.2428 - acc: 0.9063 - val_loss: 0.2108 - val_acc: 0.9242\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 00058: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 156us/step - loss: 0.2414 - acc: 0.9027 - val_loss: 0.2085 - val_acc: 0.9226\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 00059: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 156us/step - loss: 0.2361 - acc: 0.9092 - val_loss: 0.2007 - val_acc: 0.9226\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 00060: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 151us/step - loss: 0.2394 - acc: 0.9047 - val_loss: 0.2154 - val_acc: 0.9274\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 00061: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 153us/step - loss: 0.2291 - acc: 0.9080 - val_loss: 0.2297 - val_acc: 0.9129\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 00062: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 149us/step - loss: 0.2397 - acc: 0.9039 - val_loss: 0.2021 - val_acc: 0.9226\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 00063: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 147us/step - loss: 0.2386 - acc: 0.9027 - val_loss: 0.2031 - val_acc: 0.9242\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 00064: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 149us/step - loss: 0.2341 - acc: 0.9124 - val_loss: 0.2157 - val_acc: 0.9242\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 00065: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 150us/step - loss: 0.2393 - acc: 0.9055 - val_loss: 0.2124 - val_acc: 0.9210\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 00066: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 151us/step - loss: 0.2418 - acc: 0.9055 - val_loss: 0.2120 - val_acc: 0.9210\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 00067: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 151us/step - loss: 0.2334 - acc: 0.9100 - val_loss: 0.2093 - val_acc: 0.9194\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 00068: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 152us/step - loss: 0.2437 - acc: 0.9027 - val_loss: 0.2094 - val_acc: 0.9242\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 00069: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 149us/step - loss: 0.2357 - acc: 0.9104 - val_loss: 0.2072 - val_acc: 0.9194\n",
      "Epoch 70/300\n",
      "\n",
      "Epoch 00070: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 152us/step - loss: 0.2326 - acc: 0.9047 - val_loss: 0.2059 - val_acc: 0.9177\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 00071: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 148us/step - loss: 0.2330 - acc: 0.9140 - val_loss: 0.2055 - val_acc: 0.9161\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 00072: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 151us/step - loss: 0.2343 - acc: 0.9104 - val_loss: 0.2115 - val_acc: 0.9161\n",
      "Epoch 73/300\n",
      "\n",
      "Epoch 00073: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 153us/step - loss: 0.2403 - acc: 0.9088 - val_loss: 0.2065 - val_acc: 0.9258\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 00074: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 151us/step - loss: 0.2425 - acc: 0.9059 - val_loss: 0.2085 - val_acc: 0.9242\n",
      "Epoch 75/300\n",
      "\n",
      "Epoch 00075: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 148us/step - loss: 0.2341 - acc: 0.9096 - val_loss: 0.2129 - val_acc: 0.9226\n",
      "Epoch 76/300\n",
      "\n",
      "Epoch 00076: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 149us/step - loss: 0.2417 - acc: 0.9035 - val_loss: 0.2156 - val_acc: 0.9210\n",
      "Epoch 77/300\n",
      "\n",
      "Epoch 00077: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 147us/step - loss: 0.2382 - acc: 0.9043 - val_loss: 0.2124 - val_acc: 0.9194\n",
      "Epoch 78/300\n",
      "\n",
      "Epoch 00078: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 150us/step - loss: 0.2384 - acc: 0.9031 - val_loss: 0.2052 - val_acc: 0.9226\n",
      "Epoch 79/300\n",
      "\n",
      "Epoch 00079: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 152us/step - loss: 0.2393 - acc: 0.9100 - val_loss: 0.2258 - val_acc: 0.9194\n",
      "Epoch 80/300\n",
      "\n",
      "Epoch 00080: LearningRateScheduler setting learning rate to 0.001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2477/2477 [==============================] - 0s 148us/step - loss: 0.2430 - acc: 0.9055 - val_loss: 0.2154 - val_acc: 0.9145\n",
      "Epoch 81/300\n",
      "\n",
      "Epoch 00081: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 148us/step - loss: 0.2320 - acc: 0.9055 - val_loss: 0.2113 - val_acc: 0.9210\n",
      "Epoch 82/300\n",
      "\n",
      "Epoch 00082: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 147us/step - loss: 0.2294 - acc: 0.9100 - val_loss: 0.2135 - val_acc: 0.9226\n",
      "Epoch 83/300\n",
      "\n",
      "Epoch 00083: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 147us/step - loss: 0.2428 - acc: 0.9051 - val_loss: 0.2032 - val_acc: 0.9258\n",
      "Epoch 84/300\n",
      "\n",
      "Epoch 00084: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 149us/step - loss: 0.2445 - acc: 0.9080 - val_loss: 0.2076 - val_acc: 0.9226\n",
      "Epoch 85/300\n",
      "\n",
      "Epoch 00085: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 153us/step - loss: 0.2303 - acc: 0.9108 - val_loss: 0.2060 - val_acc: 0.9177\n",
      "Epoch 86/300\n",
      "\n",
      "Epoch 00086: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 157us/step - loss: 0.2426 - acc: 0.9011 - val_loss: 0.2120 - val_acc: 0.9194\n",
      "Epoch 87/300\n",
      "\n",
      "Epoch 00087: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 169us/step - loss: 0.2264 - acc: 0.9059 - val_loss: 0.2137 - val_acc: 0.9161\n",
      "Epoch 88/300\n",
      "\n",
      "Epoch 00088: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 162us/step - loss: 0.2458 - acc: 0.9031 - val_loss: 0.2352 - val_acc: 0.9194\n",
      "Epoch 89/300\n",
      "\n",
      "Epoch 00089: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 152us/step - loss: 0.2348 - acc: 0.9172 - val_loss: 0.2376 - val_acc: 0.9032\n",
      "Epoch 90/300\n",
      "\n",
      "Epoch 00090: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 150us/step - loss: 0.2319 - acc: 0.9059 - val_loss: 0.2126 - val_acc: 0.9161\n",
      "Epoch 91/300\n",
      "\n",
      "Epoch 00091: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 0s 150us/step - loss: 0.2375 - acc: 0.9096 - val_loss: 0.2072 - val_acc: 0.9210\n",
      "Epoch 92/300\n",
      "\n",
      "Epoch 00092: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 169us/step - loss: 0.2365 - acc: 0.9075 - val_loss: 0.1995 - val_acc: 0.9194\n",
      "Epoch 93/300\n",
      "\n",
      "Epoch 00093: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 163us/step - loss: 0.2303 - acc: 0.9100 - val_loss: 0.2110 - val_acc: 0.9194\n",
      "Epoch 94/300\n",
      "\n",
      "Epoch 00094: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 149us/step - loss: 0.2215 - acc: 0.9140 - val_loss: 0.2032 - val_acc: 0.9226\n",
      "Epoch 95/300\n",
      "\n",
      "Epoch 00095: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 150us/step - loss: 0.2241 - acc: 0.9164 - val_loss: 0.2022 - val_acc: 0.9226\n",
      "Epoch 96/300\n",
      "\n",
      "Epoch 00096: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 168us/step - loss: 0.2252 - acc: 0.9136 - val_loss: 0.2082 - val_acc: 0.9210\n",
      "Epoch 97/300\n",
      "\n",
      "Epoch 00097: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 153us/step - loss: 0.2231 - acc: 0.9156 - val_loss: 0.2057 - val_acc: 0.9194\n",
      "Epoch 98/300\n",
      "\n",
      "Epoch 00098: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 149us/step - loss: 0.2311 - acc: 0.9092 - val_loss: 0.2090 - val_acc: 0.9194\n",
      "Epoch 99/300\n",
      "\n",
      "Epoch 00099: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 152us/step - loss: 0.2272 - acc: 0.9051 - val_loss: 0.2207 - val_acc: 0.9210\n",
      "Epoch 100/300\n",
      "\n",
      "Epoch 00100: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 154us/step - loss: 0.2234 - acc: 0.9104 - val_loss: 0.2140 - val_acc: 0.9210\n",
      "Epoch 101/300\n",
      "\n",
      "Epoch 00101: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 156us/step - loss: 0.2334 - acc: 0.9075 - val_loss: 0.2054 - val_acc: 0.9194\n",
      "Epoch 102/300\n",
      "\n",
      "Epoch 00102: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 156us/step - loss: 0.2262 - acc: 0.9100 - val_loss: 0.2037 - val_acc: 0.9242\n",
      "Epoch 103/300\n",
      "\n",
      "Epoch 00103: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 154us/step - loss: 0.2315 - acc: 0.9092 - val_loss: 0.2058 - val_acc: 0.9210\n",
      "Epoch 104/300\n",
      "\n",
      "Epoch 00104: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 150us/step - loss: 0.2320 - acc: 0.9112 - val_loss: 0.2043 - val_acc: 0.9242\n",
      "Epoch 105/300\n",
      "\n",
      "Epoch 00105: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 151us/step - loss: 0.2343 - acc: 0.9039 - val_loss: 0.2071 - val_acc: 0.9242\n",
      "Epoch 106/300\n",
      "\n",
      "Epoch 00106: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 148us/step - loss: 0.2264 - acc: 0.9096 - val_loss: 0.2045 - val_acc: 0.9274\n",
      "Epoch 107/300\n",
      "\n",
      "Epoch 00107: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 148us/step - loss: 0.2265 - acc: 0.9084 - val_loss: 0.2023 - val_acc: 0.9194\n",
      "Epoch 108/300\n",
      "\n",
      "Epoch 00108: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 148us/step - loss: 0.2260 - acc: 0.9084 - val_loss: 0.1998 - val_acc: 0.9242\n",
      "Epoch 109/300\n",
      "\n",
      "Epoch 00109: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 147us/step - loss: 0.2223 - acc: 0.9148 - val_loss: 0.2067 - val_acc: 0.9226\n",
      "Epoch 110/300\n",
      "\n",
      "Epoch 00110: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 148us/step - loss: 0.2242 - acc: 0.9148 - val_loss: 0.2073 - val_acc: 0.9258\n",
      "Epoch 111/300\n",
      "\n",
      "Epoch 00111: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 149us/step - loss: 0.2263 - acc: 0.9152 - val_loss: 0.2092 - val_acc: 0.9161\n",
      "Epoch 112/300\n",
      "\n",
      "Epoch 00112: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 151us/step - loss: 0.2269 - acc: 0.9124 - val_loss: 0.2064 - val_acc: 0.9226\n",
      "Epoch 113/300\n",
      "\n",
      "Epoch 00113: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 150us/step - loss: 0.2335 - acc: 0.9148 - val_loss: 0.2041 - val_acc: 0.9242\n",
      "Epoch 114/300\n",
      "\n",
      "Epoch 00114: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 148us/step - loss: 0.2232 - acc: 0.9080 - val_loss: 0.2121 - val_acc: 0.9210\n",
      "Epoch 115/300\n",
      "\n",
      "Epoch 00115: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 150us/step - loss: 0.2223 - acc: 0.9132 - val_loss: 0.2028 - val_acc: 0.9226\n",
      "Epoch 116/300\n",
      "\n",
      "Epoch 00116: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 149us/step - loss: 0.2275 - acc: 0.9104 - val_loss: 0.2023 - val_acc: 0.9194\n",
      "Epoch 117/300\n",
      "\n",
      "Epoch 00117: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 149us/step - loss: 0.2351 - acc: 0.9148 - val_loss: 0.2048 - val_acc: 0.9226\n",
      "Epoch 118/300\n",
      "\n",
      "Epoch 00118: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 151us/step - loss: 0.2234 - acc: 0.9144 - val_loss: 0.2069 - val_acc: 0.9226\n",
      "Epoch 119/300\n",
      "\n",
      "Epoch 00119: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 156us/step - loss: 0.2241 - acc: 0.9124 - val_loss: 0.2087 - val_acc: 0.9194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/300\n",
      "\n",
      "Epoch 00120: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 147us/step - loss: 0.2270 - acc: 0.9140 - val_loss: 0.2073 - val_acc: 0.9210\n",
      "Epoch 121/300\n",
      "\n",
      "Epoch 00121: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 150us/step - loss: 0.2245 - acc: 0.9080 - val_loss: 0.2067 - val_acc: 0.9242\n",
      "Epoch 122/300\n",
      "\n",
      "Epoch 00122: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 157us/step - loss: 0.2317 - acc: 0.9080 - val_loss: 0.2168 - val_acc: 0.9161\n",
      "Epoch 123/300\n",
      "\n",
      "Epoch 00123: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 142us/step - loss: 0.2271 - acc: 0.9148 - val_loss: 0.2054 - val_acc: 0.9226\n",
      "Epoch 124/300\n",
      "\n",
      "Epoch 00124: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 144us/step - loss: 0.2189 - acc: 0.9148 - val_loss: 0.2069 - val_acc: 0.9177\n",
      "Epoch 125/300\n",
      "\n",
      "Epoch 00125: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 144us/step - loss: 0.2259 - acc: 0.9116 - val_loss: 0.2050 - val_acc: 0.9129\n",
      "Epoch 126/300\n",
      "\n",
      "Epoch 00126: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 148us/step - loss: 0.2256 - acc: 0.9180 - val_loss: 0.2118 - val_acc: 0.9210\n",
      "Epoch 127/300\n",
      "\n",
      "Epoch 00127: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 143us/step - loss: 0.2250 - acc: 0.9136 - val_loss: 0.2080 - val_acc: 0.9226\n",
      "Epoch 128/300\n",
      "\n",
      "Epoch 00128: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 150us/step - loss: 0.2276 - acc: 0.9112 - val_loss: 0.2039 - val_acc: 0.9194\n",
      "Epoch 129/300\n",
      "\n",
      "Epoch 00129: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 142us/step - loss: 0.2203 - acc: 0.9156 - val_loss: 0.2094 - val_acc: 0.9226\n",
      "Epoch 130/300\n",
      "\n",
      "Epoch 00130: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 146us/step - loss: 0.2259 - acc: 0.9136 - val_loss: 0.2116 - val_acc: 0.9177\n",
      "Epoch 131/300\n",
      "\n",
      "Epoch 00131: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 148us/step - loss: 0.2196 - acc: 0.9124 - val_loss: 0.2061 - val_acc: 0.9242\n",
      "Epoch 132/300\n",
      "\n",
      "Epoch 00132: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 146us/step - loss: 0.2308 - acc: 0.9144 - val_loss: 0.2061 - val_acc: 0.9226\n",
      "Epoch 133/300\n",
      "\n",
      "Epoch 00133: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 144us/step - loss: 0.2142 - acc: 0.9241 - val_loss: 0.2073 - val_acc: 0.9226\n",
      "Epoch 134/300\n",
      "\n",
      "Epoch 00134: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 151us/step - loss: 0.2225 - acc: 0.9100 - val_loss: 0.2058 - val_acc: 0.9177\n",
      "Epoch 135/300\n",
      "\n",
      "Epoch 00135: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 154us/step - loss: 0.2212 - acc: 0.9088 - val_loss: 0.2060 - val_acc: 0.9177\n",
      "Epoch 136/300\n",
      "\n",
      "Epoch 00136: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 164us/step - loss: 0.2233 - acc: 0.9140 - val_loss: 0.2144 - val_acc: 0.9226\n",
      "Epoch 137/300\n",
      "\n",
      "Epoch 00137: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 158us/step - loss: 0.2263 - acc: 0.9124 - val_loss: 0.2074 - val_acc: 0.9210\n",
      "Epoch 138/300\n",
      "\n",
      "Epoch 00138: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 152us/step - loss: 0.2210 - acc: 0.9128 - val_loss: 0.2049 - val_acc: 0.9226\n",
      "Epoch 139/300\n",
      "\n",
      "Epoch 00139: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 152us/step - loss: 0.2257 - acc: 0.9100 - val_loss: 0.2135 - val_acc: 0.9177\n",
      "Epoch 140/300\n",
      "\n",
      "Epoch 00140: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 150us/step - loss: 0.2251 - acc: 0.9124 - val_loss: 0.2036 - val_acc: 0.9177\n",
      "Epoch 141/300\n",
      "\n",
      "Epoch 00141: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 148us/step - loss: 0.2331 - acc: 0.9084 - val_loss: 0.2081 - val_acc: 0.9161\n",
      "Epoch 142/300\n",
      "\n",
      "Epoch 00142: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 149us/step - loss: 0.2210 - acc: 0.9140 - val_loss: 0.2034 - val_acc: 0.9226\n",
      "Epoch 143/300\n",
      "\n",
      "Epoch 00143: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 153us/step - loss: 0.2301 - acc: 0.9116 - val_loss: 0.2047 - val_acc: 0.9177\n",
      "Epoch 144/300\n",
      "\n",
      "Epoch 00144: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 148us/step - loss: 0.2224 - acc: 0.9148 - val_loss: 0.2071 - val_acc: 0.9194\n",
      "Epoch 145/300\n",
      "\n",
      "Epoch 00145: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 156us/step - loss: 0.2273 - acc: 0.9092 - val_loss: 0.2095 - val_acc: 0.9226\n",
      "Epoch 146/300\n",
      "\n",
      "Epoch 00146: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 157us/step - loss: 0.2223 - acc: 0.9148 - val_loss: 0.2010 - val_acc: 0.9194\n",
      "Epoch 147/300\n",
      "\n",
      "Epoch 00147: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 152us/step - loss: 0.2264 - acc: 0.9112 - val_loss: 0.2036 - val_acc: 0.9194\n",
      "Epoch 148/300\n",
      "\n",
      "Epoch 00148: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 154us/step - loss: 0.2278 - acc: 0.9189 - val_loss: 0.2050 - val_acc: 0.9226\n",
      "Epoch 149/300\n",
      "\n",
      "Epoch 00149: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 151us/step - loss: 0.2244 - acc: 0.9071 - val_loss: 0.2055 - val_acc: 0.9242\n",
      "Epoch 150/300\n",
      "\n",
      "Epoch 00150: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 150us/step - loss: 0.2265 - acc: 0.9088 - val_loss: 0.2047 - val_acc: 0.9242\n",
      "Epoch 151/300\n",
      "\n",
      "Epoch 00151: LearningRateScheduler setting learning rate to 0.0005.\n",
      "2477/2477 [==============================] - 0s 149us/step - loss: 0.2304 - acc: 0.9112 - val_loss: 0.2128 - val_acc: 0.9161\n",
      "Epoch 152/300\n",
      "\n",
      "Epoch 00152: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 149us/step - loss: 0.2177 - acc: 0.9164 - val_loss: 0.2051 - val_acc: 0.9177\n",
      "Epoch 153/300\n",
      "\n",
      "Epoch 00153: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 150us/step - loss: 0.2146 - acc: 0.9168 - val_loss: 0.2029 - val_acc: 0.9177\n",
      "Epoch 154/300\n",
      "\n",
      "Epoch 00154: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 149us/step - loss: 0.2175 - acc: 0.9156 - val_loss: 0.2040 - val_acc: 0.9210\n",
      "Epoch 155/300\n",
      "\n",
      "Epoch 00155: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 151us/step - loss: 0.2258 - acc: 0.9108 - val_loss: 0.2037 - val_acc: 0.9177\n",
      "Epoch 156/300\n",
      "\n",
      "Epoch 00156: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 151us/step - loss: 0.2187 - acc: 0.9144 - val_loss: 0.2027 - val_acc: 0.9145\n",
      "Epoch 157/300\n",
      "\n",
      "Epoch 00157: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 149us/step - loss: 0.2288 - acc: 0.9140 - val_loss: 0.2056 - val_acc: 0.9210\n",
      "Epoch 158/300\n",
      "\n",
      "Epoch 00158: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 150us/step - loss: 0.2138 - acc: 0.9160 - val_loss: 0.2031 - val_acc: 0.9194\n",
      "Epoch 159/300\n",
      "\n",
      "Epoch 00159: LearningRateScheduler setting learning rate to 0.0001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2477/2477 [==============================] - 0s 146us/step - loss: 0.2157 - acc: 0.9152 - val_loss: 0.2039 - val_acc: 0.9210\n",
      "Epoch 160/300\n",
      "\n",
      "Epoch 00160: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 144us/step - loss: 0.2105 - acc: 0.9225 - val_loss: 0.2019 - val_acc: 0.9194\n",
      "Epoch 161/300\n",
      "\n",
      "Epoch 00161: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 145us/step - loss: 0.2188 - acc: 0.9156 - val_loss: 0.2028 - val_acc: 0.9194\n",
      "Epoch 162/300\n",
      "\n",
      "Epoch 00162: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 174us/step - loss: 0.2114 - acc: 0.9152 - val_loss: 0.2023 - val_acc: 0.9177\n",
      "Epoch 163/300\n",
      "\n",
      "Epoch 00163: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 173us/step - loss: 0.2169 - acc: 0.9176 - val_loss: 0.2025 - val_acc: 0.9194\n",
      "Epoch 164/300\n",
      "\n",
      "Epoch 00164: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 169us/step - loss: 0.2092 - acc: 0.9201 - val_loss: 0.2033 - val_acc: 0.9194\n",
      "Epoch 165/300\n",
      "\n",
      "Epoch 00165: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 148us/step - loss: 0.2251 - acc: 0.9152 - val_loss: 0.2027 - val_acc: 0.9210\n",
      "Epoch 166/300\n",
      "\n",
      "Epoch 00166: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 148us/step - loss: 0.2184 - acc: 0.9164 - val_loss: 0.2035 - val_acc: 0.9210\n",
      "Epoch 167/300\n",
      "\n",
      "Epoch 00167: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 153us/step - loss: 0.2202 - acc: 0.9168 - val_loss: 0.2035 - val_acc: 0.9194\n",
      "Epoch 168/300\n",
      "\n",
      "Epoch 00168: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 150us/step - loss: 0.2167 - acc: 0.9172 - val_loss: 0.2035 - val_acc: 0.9177\n",
      "Epoch 169/300\n",
      "\n",
      "Epoch 00169: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 150us/step - loss: 0.2167 - acc: 0.9168 - val_loss: 0.2042 - val_acc: 0.9226\n",
      "Epoch 170/300\n",
      "\n",
      "Epoch 00170: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 149us/step - loss: 0.2124 - acc: 0.9176 - val_loss: 0.2044 - val_acc: 0.9210\n",
      "Epoch 171/300\n",
      "\n",
      "Epoch 00171: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 148us/step - loss: 0.2203 - acc: 0.9148 - val_loss: 0.2038 - val_acc: 0.9226\n",
      "Epoch 172/300\n",
      "\n",
      "Epoch 00172: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 149us/step - loss: 0.2151 - acc: 0.9156 - val_loss: 0.2036 - val_acc: 0.9210\n",
      "Epoch 173/300\n",
      "\n",
      "Epoch 00173: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 155us/step - loss: 0.2117 - acc: 0.9184 - val_loss: 0.2034 - val_acc: 0.9210\n",
      "Epoch 174/300\n",
      "\n",
      "Epoch 00174: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 152us/step - loss: 0.2188 - acc: 0.9193 - val_loss: 0.2029 - val_acc: 0.9210\n",
      "Epoch 175/300\n",
      "\n",
      "Epoch 00175: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 149us/step - loss: 0.2136 - acc: 0.9160 - val_loss: 0.2034 - val_acc: 0.9210\n",
      "Epoch 176/300\n",
      "\n",
      "Epoch 00176: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 149us/step - loss: 0.2145 - acc: 0.9144 - val_loss: 0.2036 - val_acc: 0.9226\n",
      "Epoch 177/300\n",
      "\n",
      "Epoch 00177: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 151us/step - loss: 0.2222 - acc: 0.9140 - val_loss: 0.2079 - val_acc: 0.9210\n",
      "Epoch 178/300\n",
      "\n",
      "Epoch 00178: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 152us/step - loss: 0.2123 - acc: 0.9201 - val_loss: 0.2045 - val_acc: 0.9226\n",
      "Epoch 179/300\n",
      "\n",
      "Epoch 00179: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 172us/step - loss: 0.2166 - acc: 0.9184 - val_loss: 0.2041 - val_acc: 0.9226\n",
      "Epoch 180/300\n",
      "\n",
      "Epoch 00180: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 178us/step - loss: 0.2237 - acc: 0.9100 - val_loss: 0.2030 - val_acc: 0.9242\n",
      "Epoch 181/300\n",
      "\n",
      "Epoch 00181: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 174us/step - loss: 0.2160 - acc: 0.9132 - val_loss: 0.2037 - val_acc: 0.9226\n",
      "Epoch 182/300\n",
      "\n",
      "Epoch 00182: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 165us/step - loss: 0.2211 - acc: 0.9112 - val_loss: 0.2028 - val_acc: 0.9194\n",
      "Epoch 183/300\n",
      "\n",
      "Epoch 00183: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 166us/step - loss: 0.2109 - acc: 0.9209 - val_loss: 0.2028 - val_acc: 0.9210\n",
      "Epoch 184/300\n",
      "\n",
      "Epoch 00184: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 166us/step - loss: 0.2083 - acc: 0.9217 - val_loss: 0.2033 - val_acc: 0.9210\n",
      "Epoch 185/300\n",
      "\n",
      "Epoch 00185: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 148us/step - loss: 0.2226 - acc: 0.9160 - val_loss: 0.2044 - val_acc: 0.9210\n",
      "Epoch 186/300\n",
      "\n",
      "Epoch 00186: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 163us/step - loss: 0.2238 - acc: 0.9120 - val_loss: 0.2034 - val_acc: 0.9210\n",
      "Epoch 187/300\n",
      "\n",
      "Epoch 00187: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 149us/step - loss: 0.2104 - acc: 0.9209 - val_loss: 0.2038 - val_acc: 0.9210\n",
      "Epoch 188/300\n",
      "\n",
      "Epoch 00188: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 147us/step - loss: 0.2170 - acc: 0.9152 - val_loss: 0.2033 - val_acc: 0.9258\n",
      "Epoch 189/300\n",
      "\n",
      "Epoch 00189: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 149us/step - loss: 0.2166 - acc: 0.9136 - val_loss: 0.2043 - val_acc: 0.9210\n",
      "Epoch 190/300\n",
      "\n",
      "Epoch 00190: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 146us/step - loss: 0.2282 - acc: 0.9116 - val_loss: 0.2036 - val_acc: 0.9226\n",
      "Epoch 191/300\n",
      "\n",
      "Epoch 00191: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 145us/step - loss: 0.2173 - acc: 0.9136 - val_loss: 0.2036 - val_acc: 0.9226\n",
      "Epoch 192/300\n",
      "\n",
      "Epoch 00192: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 154us/step - loss: 0.2219 - acc: 0.9160 - val_loss: 0.2051 - val_acc: 0.9210\n",
      "Epoch 193/300\n",
      "\n",
      "Epoch 00193: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 147us/step - loss: 0.2204 - acc: 0.9168 - val_loss: 0.2046 - val_acc: 0.9242\n",
      "Epoch 194/300\n",
      "\n",
      "Epoch 00194: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 148us/step - loss: 0.2158 - acc: 0.9168 - val_loss: 0.2036 - val_acc: 0.9226\n",
      "Epoch 195/300\n",
      "\n",
      "Epoch 00195: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 149us/step - loss: 0.2109 - acc: 0.9140 - val_loss: 0.2029 - val_acc: 0.9194\n",
      "Epoch 196/300\n",
      "\n",
      "Epoch 00196: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 147us/step - loss: 0.2104 - acc: 0.9164 - val_loss: 0.2031 - val_acc: 0.9226\n",
      "Epoch 197/300\n",
      "\n",
      "Epoch 00197: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 164us/step - loss: 0.2211 - acc: 0.9156 - val_loss: 0.2037 - val_acc: 0.9226\n",
      "Epoch 198/300\n",
      "\n",
      "Epoch 00198: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 165us/step - loss: 0.2173 - acc: 0.9148 - val_loss: 0.2041 - val_acc: 0.9210\n",
      "Epoch 199/300\n",
      "\n",
      "Epoch 00199: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 171us/step - loss: 0.2204 - acc: 0.9156 - val_loss: 0.2030 - val_acc: 0.9226\n",
      "Epoch 200/300\n",
      "\n",
      "Epoch 00200: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 180us/step - loss: 0.2207 - acc: 0.9152 - val_loss: 0.2043 - val_acc: 0.9242\n",
      "Epoch 201/300\n",
      "\n",
      "Epoch 00201: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 161us/step - loss: 0.2194 - acc: 0.9120 - val_loss: 0.2033 - val_acc: 0.9194\n",
      "Epoch 202/300\n",
      "\n",
      "Epoch 00202: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 160us/step - loss: 0.2217 - acc: 0.9172 - val_loss: 0.2041 - val_acc: 0.9194\n",
      "Epoch 203/300\n",
      "\n",
      "Epoch 00203: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 149us/step - loss: 0.2157 - acc: 0.9144 - val_loss: 0.2038 - val_acc: 0.9194\n",
      "Epoch 204/300\n",
      "\n",
      "Epoch 00204: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 145us/step - loss: 0.2109 - acc: 0.9201 - val_loss: 0.2029 - val_acc: 0.9242\n",
      "Epoch 205/300\n",
      "\n",
      "Epoch 00205: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 144us/step - loss: 0.2230 - acc: 0.9156 - val_loss: 0.2040 - val_acc: 0.9242\n",
      "Epoch 206/300\n",
      "\n",
      "Epoch 00206: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 145us/step - loss: 0.2183 - acc: 0.9144 - val_loss: 0.2037 - val_acc: 0.9226\n",
      "Epoch 207/300\n",
      "\n",
      "Epoch 00207: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 145us/step - loss: 0.2126 - acc: 0.9148 - val_loss: 0.2039 - val_acc: 0.9210\n",
      "Epoch 208/300\n",
      "\n",
      "Epoch 00208: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 146us/step - loss: 0.2205 - acc: 0.9164 - val_loss: 0.2060 - val_acc: 0.9242\n",
      "Epoch 209/300\n",
      "\n",
      "Epoch 00209: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 145us/step - loss: 0.2216 - acc: 0.9164 - val_loss: 0.2020 - val_acc: 0.9226\n",
      "Epoch 210/300\n",
      "\n",
      "Epoch 00210: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 151us/step - loss: 0.2161 - acc: 0.9156 - val_loss: 0.2020 - val_acc: 0.9242\n",
      "Epoch 211/300\n",
      "\n",
      "Epoch 00211: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 161us/step - loss: 0.2190 - acc: 0.9116 - val_loss: 0.2030 - val_acc: 0.9210\n",
      "Epoch 212/300\n",
      "\n",
      "Epoch 00212: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 153us/step - loss: 0.2193 - acc: 0.9176 - val_loss: 0.2035 - val_acc: 0.9242\n",
      "Epoch 213/300\n",
      "\n",
      "Epoch 00213: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 148us/step - loss: 0.2106 - acc: 0.9160 - val_loss: 0.2041 - val_acc: 0.9210\n",
      "Epoch 214/300\n",
      "\n",
      "Epoch 00214: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 146us/step - loss: 0.2071 - acc: 0.9233 - val_loss: 0.2039 - val_acc: 0.9194\n",
      "Epoch 215/300\n",
      "\n",
      "Epoch 00215: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 163us/step - loss: 0.2116 - acc: 0.9176 - val_loss: 0.2032 - val_acc: 0.9210\n",
      "Epoch 216/300\n",
      "\n",
      "Epoch 00216: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 152us/step - loss: 0.2147 - acc: 0.9184 - val_loss: 0.2030 - val_acc: 0.9226\n",
      "Epoch 217/300\n",
      "\n",
      "Epoch 00217: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 152us/step - loss: 0.2147 - acc: 0.9172 - val_loss: 0.2048 - val_acc: 0.9210\n",
      "Epoch 218/300\n",
      "\n",
      "Epoch 00218: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 147us/step - loss: 0.2169 - acc: 0.9172 - val_loss: 0.2066 - val_acc: 0.9226\n",
      "Epoch 219/300\n",
      "\n",
      "Epoch 00219: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 148us/step - loss: 0.2159 - acc: 0.9201 - val_loss: 0.2052 - val_acc: 0.9226\n",
      "Epoch 220/300\n",
      "\n",
      "Epoch 00220: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 146us/step - loss: 0.2163 - acc: 0.9172 - val_loss: 0.2041 - val_acc: 0.9226\n",
      "Epoch 221/300\n",
      "\n",
      "Epoch 00221: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 150us/step - loss: 0.2137 - acc: 0.9180 - val_loss: 0.2033 - val_acc: 0.9242\n",
      "Epoch 222/300\n",
      "\n",
      "Epoch 00222: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 151us/step - loss: 0.2213 - acc: 0.9104 - val_loss: 0.2034 - val_acc: 0.9242\n",
      "Epoch 223/300\n",
      "\n",
      "Epoch 00223: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 147us/step - loss: 0.2224 - acc: 0.9116 - val_loss: 0.2048 - val_acc: 0.9226\n",
      "Epoch 224/300\n",
      "\n",
      "Epoch 00224: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 147us/step - loss: 0.2184 - acc: 0.9132 - val_loss: 0.2043 - val_acc: 0.9226\n",
      "Epoch 225/300\n",
      "\n",
      "Epoch 00225: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 150us/step - loss: 0.2170 - acc: 0.9144 - val_loss: 0.2038 - val_acc: 0.9226\n",
      "Epoch 226/300\n",
      "\n",
      "Epoch 00226: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 147us/step - loss: 0.2229 - acc: 0.9096 - val_loss: 0.2063 - val_acc: 0.9210\n",
      "Epoch 227/300\n",
      "\n",
      "Epoch 00227: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 155us/step - loss: 0.2234 - acc: 0.9120 - val_loss: 0.2051 - val_acc: 0.9210\n",
      "Epoch 228/300\n",
      "\n",
      "Epoch 00228: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 151us/step - loss: 0.2254 - acc: 0.9140 - val_loss: 0.2051 - val_acc: 0.9194\n",
      "Epoch 229/300\n",
      "\n",
      "Epoch 00229: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 148us/step - loss: 0.2157 - acc: 0.9197 - val_loss: 0.2050 - val_acc: 0.9194\n",
      "Epoch 230/300\n",
      "\n",
      "Epoch 00230: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 149us/step - loss: 0.2215 - acc: 0.9136 - val_loss: 0.2041 - val_acc: 0.9242\n",
      "Epoch 231/300\n",
      "\n",
      "Epoch 00231: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 147us/step - loss: 0.2196 - acc: 0.9112 - val_loss: 0.2041 - val_acc: 0.9226\n",
      "Epoch 232/300\n",
      "\n",
      "Epoch 00232: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 151us/step - loss: 0.2194 - acc: 0.9100 - val_loss: 0.2034 - val_acc: 0.9210\n",
      "Epoch 233/300\n",
      "\n",
      "Epoch 00233: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 158us/step - loss: 0.2193 - acc: 0.9172 - val_loss: 0.2023 - val_acc: 0.9242\n",
      "Epoch 234/300\n",
      "\n",
      "Epoch 00234: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 155us/step - loss: 0.2225 - acc: 0.9116 - val_loss: 0.2027 - val_acc: 0.9226\n",
      "Epoch 235/300\n",
      "\n",
      "Epoch 00235: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 154us/step - loss: 0.2238 - acc: 0.9104 - val_loss: 0.2051 - val_acc: 0.9210\n",
      "Epoch 236/300\n",
      "\n",
      "Epoch 00236: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 157us/step - loss: 0.2168 - acc: 0.9116 - val_loss: 0.2031 - val_acc: 0.9242\n",
      "Epoch 237/300\n",
      "\n",
      "Epoch 00237: LearningRateScheduler setting learning rate to 0.0001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2477/2477 [==============================] - 0s 153us/step - loss: 0.2158 - acc: 0.9160 - val_loss: 0.2038 - val_acc: 0.9226\n",
      "Epoch 238/300\n",
      "\n",
      "Epoch 00238: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 151us/step - loss: 0.2102 - acc: 0.9176 - val_loss: 0.2020 - val_acc: 0.9226\n",
      "Epoch 239/300\n",
      "\n",
      "Epoch 00239: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 147us/step - loss: 0.2170 - acc: 0.9132 - val_loss: 0.2026 - val_acc: 0.9226\n",
      "Epoch 240/300\n",
      "\n",
      "Epoch 00240: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 147us/step - loss: 0.2097 - acc: 0.9193 - val_loss: 0.2033 - val_acc: 0.9194\n",
      "Epoch 241/300\n",
      "\n",
      "Epoch 00241: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 146us/step - loss: 0.2163 - acc: 0.9168 - val_loss: 0.2024 - val_acc: 0.9210\n",
      "Epoch 242/300\n",
      "\n",
      "Epoch 00242: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 148us/step - loss: 0.2182 - acc: 0.9193 - val_loss: 0.2033 - val_acc: 0.9210\n",
      "Epoch 243/300\n",
      "\n",
      "Epoch 00243: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 146us/step - loss: 0.2146 - acc: 0.9128 - val_loss: 0.2046 - val_acc: 0.9177\n",
      "Epoch 244/300\n",
      "\n",
      "Epoch 00244: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 151us/step - loss: 0.2180 - acc: 0.9120 - val_loss: 0.2049 - val_acc: 0.9226\n",
      "Epoch 245/300\n",
      "\n",
      "Epoch 00245: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 148us/step - loss: 0.2160 - acc: 0.9193 - val_loss: 0.2051 - val_acc: 0.9161\n",
      "Epoch 246/300\n",
      "\n",
      "Epoch 00246: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 148us/step - loss: 0.2132 - acc: 0.9156 - val_loss: 0.2037 - val_acc: 0.9161\n",
      "Epoch 247/300\n",
      "\n",
      "Epoch 00247: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 146us/step - loss: 0.2204 - acc: 0.9172 - val_loss: 0.2038 - val_acc: 0.9194\n",
      "Epoch 248/300\n",
      "\n",
      "Epoch 00248: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 147us/step - loss: 0.2165 - acc: 0.9160 - val_loss: 0.2051 - val_acc: 0.9194\n",
      "Epoch 249/300\n",
      "\n",
      "Epoch 00249: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 151us/step - loss: 0.2179 - acc: 0.9140 - val_loss: 0.2055 - val_acc: 0.9210\n",
      "Epoch 250/300\n",
      "\n",
      "Epoch 00250: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 154us/step - loss: 0.2200 - acc: 0.9172 - val_loss: 0.2058 - val_acc: 0.9194\n",
      "Epoch 251/300\n",
      "\n",
      "Epoch 00251: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 150us/step - loss: 0.2187 - acc: 0.9116 - val_loss: 0.2049 - val_acc: 0.9226\n",
      "Epoch 252/300\n",
      "\n",
      "Epoch 00252: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 149us/step - loss: 0.2167 - acc: 0.9144 - val_loss: 0.2063 - val_acc: 0.9129\n",
      "Epoch 253/300\n",
      "\n",
      "Epoch 00253: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 146us/step - loss: 0.2214 - acc: 0.9112 - val_loss: 0.2052 - val_acc: 0.9194\n",
      "Epoch 254/300\n",
      "\n",
      "Epoch 00254: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 147us/step - loss: 0.2171 - acc: 0.9221 - val_loss: 0.2044 - val_acc: 0.9226\n",
      "Epoch 255/300\n",
      "\n",
      "Epoch 00255: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 147us/step - loss: 0.2120 - acc: 0.9245 - val_loss: 0.2027 - val_acc: 0.9226\n",
      "Epoch 256/300\n",
      "\n",
      "Epoch 00256: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 148us/step - loss: 0.2164 - acc: 0.9168 - val_loss: 0.2042 - val_acc: 0.9210\n",
      "Epoch 257/300\n",
      "\n",
      "Epoch 00257: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 147us/step - loss: 0.2173 - acc: 0.9104 - val_loss: 0.2048 - val_acc: 0.9226\n",
      "Epoch 258/300\n",
      "\n",
      "Epoch 00258: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 147us/step - loss: 0.2189 - acc: 0.9148 - val_loss: 0.2050 - val_acc: 0.9226\n",
      "Epoch 259/300\n",
      "\n",
      "Epoch 00259: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 154us/step - loss: 0.2247 - acc: 0.9144 - val_loss: 0.2048 - val_acc: 0.9210\n",
      "Epoch 260/300\n",
      "\n",
      "Epoch 00260: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 150us/step - loss: 0.2205 - acc: 0.9104 - val_loss: 0.2046 - val_acc: 0.9210\n",
      "Epoch 261/300\n",
      "\n",
      "Epoch 00261: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 149us/step - loss: 0.2195 - acc: 0.9156 - val_loss: 0.2042 - val_acc: 0.9210\n",
      "Epoch 262/300\n",
      "\n",
      "Epoch 00262: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 166us/step - loss: 0.2159 - acc: 0.9160 - val_loss: 0.2041 - val_acc: 0.9194\n",
      "Epoch 263/300\n",
      "\n",
      "Epoch 00263: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 164us/step - loss: 0.2105 - acc: 0.9156 - val_loss: 0.2039 - val_acc: 0.9210\n",
      "Epoch 264/300\n",
      "\n",
      "Epoch 00264: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 178us/step - loss: 0.2122 - acc: 0.9197 - val_loss: 0.2048 - val_acc: 0.9177\n",
      "Epoch 265/300\n",
      "\n",
      "Epoch 00265: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 163us/step - loss: 0.2199 - acc: 0.9213 - val_loss: 0.2051 - val_acc: 0.9210\n",
      "Epoch 266/300\n",
      "\n",
      "Epoch 00266: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 146us/step - loss: 0.2073 - acc: 0.9253 - val_loss: 0.2039 - val_acc: 0.9194\n",
      "Epoch 267/300\n",
      "\n",
      "Epoch 00267: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 155us/step - loss: 0.2214 - acc: 0.9152 - val_loss: 0.2039 - val_acc: 0.9194\n",
      "Epoch 268/300\n",
      "\n",
      "Epoch 00268: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 155us/step - loss: 0.2198 - acc: 0.9180 - val_loss: 0.2043 - val_acc: 0.9210\n",
      "Epoch 269/300\n",
      "\n",
      "Epoch 00269: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 152us/step - loss: 0.2229 - acc: 0.9108 - val_loss: 0.2041 - val_acc: 0.9194\n",
      "Epoch 270/300\n",
      "\n",
      "Epoch 00270: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 157us/step - loss: 0.2215 - acc: 0.9132 - val_loss: 0.2039 - val_acc: 0.9226\n",
      "Epoch 271/300\n",
      "\n",
      "Epoch 00271: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 155us/step - loss: 0.2245 - acc: 0.9156 - val_loss: 0.2034 - val_acc: 0.9242\n",
      "Epoch 272/300\n",
      "\n",
      "Epoch 00272: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 154us/step - loss: 0.2284 - acc: 0.9096 - val_loss: 0.2038 - val_acc: 0.9242\n",
      "Epoch 273/300\n",
      "\n",
      "Epoch 00273: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 152us/step - loss: 0.2267 - acc: 0.9084 - val_loss: 0.2040 - val_acc: 0.9177\n",
      "Epoch 274/300\n",
      "\n",
      "Epoch 00274: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 159us/step - loss: 0.2309 - acc: 0.9156 - val_loss: 0.2046 - val_acc: 0.9194\n",
      "Epoch 275/300\n",
      "\n",
      "Epoch 00275: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 149us/step - loss: 0.2152 - acc: 0.9168 - val_loss: 0.2036 - val_acc: 0.9226\n",
      "Epoch 276/300\n",
      "\n",
      "Epoch 00276: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 148us/step - loss: 0.2118 - acc: 0.9176 - val_loss: 0.2038 - val_acc: 0.9242\n",
      "Epoch 277/300\n",
      "\n",
      "Epoch 00277: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 147us/step - loss: 0.2206 - acc: 0.9209 - val_loss: 0.2053 - val_acc: 0.9194\n",
      "Epoch 278/300\n",
      "\n",
      "Epoch 00278: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 155us/step - loss: 0.2186 - acc: 0.9148 - val_loss: 0.2046 - val_acc: 0.9210\n",
      "Epoch 279/300\n",
      "\n",
      "Epoch 00279: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 149us/step - loss: 0.2230 - acc: 0.9148 - val_loss: 0.2056 - val_acc: 0.9242\n",
      "Epoch 280/300\n",
      "\n",
      "Epoch 00280: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 149us/step - loss: 0.2156 - acc: 0.9124 - val_loss: 0.2028 - val_acc: 0.9242\n",
      "Epoch 281/300\n",
      "\n",
      "Epoch 00281: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 152us/step - loss: 0.2161 - acc: 0.9172 - val_loss: 0.2035 - val_acc: 0.9210\n",
      "Epoch 282/300\n",
      "\n",
      "Epoch 00282: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 147us/step - loss: 0.2164 - acc: 0.9176 - val_loss: 0.2057 - val_acc: 0.9177\n",
      "Epoch 283/300\n",
      "\n",
      "Epoch 00283: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 151us/step - loss: 0.2221 - acc: 0.9184 - val_loss: 0.2050 - val_acc: 0.9210\n",
      "Epoch 284/300\n",
      "\n",
      "Epoch 00284: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 150us/step - loss: 0.2198 - acc: 0.9152 - val_loss: 0.2043 - val_acc: 0.9194\n",
      "Epoch 285/300\n",
      "\n",
      "Epoch 00285: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 148us/step - loss: 0.2237 - acc: 0.9197 - val_loss: 0.2048 - val_acc: 0.9194\n",
      "Epoch 286/300\n",
      "\n",
      "Epoch 00286: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 149us/step - loss: 0.2179 - acc: 0.9124 - val_loss: 0.2039 - val_acc: 0.9226\n",
      "Epoch 287/300\n",
      "\n",
      "Epoch 00287: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 149us/step - loss: 0.2168 - acc: 0.9160 - val_loss: 0.2037 - val_acc: 0.9226\n",
      "Epoch 288/300\n",
      "\n",
      "Epoch 00288: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 148us/step - loss: 0.2192 - acc: 0.9176 - val_loss: 0.2054 - val_acc: 0.9210\n",
      "Epoch 289/300\n",
      "\n",
      "Epoch 00289: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 145us/step - loss: 0.2179 - acc: 0.9180 - val_loss: 0.2048 - val_acc: 0.9194\n",
      "Epoch 290/300\n",
      "\n",
      "Epoch 00290: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 146us/step - loss: 0.2213 - acc: 0.9152 - val_loss: 0.2059 - val_acc: 0.9226\n",
      "Epoch 291/300\n",
      "\n",
      "Epoch 00291: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 147us/step - loss: 0.2123 - acc: 0.9180 - val_loss: 0.2040 - val_acc: 0.9226\n",
      "Epoch 292/300\n",
      "\n",
      "Epoch 00292: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 146us/step - loss: 0.2230 - acc: 0.9136 - val_loss: 0.2037 - val_acc: 0.9194\n",
      "Epoch 293/300\n",
      "\n",
      "Epoch 00293: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 147us/step - loss: 0.2187 - acc: 0.9217 - val_loss: 0.2043 - val_acc: 0.9226\n",
      "Epoch 294/300\n",
      "\n",
      "Epoch 00294: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 146us/step - loss: 0.2101 - acc: 0.9168 - val_loss: 0.2056 - val_acc: 0.9210\n",
      "Epoch 295/300\n",
      "\n",
      "Epoch 00295: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 144us/step - loss: 0.2197 - acc: 0.9132 - val_loss: 0.2049 - val_acc: 0.9226\n",
      "Epoch 296/300\n",
      "\n",
      "Epoch 00296: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 145us/step - loss: 0.2159 - acc: 0.9168 - val_loss: 0.2051 - val_acc: 0.9210\n",
      "Epoch 297/300\n",
      "\n",
      "Epoch 00297: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 146us/step - loss: 0.2221 - acc: 0.9104 - val_loss: 0.2040 - val_acc: 0.9210\n",
      "Epoch 298/300\n",
      "\n",
      "Epoch 00298: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 165us/step - loss: 0.2146 - acc: 0.9172 - val_loss: 0.2038 - val_acc: 0.9226\n",
      "Epoch 299/300\n",
      "\n",
      "Epoch 00299: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 146us/step - loss: 0.2125 - acc: 0.9164 - val_loss: 0.2050 - val_acc: 0.9194\n",
      "Epoch 300/300\n",
      "\n",
      "Epoch 00300: LearningRateScheduler setting learning rate to 0.0001.\n",
      "2477/2477 [==============================] - 0s 144us/step - loss: 0.2130 - acc: 0.9233 - val_loss: 0.2040 - val_acc: 0.9226\n"
     ]
    }
   ],
   "source": [
    "# Train model Dense\n",
    "\n",
    "model_Dense_train = model_Dense.fit(train_X, train_Y, epochs=epochs, batch_size=32, validation_data=(valid_X, valid_Y), callbacks = callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3XmcFNW5//HPM8MAsggIuDHA4BLZFZzgQhBRYxCvEJUYFPdEFDUm+jM3xC1Kwo0LUaNBo8nVq4ISoldDDEo2DGquyhJFERFURlFUQEERDAzz/P441dM9Q2+zQE/3fN+vV7266vTpqlNd1U+dOnWq2twdEREpLEW5LoCIiDQ+BXcRkQKk4C4iUoAU3EVECpCCu4hIAVJwFxEpQArussuY2X5mtqmx8+aSmR1nZqt2wny/a2bPROPFZrbJzHpkylvPZf3ZzMbX9/Np5jvdzK5v7PlKdlrkugDNVRQQ9gIqge3A68CDwL3uXpXDogEQBZLXE5LaApuB2I0RJ7j7s3WZp7u/DbRr7LyFzt2300jfhZn9DCh193MT5n98Y8xbmhYF99w6yd3/amYdgOHAL4HDgPNyWyxw93dJCChm5sDB7r4y1WfMrDgKRCKSY2qWaQLcfaO7zwa+DZxjZv0BzKyVmU01s3fN7CMz+7WZ7Ra9d7SZrTaz/2dmH5vZGjOrPiiY2Sgze93MPjez983syoT3/sPMXjazDWb2TzMbWJ9yR6fd08zsaTP7AhhmZqOjeX8WlfvahPwHRAeJ2PRzZnZDVIbPo/nsUde80fvnRctbZ2ZXRd/N0SnKnbGMZnZ2NI+1ZjYp4f02ZvaQmX1qZkuBQ9N8P78xsxtrpf3JzC6Lxq8xs7ej9VlqZqNTzKdFVKayaLqrmT0Zlf8FoFet/L+Kyv6ZmS0wsyOj9P8A/hMYHzXzLEr4bs+NxovM7Dozq4j2q/8xs92z+W4yMbOLzGylma03syfMbJ+EZd4RLW+jmS0xs76xMpvZsug7Wm1ml2e7vGbP3TXkYABWAcclSX8XmBiN3wbMBvYA2gN/BH4evXc0oUlnMlACjCI0m3SK3l8DDIvGOwGDo/FBwMeEM4Ri4JyoLK0ylNeBA2qlTQc+BY4gVBRaAccA/aLpg4F1wH9E+Q8Iu1z1558DVgAHAm2AZ4Gf1SPvAOBz4MioDLdF383RKdYlYxmBXwOtgcHAv4EDo/enAs9E32lPQtPVqjTLWQVYNN0Z2ALsFU2fBuwTleMMYFPCe98FnonGW0RlKoumHwUeib6HgdG2fiZhuWdF+0wL4EfA+7HtC/wM+J9a5XwOODcanwC8SThgtAf+ANyfzXeTZP2nA9dH48cT9rtDos/eBfw9eu9E4CWgQ/Rd9AX2jt5bCxwZje9BtB9ryDyo5t70fADsYWZG+KFd7u6fuPvnwH8B4xLybgMmu/s2d59DCA4HJbzX18x2d/dP3X1xlD4BuMfdX3T37e7+AOEHeng9y/u4u/+fu1e5+7/d/e/uvjSafgWYSWhySuW/3X2Fu28Gfk/48dc177eAJ9z9n+7+b+CadAXOsozXu/uX0fe2lHAQgBCQfxZ9pxXAr9Is6hnCgfeIhM8+6+4fReWY5e5ronI8TDgQlKcru5mVAN8ErnX3ze6+BHio1vo9FO0zlcDNwO6EwJyN8cBUd38n2ueuAs4ws8RYkeq7yTTf37r7y+7+JTAJGG5mpYR9dXegd1T+1939w+hzsf24fbROi5PNXHak4N70dAM+AboSamaLouaTDcDTUXrM+ugHHLOZeDv5qYTafIWZ/cPMYgGmJ/D/YvOM5tsd2Lee5X0vccLMjjCzZ6JT9o2EGmiXNJ//MGE8sfx1ybtvYjnc/QvCGUVS2ZQxIbjUXtY+1FznilTL8XBh/HfA6VHSGcCMhHKca2avJGyH3rXLkcRehDOulGUws/80szeidfuUcDE803xj9q01vwqgJQn7XZrvJuv5uvtnUdm6ufufCWcDdwOx5sf2UdaTgdHAu9E2OyzL9Wj2FNybEDP7KiG4P0doKtgC9HP3jtHQwd2z7W2ywN3HAHsCTwCzorfeA6YkzLOju7dx90fqWezajxWdCTwGdHf3DsBvAavnvLO1BiiNTZhZW0KzSSoNKeOHhINhTNLuiQkeAb5lZr0IzRj/G5VxP0Iwmwh0dveOwBtZlOMjoCpVGcxsBHAF4eDekfA9bEqYb6bHwH5AqAAkznsroXmkIWrMNwrenQhNRrj77e4+GOhPaJa5Ikp/0d1HE/bjJwnbTrKg4N4EmNnu0cWumcB0d381qvX9BrjNzPaM8nUzs29kMb+WZjbezDq4+zbgM0JAIJrnRWZ2mAVtzezEhJpSQ7UHPnH3L83scGo2I+0svwe+aWaHm1lLwnWIdBpSxlnAVWbW0UJ30UvTZXb3BYTv/15gTtTUAaG264SgaWZ2AVGzRIb5bSMcrG8ws90sXHw/q9a6VRIqByXA9YSae8xHQFnU7JfMI8AVZlYW7RNTgEe84d1zHwG+Y2YDzawV8HNCE9VqMxsSDS2ALwgHk6po/c6Imha3Ea6r5LybcL5QcM+tP5rZ54Ta9NXArdTsBvkjYCXwgpl9BvyVeJt6JmcBq6LPXURo88TdFwIXENqKP43mf26D1yRuIvDzaL2uIn7GsNNE7c6XE4L8B8D6aPj3TijjTwhnCquApwj3JmTyCHAc8HCtMt9JuJC4hrBdX8yyDBMJtd6PgP8G7k94bw5hP1kRlfGzaP4xvyM0s3xiZi8lmfdvojzPAm8TAur3syxXSu7+NOGg+3hUnh5E+yThDOO/gQ1RmdcQfgsQLvhXRPvxd4AzG1qW5iJ2FV+kYERd9zYAPd39vUz5RQqRau5SECz0XW9jZu2AXwCLFdilOVNwl0JxMqFJZjVQRryHikizpGYZEZECpJq7iEgBytmDw7p06eJlZWW5WryISF5atGjROnfvmilfVsHdzEYSnlhYTLiFuPbDkG4DRkSTbYA9o5syUiorK2PhwoXZLF5ERCJmlvKu6EQZg7uZFQPTgK8TLlYtMLPZ7l79rG93vzwh//cID6cSEZEcyabNfQiw0t3fdvethLsox6TJfzrhpg0REcmRbIJ7N2o+pGh1lLYDM+tJeFTo31O8P8HMFprZwrVrG/qoChERSaWxL6iOAx71FP/G4+73Ep6xQXl5ufpgiuxC27ZtY/Xq1Xz55Ze5LopkoXXr1pSWllJSUlKvz2cT3N+n5hPoSqO0ZMYBl9SrJCKyU61evZr27dtTVlZG6ueGSVPg7qxfv57Vq1fTq1evzB9IIptmmQXAgWbWK3ri3jjCvwPVYGa9CQ8z+r96lSQLM2ZAWRkUFYXXGTMyfUJEYr788ks6d+6swJ4HzIzOnTs36CwrY83d3SvN7FJgLqEr5H3uvtTMJgMLPfz3J4SgP9N30i2vM2bAhAmweXOYrqgI0wDjx6f+nIjEKbDnj4Zuq6za3KO/cJtTK+26WtPXN6gkGVx9dTywx2zeHNIV3EVEasqbxw+8+27d0kWkaVm/fj2HHHIIhxxyCHvvvTfdunWrnt66dWtW8zjvvPNYvnx52jzTpk1jRiO12X7ta1/j5ZdfbpR57Wo5e/xAXfXoEZpikqWLSOObMSOcGb/7bvidTZnSsLPkzp07VwfK66+/nnbt2nHllVfWyOPuuDtFRcnrnffff3/S9ESXXKI+HZBHNfcpU6BNm5ppbdqEdBFpXLFrXBUV4B6/xrUzOjGsXLmSvn37Mn78ePr168eaNWuYMGEC5eXl9OvXj8mT4/+aGKtJV1ZW0rFjRyZNmsTBBx/MEUccwccffwzANddcw+23316df9KkSQwZMoSDDjqIf/7znwB88cUXnHrqqfTt25exY8dSXl6esYY+ffp0BgwYQP/+/bnqqqsAqKys5KyzzqpOv+OOOwC47bbb6Nu3LwMHDuTMM3Pz51F5U3OP1RgasyYhIsnt6mtcb7zxBg8++CDl5eUA3Hjjjeyxxx5UVlYyYsQIxo4dS9++fWt8ZuPGjQwfPpwbb7yRK664gvvuu49JkybtMG9356WXXmL27NlMnjyZp59+mjvvvJO9996bxx57jFdeeYXBgwenLd/q1au55pprWLhwIR06dOC4447jySefpGvXrqxbt45XX30VgA0bNgBw8803U1FRQcuWLavTdrW8qblD2KlWrYKqqvCqwC6yc+zqa1z7779/dWAHeOSRRxg8eDCDBw9m2bJlvP766zt8ZrfdduOEE04A4NBDD2XVqlVJ533KKafskOe5555j3Ljwv+gHH3ww/fr1S1u+F198kWOOOYYuXbpQUlLCGWecwfz58znggANYvnw5l112GXPnzqVDhw4A9OvXjzPPPJMZM2bU+yakhsqr4C4iu0aqa1k76xpX27Ztq8dXrFjBL3/5S/7+97+zZMkSRo4cmbS/d8uWLavHi4uLqaysTDrvVq1aZcxTX507d2bJkiUMGzaMadOmceGFFwIwd+5cLrroIhYsWMCQIUPYvj3pTfs7lYK7iOwgl9e4PvvsM9q3b8/uu+/OmjVrmDt3bqMvY+jQocyaNQuAV199NemZQaLDDjuMefPmsX79eiorK5k5cybDhw9n7dq1uDvf+ta3mDx5MosXL2b79u2sXr2aY445hptvvpl169axuXYb1y6QN23uIrLr5PIa1+DBg+nbty+9e/emZ8+eDB06tNGX8b3vfY+zzz6bvn37Vg+xJpVkSktL+elPf8rRRx+Nu3PSSSdx4oknsnjxYr7zne/g7pgZN910E5WVlZxxxhl8/vnnVFVVceWVV9K+fftGX4dMcvYfquXl5a4/6xDZdZYtW0afPn1yXYwmobKyksrKSlq3bs2KFSs4/vjjWbFiBS1aNK36brJtZmaL3L08xUeqNa01ERHZBTZt2sSxxx5LZWUl7s4999zT5AJ7QxXW2oiIZKFjx44sWrQo18XYqXRBVUSkACm4i4gUIAV3EZECpOAuIlKAFNxFZJcYMWLEDjck3X777UycODHt59q1awfABx98wNixY5PmOfroo8nUtfr222+vcTPRqFGjGuW5L9dffz1Tp05t8Hwam4K7iOwSp59+OjNnzqyRNnPmTE4//fSsPr/vvvvy6KOP1nv5tYP7nDlz6NixY73n19QpuIvILjF27Fj+9Kc/Vf8xx6pVq/jggw8YNmxYdb/zwYMHM2DAAP7whz/s8PlVq1bRv39/ALZs2cK4cePo06cPJ598Mlu2bKnON3HixOrHBf/kJz8B4I477uCDDz5gxIgRjBgxAoCysjLWrVsHwK233kr//v3p379/9eOCV61aRZ8+fbjgggvo168fxx9/fI3lJPPyyy9z+OGHM3DgQE4++WQ+/fTT6uXHHgEce2DZP/7xj+o/Kxk0aBCff/55vb/bZNTPXaQZ+sEPoLH/YOiQQyCKi0ntscceDBkyhKeeeooxY8Ywc+ZMTjvtNMyM1q1b8/jjj7P77ruzbt06Dj/8cEaPHp3yf0Tvvvtu2rRpw7Jly1iyZEmNR/ZOmTKFPfbYg+3bt3PssceyZMkSLrvsMm699VbmzZtHly5dasxr0aJF3H///bz44ou4O4cddhjDhw+nU6dOrFixgkceeYTf/OY3nHbaaTz22GNpn89+9tlnc+eddzJ8+HCuu+46brjhBm6//XZuvPFG3nnnHVq1alXdFDR16lSmTZvG0KFD2bRpE61bt67Dt52Zau4issskNs0kNsm4O1dddRUDBw7kuOOO4/333+ejjz5KOZ/58+dXB9mBAwcycODA6vdmzZrF4MGDGTRoEEuXLs34ULDnnnuOk08+mbZt29KuXTtOOeUUnn32WQB69erFIYccAqR/rDCE58tv2LCB4cOHA3DOOecwf/786jKOHz+e6dOnV98JO3ToUK644gruuOMONmzY0Oh3yKrmLtIMpath70xjxozh8ssvZ/HixWzevJlDDz0UgBkzZrB27VoWLVpESUkJZWVlSR/zm8k777zD1KlTWbBgAZ06deLcc8+t13xiYo8LhvDI4EzNMqn86U9/Yv78+fzxj39kypQpvPrqq0yaNIkTTzyROXPmMHToUObOnUvv3r3rXdbaVHMXkV2mXbt2jBgxgvPPP7/GhdSNGzey5557UlJSwrx586hI9ofJCY466igefvhhAF577TWWLFkChMcFt23blg4dOvDRRx/x1FNPVX+mffv2Sdu1hw0bxhNPPMHmzZv54osvePzxxxk2bFid161Dhw506tSputb/0EMPMXz4cKqqqnjvvfcYMWIEN910Exs3bmTTpk289dZbDBgwgB/96Ed89atf5Y033qjzMtNRzV1EdqnTTz+dk08+uUbPmfHjx3PSSScxYMAAysvLM9ZgJ06cyHnnnUefPn3o06dP9RnAwQcfzKBBg+jduzfdu3ev8bjgCRMmMHLkSPbdd1/mzZtXnT548GDOPfdchgwZAsB3v/tdBg0alLYJJpUHHniAiy66iM2bN7Pffvtx//33s337ds4880w2btyIu3PZZZfRsWNHrr32WubNm0dRURH9+vWr/lepxqJH/oo0E3rkb/5pyCN/1SwjIlKAFNxFRAqQgrtIM5KrZlipu4ZuKwV3kWaidevWrF+/XgE+D7g769evb9CNTeotI9JMlJaWsnr1atauXZvrokgWWrduTWlpab0/r+Au0kyUlJTQq1evXBdDdhE1y4iIFCAFdxGRApRVcDezkWa23MxWmtmkFHlOM7PXzWypmT3cuMUUEZG6yNjmbmbFwDTg68BqYIGZzXb31xPyHAj8GBjq7p+a2Z47q8AiIpJZNjX3IcBKd3/b3bcCM4ExtfJcAExz908B3P3jxi2miIjURTbBvRvwXsL06igt0VeAr5jZ82b2gpmNTDYjM5tgZgvNbKG6Y4mI7DyNdUG1BXAgcDRwOvAbM9vhzwnd/V53L3f38q5duzbSokVEpLZsgvv7QPeE6dIoLdFqYLa7b3P3d4A3CcFeRERyIJvgvgA40Mx6mVlLYBwwu1aeJwi1dsysC6GZ5u1GLKeIiNRBxuDu7pXApcBcYBkwy92XmtlkMxsdZZsLrDez14F5wA/dff3OKrSIiKSnP+sQEckj+rMOEZFmTMFdRKQAKbiLiBQgBXcRkQKk4C4iUoAU3EVECpCCu4hIAcq74D51KhQVwebNuS6JiEjTlXfBvagI3KGyMtclERFpuvIuuLeI/l5EwV1EJDUFdxGRApR3wX3RovC6115QVgYzZuS0OCIiTVJeBfcZM2D69Ph0RQVMmKAALyJSW14F96uvhq1ba6Zt3hzSRUQkLq+C+7vv1i1dRKS5yqvg3qNH3dJFRJqrvAruU6ZAy5Y109q0CekiIhKXV8F9/Hi49NL4dM+ecO+9IV1EROLyKrgDHHdceH3hBVi1SoFdRCSZvAvuuolJRCQzBXcRkQKk4C4iUoAU3EVEClDeBvdt23JbDhGRpixvg7tq7iIiqeVdcC8pCa8K7iIiqeVdcFfNXUQkMwV3EZECpOAuIlKAFNxFRAqQgruISAHKKrib2UgzW25mK81sUpL3zzWztWb2cjR8t/GLGii4i4hk1iJTBjMrBqYBXwdWAwvMbLa7v14r6+/c/dIdZtDIdBOTiEhm2dTchwAr3f1td98KzATG7Nxipaaau4hIZtkE927AewnTq6O02k41syVm9qiZdU82IzObYGYLzWzh2rVr61Fc3cQkIpKNxrqg+kegzN0HAn8BHkiWyd3vdfdydy/v2rVrvRakmruISGbZBPf3gcSaeGmUVs3d17v7v6PJ3wKHNk7xdlQUlVjBXUQktWyC+wLgQDPrZWYtgXHA7MQMZrZPwuRoYFnjFbEms1B7V3AXEUktY28Zd680s0uBuUAxcJ+7LzWzycBCd58NXGZmo4FK4BPg3J1YZgV3EZEMMgZ3AHefA8yplXZdwviPgR83btFSU3AXEUkv7+5QhRDc1c9dRCS1vA3uqrmLiKSWl8G9pETBXUQknbwM7qq5i4ikp+AuIlKAFNxFRAqQgruISAFScBcRKUAK7iIiBShvg7tuYhIRSS1vg7tq7iIiqeVlcNdNTCIi6eVlcFfNXUQkPQV3EZECpOAuIlKAFNxFRAqQgruISAHK2+Cufu4iIqnlbXBXzV1EJLW8DO7q5y4ikl5eBnfV3EVE0lNwFxEpQAruIiIFSMFdRKQAKbiLiBSgvA3u6ucuIpJa3gZ31dxFRFLLy+BeUgJVVWEQEZEd5WVwb9EivG7fnttyiIg0VXkd3NU0IyKSnIK7iEgByuvgrh4zIiLJZRXczWykmS03s5VmNilNvlPNzM2svPGKuKOSkvCq4C4iklzG4G5mxcA04ASgL3C6mfVNkq898H3gxcYuZG1qlhERSS+bmvsQYKW7v+3uW4GZwJgk+X4K3AR82YjlS2rhwvBaWgplZTBjxs5eoohIfskmuHcD3kuYXh2lVTOzwUB3d/9TuhmZ2QQzW2hmC9euXVvnwkII5A88EJ+uqIAJExTgRUQSNfiCqpkVAbcC/y9TXne/193L3b28a9eu9Vre1VfD1q010zZvDukiIhJkE9zfB7onTJdGaTHtgf7AM2a2CjgcmL2zLqq++27d0kVEmqNsgvsC4EAz62VmLYFxwOzYm+6+0d27uHuZu5cBLwCj3X3hzihwjx51SxcRaY4yBnd3rwQuBeYCy4BZ7r7UzCab2eidXcDapkyBVq1qprVpE9JFRCRokU0md58DzKmVdl2KvEc3vFipjR8Pr70GN94Ypnv2DIF9/PiduVQRkfySl3eojhwZXv/2N1i1SoFdRKS2vAzusTtUdROTiEhyeRnc9WwZEZH08jK469kyIiLpKbiLiBQgBXcRkQKUl8FdT4UUEUkvL4O7au4iIukpuIuIFCAFdxGRApTXwV1t7iIiyeVlcNdNTCIi6eVlcFezjIhIegruIiIFKC+DuxkUFyu4i4ikkpfBHUK7uy6oiogkl7fBvaRENXcRkVQU3EVECpCCu4hIAVJwFxEpQHkb3HVBVUQktbwN7qq5i4ikpuAuIlKAFNxFRApQ3gb3zz+HP/8ZioqgrAxmzMh1iUREmo4WuS5AfcyYARUVUFUVpisqYMKEMD5+fO7KJSLSVORlzf3qq+OBPWbz5pAuIiJ5Gtzffbdu6SIizU1eBvcePeqWLiLS3ORlcJ8yJVxITWQGo0blpjwiIk1NXgb38eN3rKW7wwMPqNeMiAjkaXAH+PjjHdN0UVVEJMgquJvZSDNbbmYrzWxSkvcvMrNXzexlM3vOzPo2flFr2rw5ebouqoqIZBHczawYmAacAPQFTk8SvB929wHufghwM3Bro5e0lrZtk6froqqISHY19yHASnd/2923AjOBMYkZ3P2zhMm2gDdeEZMrLw8XURO1aRMutoqINHfZBPduwHsJ06ujtBrM7BIze4tQc78s2YzMbIKZLTSzhWvXrq1Peav17g3t20PnzvG03XZr0CxFRApGo11Qdfdp7r4/8CPgmhR57nX3cncv79q1a4OWV1IC27fDli3xtPXrw2MI1GNGRJq7bIL7+0D3hOnSKC2VmcA3G1KobJSUhIuqtS+sqseMiEh2wX0BcKCZ9TKzlsA4YHZiBjM7MGHyRGBF4xUxuRYtQt/2ZNRjRkSau4xPhXT3SjO7FJgLFAP3uftSM5sMLHT32cClZnYcsA34FDhnZxYaQs09lT322NlLFxFp2rJ65K+7zwHm1Eq7LmH8+41croxiwT3Zf6l+/nlod9fjf0WkucrbO1RjwX333Xd8b+tW+P4uP9yIiDQdeR/cP/kk+fvr10OXLuo5IyLNU94G9xZRg1L37qnzrF8PZ56pIC8izU/eBvcOHcLrWWdlzqv+7yLS3ORtcB83Lvwx9mOPZdc7Rv3fRaQ5ydvg3rYtXHcdLF8OP/xhdp9R/3cRaS7yNrgDHHxweD3ggJrPmEmlqEhNMyLSPOR1cP/KV8Lr8uXwy1+Gp0Kms327LrCKSPOQ18G9XTsoLQ3Bffx4uPdeKC7O/LnEC6wzZoS2e7PQA8csTKcK/rH8RUXp84mI5FJWd6g2ZQcdBG+8EcZjd6ROmJD6n5piNm8ONzpt2RLPu317eK2oCPNInCeEQJ4471T5RERyLa9r7hCe6758efwhYrEafM+emT+7fn3qg0Cy3jVXX538KZTnnKMavIg0LXkf3A86CD77DD78MJ42fjysWgXTpzds3hUVoZkm1kafqrfN9u3qRy8iTUveB/fevcPr8uU7vjd+fHa9aDKJ3ema6hHDoH70ItK0FExwj7W715ZNL5rGUlGR/kJrXS/e5oIuGIsUhrwP7t26heAdq7l//nnN9+vSi6YxuIcgf955oTmnqCi8tmsXav8VFSFf4sXbs86KN/+0axfGE5uDUkkXiFO9l5hee3lm8TLG1kPNTSL5yTxdW8NOVF5e7gsXLmyUeQ0eDHvtBVdcAccfH+5YvemmEKxiiorSN6s0ZcceCytXhjb/Hj1g1CiYNSs0FyVq0yYcyJ5/Hn796x3Xt21b2LYtPBK5Lnr2DNcwRCT3zGyRu5dnypf3NXcIF1WXL4e//S1M33IL/OUvNfP06LHry9VY/va3mrXpu+/eMbBDvHtnssAO8MUXdQ/sEJapJhqR/FIQwb1371CzfP556NsXOnWC//mfmnmmTKlZky9U69fvnDOUWPPRxRc3/rzVzi/S+AoiuA8aFALac8/BkUfCGWfA44/Dhg3wwQcwbBgMHw4XXZTrkuY393BWUDv41g7OF1+cPljXbvc///yd386vA4g0O+6ek+HQQw/1xrJ1q/s++7iD+913u7/wQhh/8EH3GTPi4+7u550XpjXUf+jc2X36dPeePcO0Wfr8bdqE/O7htU2b7JbRs2eYd8+e8c8nEytLqryplhlbj7rMSwpPvm1zYKFnEWMbJVDXZ2jM4O7u/pOfhLVZsMB9+3b3ffd1P/lk9+uuC+lXXx3yXXllmG7RYscANHFizY2c6yDalIeWLeuWv2fP8P3X93utfYCIbafOnXcsS2LeTMtM3O6w44EqNq+mEgCaSjkKwfTpYf9Jt681Rc0uuH/xhfusWe5VVWH64ovdd9vNffTosJannhq8WU5YAAARF0lEQVTSv/nNMD10aOYfSaqgUFwc8uc6wObb0LbtrltWrObfGPNKdmZSOwDsiqCb7AykqQSifDvoZDqDjFVGmqJmF9xr+8tfwtoVFYXX/v1Dev/+Yfob38g8j0w/pmyDR3Fx6iChIX+HWABojKCbLDjWrlnG9uVU5ajPMhpq+vTkB+3EM57EdUjWFNYYsl23xObEbLZv7fll0wS4sw9yzT64b93q3rFjfEO1auW+bVuozYP7oEHZzSfdxsqm/Ti2k//iF+7t22cfOHr0aPxgtNtu2bV3a8huMAv7QbozvNpNfdk2LdV16Nw5dfNS7KCQrnKRbdBN1ZSR6vuJVWwSh5YtMy+r9vfTufOO44kHwdr7tVn4PupT7mRDSUnqg1imZr1U+0B9Nfvg7u5+5plhDQ89NP5ji71269Y4y3jwwfiG3HffHTdk4gXedEPtAHHuuelrGLV38okTd9x527atmeeXv2xYANFQcygurvvZWCx/Uz6LSxXop08PQa4xlpF4tpFYm67rWW6bNu7t2qV+/9hjGxbUd8bQ0KY0BXd3/8MfwhrecUd47dTJfcAA9zFjwk4aa5//5JMwfsst7qtW1W0Z774b32jXXrvj+2PGZL+jV1TE0046qfHbWJ99Nsyja9e6BZdUzQEaCneI1a4bWuNNNzTn/aohbfoK7h4C9oIF4fWNN9wrK0P6L34R1vzTT90/+ig0V0yaFNJivWqyNW9efIMdfviOy4/VKgYM8OofTeJGTgzW//pXPP3II0Pa3XfH0zp2bNgRP3YRuLjY/be/rXmGMXFi8iabSy9t2rVMDRrydahvE022wb0gbmJKxQzKy8PrQQfFHx62557hde1aWLAg/BvTbbeFtFdeST2/sWPhvvvC+Pz5cPnlcNddYfqkk+DVV6GqCh59FB5+GN57DzZtgmnTYOrUkC/x9v/OnWH//cNNVxB/pMCee8bH+/SJ5z/rLBg9Gu65J/vHCGzfDjNnwief1HxoWY8e4a7eqqrwetdd4bk0rVuHPKWl4XW//er+6IbOnWHixMZ53LJIodrpD+bL5giwM4ZdUXNP5emnw5Hzuefcb7ih5tG0tNT9hz90X7w45J0xw/3NN+PNLyNGhPS9945/pkUL93vuCeMrV7r37Rsunj7wQEh74QX3116ruZxrronXiOfPD/OcNStMf+1r7l26hLQ77wxpHTq4n3aa+69+FabPOSdcNE6nqsr9ootC/uHD3c8/P5SrdevQNfTGG92XL4/n37gxrP+3vx0+26pVuC8g2xuPsr2ByCx8R+nOCNKdsnfuvHMuDOsMRUMuhro20aBmmdQWLw5r/uij8X7wUPPHPX68+5NPxr/8W28N423buq9eHcYHDYrnX7AgvE6fHg9M++4bAv/mzaEJKHGDlpa6H3hgGD/jjJAn1gRz/vlhHpWV7hdeGK4VDBsWAvSFF8bn0b69+5//XHPd1q0L7fXLlrk//njId+yx8c8cemjNA9rpp4fPbdsWAm5RkfvDD4e0Aw90//rXw/gVV7jvtVfNK/6xA83YsaG5adOm5N93qh5Hl14a/66SHRj69t3xh2DmPnlyyNuhQ+YfTqzHVNu2NXs1JF5cT1y+7l9oXkOsY0IuyxDrdZUtBfc0Nm0KweiKK0KvmWOOCbXUU0+Nf+GdOoUffFlZyJtYk7zllvAaC/6xAF5U5D5qVEhr1Sq8fvWrYZlVVfHaZnl5/HOxefbs6f6974Xxhx4Kr2ecEdrehw0LAbR37zC/YcPcH3kktOO3aBEevfDYY2E5//Vf4bOjRrkfcIB7nz4hcMfa/E85xX3LlhDUIQTwtWvjZw2JAfanPw1pS5aEZfTpE9JfeSUcIE87LZzBxO4pmD275vccu2Ads327+89/7n7cce4ff+w+cmT43OOPx/P861/ul1wSXmPdVktKwg8gFswHDAhnSNkE9x/+MLx27+7+/PNhue7uH3wQbmS76aaaZXzrrdz+0At1aNs2ebfIXA2JPYJyfUDPac0dGAksB1YCk5K8fwXwOrAE+BvQM9M8cxnc3UOAiT2P5he/CLXx998P0716xb/4+fPdf/3rMB6rqR9wQHhdt879mWdC8HMPwTf2ueefD0FvzZr4MmM19dgFXXCfMsV92rT49PHHh7xXXx2mY/11L7kk/EBiTSXuYd5XXhlq42ah+2T37vHuai1ahDK4h+VA/E5d9/hBKnbRt7Q0HAhi1q8Py/z61+Ple/XVkC+2jMsvd//3v8M8LrwwBP1HHw3NQf36ub/zTpjXk0/Ga0ixZpnYPC++OOR58MHQZJS44x9ySHjt3j308unWLT5dXOx+9tnJ+1MXFe3YzGLmfvTRoYkttg3LysJB6JNP3OfMCWcTyX6AJSWN1w2wMYd0TUmxO6l3Vs20RYv4vNu3Dz3DYr+pZGdi06bVv4dMmzahwrL77um/i/vuy259awfUbL+jjh1D5eu+++JngQ3p9WNW94uqjRbcgWLgLWA/oCXwCtC3Vp4RQJtofCLwu0zzzXVwjwXYWDNLzJNPhu6Qbdu6X3BBSKuqCjvm4sWhqQVCjbW2WACFUEut7eijw3uxZh0I7f/uoS0fQndF9xBYYzX7u+6K16Ih3mwS88UXIcjGzhYeeCA8IO0f/4jnWb48vHfDDfG0F18MaS1bug8e7H7vvTuW+ayzau6Mhx8eXrt1C+NbtoR83/52CCaJvYFatgwHgjffdB8yJBw0f/e7eHMRhGBwwAHxZwMNH+6+dKn7zTe7f//7odkJ4geghx5y/9a3wvj48aG301e+Es60YmdGZu4/+IH71Klh+ogjaq5Dp04hSFxwQZi+8MKafaX79w9ljR0wdtstnPFkc6YQG1q3rntNtaTE/eCDM+crLnY/7LD4ASxZgC8qCmc4Y8eGg3+6ANS6tfu4cdmVMbaPtW4d5hm7vlQ7X//+7kcdFboHv/lmvDfakUfu+FynVOuYLD2bQJrtzYLnnRcO5qNGxZsck+X7z/9033//+HS7diHvUUfFz4jrM9S+0SpbjRncjwDmJkz/GPhxmvyDgOczzTfXwf3NN8OOUvu0PGb16pq12JjYKdw+++z43pYt4b1evZLP88ILQ+3ePX528NZbYXrZsnAASRSrMc+fH7ouxnaKiork8//yy1ArTWXlypoXYbdtCzXgxIBf2xNPxH/MxxwTxnv0CJ+NdS11D9cUvvvdcA3jnnvCWcG//hVq2126hM9NnRrP//TTIXjfdVd8vc4+O/l3vmZNODv461/DQXPNmhDYV6zYMW9lZc3moH/8w33DhlD2O+4IwXP48NC0lHiQPeGE0K314YfdFy0KF8BLS0Mw69Mn5OnYMTSpZRO0S0pCEIg1LSU7k9h773hNN3ZQzDTvli1DLXPvvcPZ3LXXhm1YOzAWFYWKSOfOYXuVldV8P/FmoUxnJO3ahabMm26KB7k993S/7LJwfWfUqHBG97//Gw7et9wSHu8ROwDF5hNrjkwVRIuLQ1PknXe6DxzoftBB4aDwf/8XzqqffDKcCS5YEM68E4P5eeeFfelnPwv74cEHpz+ItGoV1qFNm3B2OGZMODOPfS+dO4f98YQT4tvqiSdC2SZODM195eXh7PTee8OZ6p57xrd17YNQUVH4HcQOPHvuWf9uzY0Z3McCv02YPgv4VZr8vwKuyTTfXAd39xAka7cLZ1JVFYLhnDnJ31+zxv3DD5O/99ln8WaaU04JP6pkwSzm4YdDbXHDhhA8b73V/fXX61beTLZvT/8dbN4cftxHHhkOXjNnhhp/tl5/PdSKiorCD7S2qir3p54KvZISDxa7yrhx4aCbbNkbNoSmN/ewr1RUhIPjZ5+Fg9MNN4RrH+ecEw5isSAQe7x0Mhs3hma8DRtqpldVxQ9iL74YmuVKS0Mg7NEjBIJkZ4Mxb73l/t578W356afhYF9VFU/78MOQnmjevHCQvf32cAdz7V5Iu+1WMwhVVYUzxWx/N++8Ew6qc+aE8m/cGNbzhhtCkIwd3Lp1q3uwe/PN+PZJJd2NgInfTSpffun+0kuZe6YlW+7OesZMToI7cCbwAtAqxfsTgIXAwh49ejTe2uah558PO30muQh4tf3+9+HaQn29806odUvTl29Pd8xGoa1TtsE94x9km9kRwPXu/o1o+sdR//if18p3HHAnMNzdP047Uxr3D7JFRJqLxvyD7AXAgWbWy8xaAuOA2bUWNgi4BxidTWAXEZGdK2Nwd/dK4FJgLrAMmOXuS81sspmNjrLdArQDfm9mL5vZ7BSzExGRXaBFNpncfQ4wp1badQnjxzVyuUREpAEK+sFhIiLNlYK7iEgBUnAXESlACu4iIgVIwV1EpABlvIlppy3YbC1QUc+PdwHWNWJxcknr0jRpXZomrUt46m7XTJlyFtwbwswWZnOHVj7QujRNWpemSeuSPTXLiIgUIAV3EZEClK/B/d5cF6ARaV2aJq1L06R1yVJetrmLiEh6+VpzFxGRNBTcRUQKUN4FdzMbaWbLzWylmU3KdXnqysxWmdmr0aORF0Zpe5jZX8xsRfTaKdflTMbM7jOzj83stYS0pGW34I5oOy0xs8G5K/mOUqzL9Wb2frRtXjazUQnv/Thal+Vm9o3clHpHZtbdzOaZ2etmttTMvh+l5912SbMu+bhdWpvZS2b2SrQuN0TpvczsxajMv4v+IwMzaxVNr4zeL2twIbL5u6amMgDFwFvAfkBL4BWgb67LVcd1WAV0qZV2MzApGp8E3JTrcqYo+1HAYOC1TGUHRgFPAQYcDryY6/JnsS7XA1cmyds32tdaAb2ifbA41+sQlW0fYHA03h54Mypv3m2XNOuSj9vFgHbReAnwYvR9zwLGRem/BiZG4xcDv47GxwG/a2gZ8q3mPgRY6e5vu/tWYCYwJsdlagxjgAei8QeAb+awLCm5+3zgk1rJqco+Boj9VfQLQEcz22fXlDSzFOuSyhhgprv/293fAVYS9sWcc/c17r44Gv+c8Ic63cjD7ZJmXVJpytvF3X1TNFkSDQ4cAzwapdfeLrHt9ShwrJlZQ8qQb8G9G/BewvRq0m/8psiBP5vZIjObEKXt5e5rovEPgb1yU7R6SVX2fN1Wl0bNFfclNI/lxbpEp/KDCLXEvN4utdYF8nC7mFmxmb0MfAz8hXBmscHDv9tBzfJWr0v0/kagc0OWn2/BvRB8zd0HAycAl5jZUYlvejgvy8v+qflc9sjdwP7AIcAa4Be5LU72zKwd8BjwA3f/LPG9fNsuSdYlL7eLu29390OAUsIZRe9dufx8C+7vA90TpkujtLzh7u9Hrx8DjxM2+kexU+PoNZ/+ZDxV2fNuW7n7R9EPsgr4DfFT/Ca9LmZWQgiGM9z9f6PkvNwuydYlX7dLjLtvAOYBRxCawWJ/b5pY3up1id7vAKxvyHLzLbgvAA6Mrji3JFx4yJs/4zaztmbWPjYOHA+8RliHc6Js5wB/yE0J6yVV2WcDZ0e9Mw4HNiY0EzRJtdqeTyZsGwjrMi7q0dALOBB4aVeXL5moXfa/gWXufmvCW3m3XVKtS55ul65m1jEa3w34OuEawjxgbJSt9naJba+xwN+jM676y/VV5XpchR5FuIr+FnB1rstTx7LvR7i6/wqwNFZ+Qtva34AVwF+BPXJd1hTlf4RwWryN0F74nVRlJ/QWmBZtp1eB8lyXP4t1eSgq65Lox7ZPQv6ro3VZDpyQ6/InlOtrhCaXJcDL0TAqH7dLmnXJx+0yEPhXVObXgOui9P0IB6CVwO+BVlF662h6ZfT+fg0tgx4/ICJSgPKtWUZERLKg4C4iUoAU3EVECpCCu4hIAVJwFxEpQAruIiIFSMFdRKQA/X+OUOi9oscC3AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PELM_s Result\n",
      "Accuracy : 0.9225806451612903\n",
      "AUC : 0.9219642336070699\n",
      "Sensitivity : 0.937888198757764\n",
      "Specificity : 0.9060402684563759\n",
      "F1 : 0.9263803680981596\n",
      "MCC : 0.8450564787009021\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Plot the training accuracy model Dense\n",
    "\n",
    "accuracy = model_Dense_train.history['acc']\n",
    "val_accuracy = model_Dense_train.history['val_acc']\n",
    "loss = model_Dense_train.history['loss']\n",
    "val_loss = model_Dense_train.history['val_loss']\n",
    "epochs = range(len(accuracy))\n",
    "plt.plot(epochs, accuracy, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
    "plt.title('Dense Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Dense Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "y_pred = np.argmax(model_Dense.predict(valid_X), axis=1)\n",
    "y_true = np.argmax(valid_Y, axis = 1)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "f1 = f1_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
    "auc = roc_auc_score(y_true, y_pred, average='macro', sample_weight=None, max_fpr=None)\n",
    "sensi = tp/(tp+fn)\n",
    "specificity = tn/(tn+fp)\n",
    "accu = (tn + tp)/(tn + tp + fn + fp)\n",
    "mcc = ((tp*tn)-(fp*fn))/np.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n",
    "print('{} Result'.format(string_name))\n",
    "print('Accuracy :', accu)\n",
    "print('AUC :', auc)\n",
    "print('Sensitivity :', sensi)\n",
    "print('Specificity :', specificity)\n",
    "print('F1 :', f1)\n",
    "print('MCC :', mcc)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library Transformer\n",
    "\n",
    "def GetPosEncodingMatrix(max_len, d_emb):\n",
    "    pos_enc = np.array([\n",
    "        [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)] \n",
    "        if pos != 0 else np.zeros(d_emb) \n",
    "            for pos in range(max_len)\n",
    "            ])\n",
    "    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2]) # dim 2i\n",
    "    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2]) # dim 2i+1\n",
    "    return pos_enc\n",
    "\n",
    "class LayerNormalization(Layer):\n",
    "\tdef __init__(self, eps=1e-6, **kwargs):\n",
    "\t\tself.eps = eps\n",
    "\t\tsuper(LayerNormalization, self).__init__(**kwargs)\n",
    "\tdef build(self, input_shape):\n",
    "\t\tself.gamma = self.add_weight(name='gamma', shape=input_shape[-1:],\n",
    "\t\t\t\t\t\t\t\t\t initializer=Ones(), trainable=True)\n",
    "\t\tself.beta = self.add_weight(name='beta', shape=input_shape[-1:],\n",
    "\t\t\t\t\t\t\t\t\tinitializer=Zeros(), trainable=True)\n",
    "\t\tsuper(LayerNormalization, self).build(input_shape)\n",
    "\tdef call(self, x):\n",
    "\t\tmean = K.mean(x, axis=-1, keepdims=True)\n",
    "\t\tstd = K.std(x, axis=-1, keepdims=True)\n",
    "\t\treturn self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
    "\tdef compute_output_shape(self, input_shape):\n",
    "\t\treturn input_shape\n",
    "    \n",
    "class ScaledDotProductAttention():\n",
    "\tdef __init__(self, d_model, attn_dropout=0.1):\n",
    "\t\tself.temper = np.sqrt(d_model)\n",
    "\t\tself.dropout = Dropout(attn_dropout)\n",
    "\tdef __call__(self, q, k, v, mask):\n",
    "\t\tattn = Lambda(lambda x:K.batch_dot(x[0],x[1],axes=[2,2])/self.temper)([q, k])\n",
    "\t\tif mask is not None:\n",
    "\t\t\tmmask = Lambda(lambda x:(-1e+10)*(1-x))(mask)\n",
    "\t\t\tattn = Add()([attn, mmask])\n",
    "\t\tattn = Activation('softmax')(attn)\n",
    "\t\tattn = self.dropout(attn)\n",
    "\t\toutput = Lambda(lambda x:K.batch_dot(x[0], x[1]))([attn, v])\n",
    "\t\treturn output, attn\n",
    "    \n",
    "class MultiHeadAttention():\n",
    "\t# mode 0 - big martixes, faster; mode 1 - more clear implementation\n",
    "\tdef __init__(self, n_head, d_model, d_k, d_v, dropout, mode=0, use_norm=True):\n",
    "\t\tself.mode = mode\n",
    "\t\tself.n_head = n_head\n",
    "\t\tself.d_k = d_k\n",
    "\t\tself.d_v = d_v\n",
    "\t\tself.dropout = dropout\n",
    "\t\tif mode == 0:\n",
    "\t\t\tself.qs_layer = Dense(n_head*d_k, use_bias=False)\n",
    "\t\t\tself.ks_layer = Dense(n_head*d_k, use_bias=False)\n",
    "\t\t\tself.vs_layer = Dense(n_head*d_v, use_bias=False)\n",
    "\t\telif mode == 1:\n",
    "\t\t\tself.qs_layers = []\n",
    "\t\t\tself.ks_layers = []\n",
    "\t\t\tself.vs_layers = []\n",
    "\t\t\tfor _ in range(n_head):\n",
    "\t\t\t\tself.qs_layers.append(TimeDistributed(Dense(d_k, use_bias=False)))\n",
    "\t\t\t\tself.ks_layers.append(TimeDistributed(Dense(d_k, use_bias=False)))\n",
    "\t\t\t\tself.vs_layers.append(TimeDistributed(Dense(d_v, use_bias=False)))\n",
    "\t\tself.attention = ScaledDotProductAttention(d_model)\n",
    "\t\tself.layer_norm = LayerNormalization() if use_norm else None\n",
    "\t\tself.w_o = TimeDistributed(Dense(d_model))\n",
    "\n",
    "\tdef __call__(self, q, k, v, mask=None):\n",
    "\t\td_k, d_v = self.d_k, self.d_v\n",
    "\t\tn_head = self.n_head\n",
    "\n",
    "\t\tif self.mode == 0:\n",
    "\t\t\tqs = self.qs_layer(q)  # [batch_size, len_q, n_head*d_k]\n",
    "\t\t\tks = self.ks_layer(k)\n",
    "\t\t\tvs = self.vs_layer(v)\n",
    "\n",
    "\t\t\tdef reshape1(x):\n",
    "\t\t\t\ts = tf.shape(x)   # [batch_size, len_q, n_head * d_k]\n",
    "\t\t\t\tx = tf.reshape(x, [s[0], s[1], n_head, s[2]//n_head])\n",
    "\t\t\t\tx = tf.transpose(x, [2, 0, 1, 3])  \n",
    "\t\t\t\tx = tf.reshape(x, [-1, s[1], s[2]//n_head])  # [n_head * batch_size, len_q, d_k]\n",
    "\t\t\t\treturn x\n",
    "\t\t\tqs = Lambda(reshape1)(qs)\n",
    "\t\t\tks = Lambda(reshape1)(ks)\n",
    "\t\t\tvs = Lambda(reshape1)(vs)\n",
    "\n",
    "\t\t\tif mask is not None:\n",
    "\t\t\t\tmask = Lambda(lambda x:K.repeat_elements(x, n_head, 0))(mask)\n",
    "\t\t\thead, attn = self.attention(qs, ks, vs, mask=mask)  \n",
    "\t\t\t\t\n",
    "\t\t\tdef reshape2(x):\n",
    "\t\t\t\ts = tf.shape(x)   # [n_head * batch_size, len_v, d_v]\n",
    "\t\t\t\tx = tf.reshape(x, [n_head, -1, s[1], s[2]]) \n",
    "\t\t\t\tx = tf.transpose(x, [1, 2, 0, 3])\n",
    "\t\t\t\tx = tf.reshape(x, [-1, s[1], n_head*d_v])  # [batch_size, len_v, n_head * d_v]\n",
    "\t\t\t\treturn x\n",
    "\t\t\thead = Lambda(reshape2)(head)\n",
    "\t\telif self.mode == 1:\n",
    "\t\t\theads = []; attns = []\n",
    "\t\t\tfor i in range(n_head):\n",
    "\t\t\t\tqs = self.qs_layers[i](q)   \n",
    "\t\t\t\tks = self.ks_layers[i](k) \n",
    "\t\t\t\tvs = self.vs_layers[i](v) \n",
    "\t\t\t\thead, attn = self.attention(qs, ks, vs, mask)\n",
    "\t\t\t\theads.append(head); attns.append(attn)\n",
    "\t\t\thead = Concatenate()(heads) if n_head > 1 else heads[0]\n",
    "\t\t\tattn = Concatenate()(attns) if n_head > 1 else attns[0]\n",
    "\n",
    "\t\toutputs = self.w_o(head)\n",
    "\t\toutputs = Dropout(self.dropout)(outputs)\n",
    "\t\tif not self.layer_norm: return outputs, attn\n",
    "\t\toutputs = Add()([outputs, q])\n",
    "\t\treturn self.layer_norm(outputs), attn\n",
    "\n",
    "class PositionwiseFeedForward():\n",
    "    def __init__(self, d_hid, d_inner_hid, dropout=0.1):\n",
    "        self.w_1 = Conv1D(d_inner_hid, 1, activation='relu')\n",
    "        self.w_2 = Conv1D(d_hid, 1)\n",
    "        self.layer_norm = LayerNormalization()\n",
    "        self.dropout = Dropout(dropout)\n",
    "    def __call__(self, x):\n",
    "        output = self.w_1(x) \n",
    "        output = self.w_2(output)\n",
    "        output = self.dropout(output)\n",
    "        output = Add()([output, x])\n",
    "        output = self.layer_norm(output)\n",
    "        return output\n",
    "    \n",
    "class EncoderLayer():\n",
    "    def __init__(self, d_model, d_inner_hid, n_head, d_k, d_v, dropout=0.1):\n",
    "        self.self_att_layer = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "        self.pos_ffn_layer  = PositionwiseFeedForward(d_model, d_inner_hid, dropout=dropout)\n",
    "    def __call__(self, enc_input, mask=None):\n",
    "        output, slf_attn = self.self_att_layer(enc_input, enc_input, enc_input, mask=mask)\n",
    "        output = self.pos_ffn_layer(output)\n",
    "        return output, slf_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformer\n",
    "\n",
    "epochs = 100\n",
    "d_model = 16\n",
    "d_emb = d_model\n",
    "d_inner_hid = 2*d_model\n",
    "n_head = 4\n",
    "d_k = d_model//4\n",
    "d_v = d_model//4\n",
    "dropout = 0.5\n",
    "\n",
    "inp = Input(shape=(9,))\n",
    "\n",
    "emb = Embedding(20, d_emb, input_length=9)(inp)\n",
    "\n",
    "out, slf_attn = EncoderLayer(d_emb, d_inner_hid, n_head, d_k, d_v, dropout)(emb)\n",
    "out, slf_attn = EncoderLayer(d_emb, d_inner_hid, n_head, d_k, d_v, dropout)(out)\n",
    "out, slf_attn = EncoderLayer(d_emb, d_inner_hid, n_head, d_k, d_v, dropout)(out)\n",
    "out, slf_attn = EncoderLayer(d_emb, d_inner_hid, n_head, d_k, d_v, dropout)(out)\n",
    "out, slf_attn = EncoderLayer(d_emb, d_inner_hid, n_head, d_k, d_v, dropout)(out)\n",
    "out, slf_attn = EncoderLayer(d_emb, d_inner_hid, n_head, d_k, d_v, dropout)(out)\n",
    "out = GlobalAveragePooling1D()(out)\n",
    "\n",
    "out = Dense(2, activation='softmax')(out)\n",
    "\n",
    "model_transformer = Model(inputs=inp, outputs=out)\n",
    "model_transformer.compile(loss='categorical_crossentropy', optimizer='sgd',metrics=['accuracy'])\n",
    "\n",
    "# model.summary()\n",
    "# Callback\n",
    "\n",
    "def step_decay(epoch):\n",
    "   if (0 <= epoch <= 30):\n",
    "    lrate = 1e-1\n",
    "   elif (30 < epoch <= 60):\n",
    "    lrate = 1e-2\n",
    "   elif (60 < epoch):\n",
    "    lrate = 1e-3\n",
    "\n",
    "   return lrate\n",
    "lrate = LearningRateScheduler(step_decay, verbose=1)\n",
    "callbacks_list = [lrate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2477 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 0.1.\n",
      "2477/2477 [==============================] - 61s 25ms/step - loss: 0.7293 - acc: 0.5188 - val_loss: 0.6966 - val_acc: 0.4806\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: LearningRateScheduler setting learning rate to 0.1.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.7072 - acc: 0.5176 - val_loss: 0.7320 - val_acc: 0.5194\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: LearningRateScheduler setting learning rate to 0.1.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.7071 - acc: 0.5079 - val_loss: 0.6961 - val_acc: 0.4806\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: LearningRateScheduler setting learning rate to 0.1.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.6453 - acc: 0.6193 - val_loss: 0.4909 - val_acc: 0.8339\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: LearningRateScheduler setting learning rate to 0.1.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.4238 - acc: 0.8280 - val_loss: 0.3407 - val_acc: 0.8855\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: LearningRateScheduler setting learning rate to 0.1.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.3431 - acc: 0.8652 - val_loss: 0.3603 - val_acc: 0.8823\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: LearningRateScheduler setting learning rate to 0.1.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.3269 - acc: 0.8736 - val_loss: 0.3531 - val_acc: 0.8887\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: LearningRateScheduler setting learning rate to 0.1.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.3211 - acc: 0.8724 - val_loss: 0.5993 - val_acc: 0.7468\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: LearningRateScheduler setting learning rate to 0.1.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.3061 - acc: 0.8833 - val_loss: 0.4382 - val_acc: 0.7726\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: LearningRateScheduler setting learning rate to 0.1.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.3021 - acc: 0.8793 - val_loss: 0.3684 - val_acc: 0.8452\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: LearningRateScheduler setting learning rate to 0.1.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2930 - acc: 0.8898 - val_loss: 0.2795 - val_acc: 0.8887\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: LearningRateScheduler setting learning rate to 0.1.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2909 - acc: 0.8821 - val_loss: 0.3539 - val_acc: 0.8677\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: LearningRateScheduler setting learning rate to 0.1.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2909 - acc: 0.8837 - val_loss: 0.3046 - val_acc: 0.8871\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: LearningRateScheduler setting learning rate to 0.1.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2903 - acc: 0.8878 - val_loss: 0.2724 - val_acc: 0.8871\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: LearningRateScheduler setting learning rate to 0.1.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2925 - acc: 0.8809 - val_loss: 0.3172 - val_acc: 0.8790\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: LearningRateScheduler setting learning rate to 0.1.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2846 - acc: 0.8918 - val_loss: 0.2850 - val_acc: 0.8839\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: LearningRateScheduler setting learning rate to 0.1.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2823 - acc: 0.8890 - val_loss: 0.3085 - val_acc: 0.8952\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 00018: LearningRateScheduler setting learning rate to 0.1.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2821 - acc: 0.8825 - val_loss: 0.2952 - val_acc: 0.8919\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 00019: LearningRateScheduler setting learning rate to 0.1.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2823 - acc: 0.8870 - val_loss: 0.2728 - val_acc: 0.8806\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 00020: LearningRateScheduler setting learning rate to 0.1.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2811 - acc: 0.8849 - val_loss: 0.2560 - val_acc: 0.8984\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 00021: LearningRateScheduler setting learning rate to 0.1.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2704 - acc: 0.8890 - val_loss: 0.2576 - val_acc: 0.8887\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 00022: LearningRateScheduler setting learning rate to 0.1.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2780 - acc: 0.8825 - val_loss: 0.2510 - val_acc: 0.8887\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 00023: LearningRateScheduler setting learning rate to 0.1.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2827 - acc: 0.8870 - val_loss: 0.2771 - val_acc: 0.8855\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 00024: LearningRateScheduler setting learning rate to 0.1.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2808 - acc: 0.8886 - val_loss: 0.2636 - val_acc: 0.8855\n",
      "Epoch 25/100\n",
      "\n",
      "Epoch 00025: LearningRateScheduler setting learning rate to 0.1.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2737 - acc: 0.8829 - val_loss: 0.2740 - val_acc: 0.8984\n",
      "Epoch 26/100\n",
      "\n",
      "Epoch 00026: LearningRateScheduler setting learning rate to 0.1.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2808 - acc: 0.8902 - val_loss: 0.2813 - val_acc: 0.8855\n",
      "Epoch 27/100\n",
      "\n",
      "Epoch 00027: LearningRateScheduler setting learning rate to 0.1.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2760 - acc: 0.8845 - val_loss: 0.2615 - val_acc: 0.8855\n",
      "Epoch 28/100\n",
      "\n",
      "Epoch 00028: LearningRateScheduler setting learning rate to 0.1.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2656 - acc: 0.8894 - val_loss: 0.2559 - val_acc: 0.8871\n",
      "Epoch 29/100\n",
      "\n",
      "Epoch 00029: LearningRateScheduler setting learning rate to 0.1.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2689 - acc: 0.8874 - val_loss: 0.2449 - val_acc: 0.8839\n",
      "Epoch 30/100\n",
      "\n",
      "Epoch 00030: LearningRateScheduler setting learning rate to 0.1.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2672 - acc: 0.8910 - val_loss: 0.2510 - val_acc: 0.8855\n",
      "Epoch 31/100\n",
      "\n",
      "Epoch 00031: LearningRateScheduler setting learning rate to 0.1.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2750 - acc: 0.8902 - val_loss: 0.3520 - val_acc: 0.8452\n",
      "Epoch 32/100\n",
      "\n",
      "Epoch 00032: LearningRateScheduler setting learning rate to 0.01.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2678 - acc: 0.8902 - val_loss: 0.2495 - val_acc: 0.9000\n",
      "Epoch 33/100\n",
      "\n",
      "Epoch 00033: LearningRateScheduler setting learning rate to 0.01.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2589 - acc: 0.8979 - val_loss: 0.2475 - val_acc: 0.8968\n",
      "Epoch 34/100\n",
      "\n",
      "Epoch 00034: LearningRateScheduler setting learning rate to 0.01.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2561 - acc: 0.8942 - val_loss: 0.2551 - val_acc: 0.9000\n",
      "Epoch 35/100\n",
      "\n",
      "Epoch 00035: LearningRateScheduler setting learning rate to 0.01.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2606 - acc: 0.8934 - val_loss: 0.2482 - val_acc: 0.8984\n",
      "Epoch 36/100\n",
      "\n",
      "Epoch 00036: LearningRateScheduler setting learning rate to 0.01.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2597 - acc: 0.8938 - val_loss: 0.2495 - val_acc: 0.8984\n",
      "Epoch 37/100\n",
      "\n",
      "Epoch 00037: LearningRateScheduler setting learning rate to 0.01.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2583 - acc: 0.8950 - val_loss: 0.2579 - val_acc: 0.8984\n",
      "Epoch 38/100\n",
      "\n",
      "Epoch 00038: LearningRateScheduler setting learning rate to 0.01.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2587 - acc: 0.8906 - val_loss: 0.2529 - val_acc: 0.9016\n",
      "Epoch 39/100\n",
      "\n",
      "Epoch 00039: LearningRateScheduler setting learning rate to 0.01.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2556 - acc: 0.8918 - val_loss: 0.2555 - val_acc: 0.9000\n",
      "Epoch 40/100\n",
      "\n",
      "Epoch 00040: LearningRateScheduler setting learning rate to 0.01.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2595 - acc: 0.8934 - val_loss: 0.2502 - val_acc: 0.8968\n",
      "Epoch 41/100\n",
      "\n",
      "Epoch 00041: LearningRateScheduler setting learning rate to 0.01.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2559 - acc: 0.8926 - val_loss: 0.2529 - val_acc: 0.8984\n",
      "Epoch 42/100\n",
      "\n",
      "Epoch 00042: LearningRateScheduler setting learning rate to 0.01.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2589 - acc: 0.8878 - val_loss: 0.2533 - val_acc: 0.9000\n",
      "Epoch 43/100\n",
      "\n",
      "Epoch 00043: LearningRateScheduler setting learning rate to 0.01.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2563 - acc: 0.8930 - val_loss: 0.2491 - val_acc: 0.8935\n",
      "Epoch 44/100\n",
      "\n",
      "Epoch 00044: LearningRateScheduler setting learning rate to 0.01.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2571 - acc: 0.8938 - val_loss: 0.2509 - val_acc: 0.9000\n",
      "Epoch 45/100\n",
      "\n",
      "Epoch 00045: LearningRateScheduler setting learning rate to 0.01.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2565 - acc: 0.8902 - val_loss: 0.2559 - val_acc: 0.8984\n",
      "Epoch 46/100\n",
      "\n",
      "Epoch 00046: LearningRateScheduler setting learning rate to 0.01.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2567 - acc: 0.8946 - val_loss: 0.2547 - val_acc: 0.9000\n",
      "Epoch 47/100\n",
      "\n",
      "Epoch 00047: LearningRateScheduler setting learning rate to 0.01.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2560 - acc: 0.8934 - val_loss: 0.2566 - val_acc: 0.8968\n",
      "Epoch 48/100\n",
      "\n",
      "Epoch 00048: LearningRateScheduler setting learning rate to 0.01.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2542 - acc: 0.8918 - val_loss: 0.2513 - val_acc: 0.8984\n",
      "Epoch 49/100\n",
      "\n",
      "Epoch 00049: LearningRateScheduler setting learning rate to 0.01.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2569 - acc: 0.8882 - val_loss: 0.2616 - val_acc: 0.8968\n",
      "Epoch 50/100\n",
      "\n",
      "Epoch 00050: LearningRateScheduler setting learning rate to 0.01.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2552 - acc: 0.8906 - val_loss: 0.2570 - val_acc: 0.8968\n",
      "Epoch 51/100\n",
      "\n",
      "Epoch 00051: LearningRateScheduler setting learning rate to 0.01.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2547 - acc: 0.8922 - val_loss: 0.2515 - val_acc: 0.8919\n",
      "Epoch 52/100\n",
      "\n",
      "Epoch 00052: LearningRateScheduler setting learning rate to 0.01.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2551 - acc: 0.8922 - val_loss: 0.2632 - val_acc: 0.8919\n",
      "Epoch 53/100\n",
      "\n",
      "Epoch 00053: LearningRateScheduler setting learning rate to 0.01.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2615 - acc: 0.8870 - val_loss: 0.2480 - val_acc: 0.8935\n",
      "Epoch 54/100\n",
      "\n",
      "Epoch 00054: LearningRateScheduler setting learning rate to 0.01.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2546 - acc: 0.8946 - val_loss: 0.2613 - val_acc: 0.8919\n",
      "Epoch 55/100\n",
      "\n",
      "Epoch 00055: LearningRateScheduler setting learning rate to 0.01.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2546 - acc: 0.8966 - val_loss: 0.2551 - val_acc: 0.8984\n",
      "Epoch 56/100\n",
      "\n",
      "Epoch 00056: LearningRateScheduler setting learning rate to 0.01.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2538 - acc: 0.8987 - val_loss: 0.2583 - val_acc: 0.8952\n",
      "Epoch 57/100\n",
      "\n",
      "Epoch 00057: LearningRateScheduler setting learning rate to 0.01.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2579 - acc: 0.8922 - val_loss: 0.2489 - val_acc: 0.8952\n",
      "Epoch 58/100\n",
      "\n",
      "Epoch 00058: LearningRateScheduler setting learning rate to 0.01.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2575 - acc: 0.8926 - val_loss: 0.2529 - val_acc: 0.8984\n",
      "Epoch 59/100\n",
      "\n",
      "Epoch 00059: LearningRateScheduler setting learning rate to 0.01.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2519 - acc: 0.8918 - val_loss: 0.2584 - val_acc: 0.8984\n",
      "Epoch 60/100\n",
      "\n",
      "Epoch 00060: LearningRateScheduler setting learning rate to 0.01.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2579 - acc: 0.8918 - val_loss: 0.2542 - val_acc: 0.9000\n",
      "Epoch 61/100\n",
      "\n",
      "Epoch 00061: LearningRateScheduler setting learning rate to 0.01.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2570 - acc: 0.8938 - val_loss: 0.2569 - val_acc: 0.8968\n",
      "Epoch 62/100\n",
      "\n",
      "Epoch 00062: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2537 - acc: 0.8922 - val_loss: 0.2561 - val_acc: 0.8984\n",
      "Epoch 63/100\n",
      "\n",
      "Epoch 00063: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2570 - acc: 0.8886 - val_loss: 0.2549 - val_acc: 0.9016\n",
      "Epoch 64/100\n",
      "\n",
      "Epoch 00064: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2541 - acc: 0.8918 - val_loss: 0.2539 - val_acc: 0.9016\n",
      "Epoch 65/100\n",
      "\n",
      "Epoch 00065: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2531 - acc: 0.8950 - val_loss: 0.2528 - val_acc: 0.9016\n",
      "Epoch 66/100\n",
      "\n",
      "Epoch 00066: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2507 - acc: 0.8950 - val_loss: 0.2528 - val_acc: 0.9016\n",
      "Epoch 67/100\n",
      "\n",
      "Epoch 00067: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2486 - acc: 0.8930 - val_loss: 0.2530 - val_acc: 0.9016\n",
      "Epoch 68/100\n",
      "\n",
      "Epoch 00068: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2551 - acc: 0.8902 - val_loss: 0.2529 - val_acc: 0.9016\n",
      "Epoch 69/100\n",
      "\n",
      "Epoch 00069: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2501 - acc: 0.8922 - val_loss: 0.2536 - val_acc: 0.9016\n",
      "Epoch 70/100\n",
      "\n",
      "Epoch 00070: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2510 - acc: 0.8950 - val_loss: 0.2530 - val_acc: 0.9016\n",
      "Epoch 71/100\n",
      "\n",
      "Epoch 00071: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2461 - acc: 0.8930 - val_loss: 0.2529 - val_acc: 0.9016\n",
      "Epoch 72/100\n",
      "\n",
      "Epoch 00072: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2568 - acc: 0.8918 - val_loss: 0.2527 - val_acc: 0.9016\n",
      "Epoch 73/100\n",
      "\n",
      "Epoch 00073: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2546 - acc: 0.8930 - val_loss: 0.2518 - val_acc: 0.9016\n",
      "Epoch 74/100\n",
      "\n",
      "Epoch 00074: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2529 - acc: 0.8914 - val_loss: 0.2525 - val_acc: 0.9016\n",
      "Epoch 75/100\n",
      "\n",
      "Epoch 00075: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2510 - acc: 0.8946 - val_loss: 0.2527 - val_acc: 0.9016\n",
      "Epoch 76/100\n",
      "\n",
      "Epoch 00076: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2496 - acc: 0.8934 - val_loss: 0.2534 - val_acc: 0.9016\n",
      "Epoch 77/100\n",
      "\n",
      "Epoch 00077: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2505 - acc: 0.8894 - val_loss: 0.2529 - val_acc: 0.9016\n",
      "Epoch 78/100\n",
      "\n",
      "Epoch 00078: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2503 - acc: 0.8942 - val_loss: 0.2536 - val_acc: 0.9000\n",
      "Epoch 79/100\n",
      "\n",
      "Epoch 00079: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2541 - acc: 0.8930 - val_loss: 0.2525 - val_acc: 0.9016\n",
      "Epoch 80/100\n",
      "\n",
      "Epoch 00080: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2551 - acc: 0.8938 - val_loss: 0.2527 - val_acc: 0.9016\n",
      "Epoch 81/100\n",
      "\n",
      "Epoch 00081: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2515 - acc: 0.8946 - val_loss: 0.2529 - val_acc: 0.9000\n",
      "Epoch 82/100\n",
      "\n",
      "Epoch 00082: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2555 - acc: 0.8922 - val_loss: 0.2528 - val_acc: 0.9000\n",
      "Epoch 83/100\n",
      "\n",
      "Epoch 00083: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2548 - acc: 0.8930 - val_loss: 0.2527 - val_acc: 0.9000\n",
      "Epoch 84/100\n",
      "\n",
      "Epoch 00084: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2534 - acc: 0.8930 - val_loss: 0.2523 - val_acc: 0.9000\n",
      "Epoch 85/100\n",
      "\n",
      "Epoch 00085: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2510 - acc: 0.8926 - val_loss: 0.2524 - val_acc: 0.9000\n",
      "Epoch 86/100\n",
      "\n",
      "Epoch 00086: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2536 - acc: 0.8942 - val_loss: 0.2520 - val_acc: 0.9000\n",
      "Epoch 87/100\n",
      "\n",
      "Epoch 00087: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2526 - acc: 0.8906 - val_loss: 0.2525 - val_acc: 0.9000\n",
      "Epoch 88/100\n",
      "\n",
      "Epoch 00088: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2535 - acc: 0.8914 - val_loss: 0.2521 - val_acc: 0.9000\n",
      "Epoch 89/100\n",
      "\n",
      "Epoch 00089: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2554 - acc: 0.8906 - val_loss: 0.2519 - val_acc: 0.9000\n",
      "Epoch 90/100\n",
      "\n",
      "Epoch 00090: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2527 - acc: 0.8914 - val_loss: 0.2523 - val_acc: 0.9000\n",
      "Epoch 91/100\n",
      "\n",
      "Epoch 00091: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2511 - acc: 0.9015 - val_loss: 0.2528 - val_acc: 0.9000\n",
      "Epoch 92/100\n",
      "\n",
      "Epoch 00092: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2534 - acc: 0.8942 - val_loss: 0.2523 - val_acc: 0.9000\n",
      "Epoch 93/100\n",
      "\n",
      "Epoch 00093: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2507 - acc: 0.8926 - val_loss: 0.2521 - val_acc: 0.9000\n",
      "Epoch 94/100\n",
      "\n",
      "Epoch 00094: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2516 - acc: 0.8958 - val_loss: 0.2527 - val_acc: 0.9000\n",
      "Epoch 95/100\n",
      "\n",
      "Epoch 00095: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2522 - acc: 0.8946 - val_loss: 0.2535 - val_acc: 0.9000\n",
      "Epoch 96/100\n",
      "\n",
      "Epoch 00096: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2499 - acc: 0.8946 - val_loss: 0.2533 - val_acc: 0.9000\n",
      "Epoch 97/100\n",
      "\n",
      "Epoch 00097: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2537 - acc: 0.8950 - val_loss: 0.2529 - val_acc: 0.9000\n",
      "Epoch 98/100\n",
      "\n",
      "Epoch 00098: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2564 - acc: 0.8926 - val_loss: 0.2525 - val_acc: 0.9016\n",
      "Epoch 99/100\n",
      "\n",
      "Epoch 00099: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2568 - acc: 0.8934 - val_loss: 0.2527 - val_acc: 0.9000\n",
      "Epoch 100/100\n",
      "\n",
      "Epoch 00100: LearningRateScheduler setting learning rate to 0.001.\n",
      "2477/2477 [==============================] - 3s 1ms/step - loss: 0.2533 - acc: 0.8942 - val_loss: 0.2524 - val_acc: 0.9016\n"
     ]
    }
   ],
   "source": [
    "# Training model transformer\n",
    "\n",
    "model_transformer_train = model_transformer.fit(train_X, train_Y, epochs=epochs, batch_size=32, validation_data=(valid_X, valid_Y), \n",
    "   callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEICAYAAABfz4NwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJztnXl8FtXZ978XYQlhJ0RBkAQVZQ9LRNzYFEUe61YUMLgr1adq9Wnti4UqxdLXpSr6lFpRwQ1FrBtWLYqitq9aCbugCCJKADEssgiKgev948wkkzv3ljv3nYVc389nPjNz5syZa87MnN/ZR1QVwzAMw6hX3QYYhmEYNQMTBMMwDAMwQTAMwzA8TBAMwzAMwATBMAzD8DBBMAzDMAAThFqFiJwqImtFZI+InF3d9lQ1IvK4iPw22X6rExH5SETGpiDcb0TkFG/7DyLyl3j8JnCd00VkWaJ2GjWL+tVtQE1HRPYEdjOAH4ED3v4vVHVWFZrzR+B+VZ1WhddMCBF5FBjt7TYEBBd3AAtU9WcVDVNVL0+F30MdVb09GeGISDqwDzhSVQu9sOcDuckI36h+TBBioKpN/W0RWQ9c7X0EYRGR+qpanCJzsoGViZyYYrvKha+qVwNXe8f+CHSIlkin2j7DqAh19X20KqNKIiJ/FJHnRORZEdkNjBWRE72qgO9EZLOIPCgiDTz/9UVEReQXXvXPDhF5MBDesSLyvojsFJGtIvKM574e6Ai84VUZpYlIBxH5h4hsF5E1InJlDLv+KCKzPbc9IrJMRI4WkYkiUiQiX4vI6YEwWorITO8eCkVksojU845d7dn5oIhsByZWMN66iEixiFwjIhuA1724eUFEtnhxt0BEjgucM1tEJnrbw734+51n+0YRyU/Q72Ei8oaI7PKe250iElb047RxqojME5HdIvL/RCQ7cPy/vGf1nYjcFyV+ckTkexFpFnA70XsWaV78ves9+yIReSLoNySsO70Sm79/lfesi0TklhC/J4vIfzz7NonI/SLiZxzf99arvffnPD9uA+f3FJF/eecvF5Gz4o2bCsZzE+/d2yDuW3nPt1NEBnvPcad3nxd77mWq50TkWv85i0i6uO/yOhH5AvjEc3/Ie/d3icjHIjIgxMbbRWSdd3yhiLQVkcdEZErI/bwpIteFu9eahAlCcjgfeAZoATwHFAO/AtoAJwPDgV+EnDMC6Af0wSXWfkI8BXgNaAV0AKYBqGoOsAk4S1WbquoB71pfAkcAo4C7RWRQFLsAzgUeA1riShvzPXvbAf8XeChw/lO4KoKjPVv/C7gicPwk4FMgC7grjngKJQ04ATjOswvgFe96bYHPgCeinJ+Nq4o6Arge+JuINE3A73SgCDgcGAdcFsPuWDZeDNwKtAY2A38AEJF2wBzg17g4KwLywl1AVdcDy4DzQsJ9znv2AJM9G3ri4nBCDLsRkT7AVNz70gHIwb2nPj/h4icTOBX4GV5JDxjorY/z3sGXQ8JOx727L3v3dwvwvIh0CrmHcnETgWjx/CDQBTjeC2sioCJyDPAP4B7vHvpRsVL12ZR+lwAf4uI307PnefEyd959nAecgfuexgE/eHZeLCICICJHAKdQ+g3WXFTVljgXYD1weojbH4F3Ypz3G+B5b7s+oMCAwPEXgd9428/gEuX2YcIpBAZ7251wH2+TwPF7gEcj2eW5vRHYPx/YCdTz9lt5tjUF2uPEoFHA/yXAW9721cC6OOPtj8DjIW5dvGsdEeW8tsBBIN3bnw1M9LaHB2333HYBvSviF0j3rpEdOPZnYH6c9xbOxr8Ejl8ALPW2xwHvBo6lAd8CYyOEfT3wesDvFqB/BL+jgQ8D+98Ap3jbdwbeiz8FnwUus3DQ9xsm3PHAs952uvfMOgSODwfWetvDgK8ACRx/CRgfK24qEs9AA9y7f1wYf3/w7Q1z7KNgXAPX+s85cG8nRbFBgL3+db17PTOCv3XAqVr6/b8Yz31W92IlhOSwIbjjFedfE9d7YxcuJ9cm5JxvAtt7cYkwuNxjA6BARFaISKTc6hHAVlX9PuD2FS4hD2uXx5bA9j6gSFUPBvbxbMkGGgF+kf07XGnl8BjhV4SDqrrJ3/GK4Pf6RXBcrlBwubNwBG2HsvEYr9+23jUKA8ci3lecNkZ6tkcEw1aX098Y6Vq40sQQEWkDnA7sUtWPPTuOEJHnxVV/7QIepfw7Fo5QG3bixNK/v27iqs+2eOHeFme4fthfq5cKeoS+k5Hipgwx4rkdLmP1RZhTj4zgHi+h3/KtIrJaRHYCO3DC0cbL/bcPdy3v/p8E/OqpsbjSdo3HBCE5hE4Z+zCuDvIYVW2O+6gkroBUN6vq1araDvglMD2kyO2zCfdiNgm4daRsAlOZqWw34D7Y1qra0luaq2qvJIUf7vwrcLnMIbicaxfPPa64S5BvPDuCidaRUfxXxsbNwbDFtce0j+RZVb/F1duPxFW1PBM4fA/wPdDDe8euTtCGFt59+DwCLAaO9sKdHAg31vPehHsHg4S+k/ESLZ4346o5jw5z3oYI7uDiKyOw3zaMn5J7FJFhwA24knRLXNXUPlwJSHH3FelaTwIjRaQfLr5fi+CvRmGCkBqa4XJd34tIV8q3H0RERC4SET+R+A73gh4I9aeqXwIFwJ9EpJGI9MZ9RE9X1ngv/A3Ae8CfRaS5iNQTkWNEZGCscytBM1wd7DagCa6qKaWo6g/Aq8AfvIbFHrjENxU2zgWOF5GzvXroW3CJTDSewT3X8ygrCM2APcAuEekI/E+cNswBLhCRE0SkkWd/sOTUDNipqntEpDtwjX9AVX/EvddHRQj7X0A9EbnJy+EPw9Wvz4nTtiAR41lVf8IluA+IyOHiGtlPEZE0XE78bBE537MhS0T8TMxSXCKdLiJdgMvjsOEnXFtPQ5w4pgeOP4r7/o4SRx8RaenZuA5YBczEtfvsTyAOqhwThNTwa1zD5G5caaEijUknAAtF5Htc28IvVfXrCH5HAZ1xudy/A79T1XcTNToMY3Ef4ypccfl5wueqksVjuI/vG2AF8O8UXivIL3DVHUW4j/xZSsdMhJKwjaq6GVfXP5XSRuyCGKe9CPTC1dOvDrjfhmuo3Imrp38hThuW4N7Pv+Oqyb4Gtga83AxcLW78zTTKv7u34RpWvxORc0LC/gHXKDsSl5DfB4zyEseKEiueb8RV1yzxrnUHLue+FtdB4XfAdlz8dvfOuRtX1VSE60gQK/P0Kq6E9gWuTWCrd67Pnbic/zu4Nqm/4apZfZ7ANUjXiuoi8Bp/DMMoRUQewDUSx12yM4xQROQM4K+qekx12xIvNjDNqPN41USKKwmdCFwKjKlWo4xajYg0xJViple3LRXBqowMwzVavoprdHwa+KOq/rN6TTJqK1573g5cG0SNn2YmiFUZGYZhGICVEAzDMAyPWtWG0KZNG83JyaluMwzDMGoVixYt2qqqWbH81SpByMnJoaAgVi89wzAMI4iIfBWPP6syMgzDMAATBMMwDMPDBMEwDMMATBAMwzAMDxMEwzAMAzBBMFLMrFmQkwP16rn1rFnVbZFRW7F3KfXUqm6nRu1i1iwYNw727nX7X33l9gHy8yOfZxih2LtUNVgJwUgZEyaUfsA+e/fCr35VmtNr08YttTnXl+qca7LDr4057Ujv0oSYf5E2KkSc/zMdDqwG1uL9HzXkeDbwNrAceBfvn6u4f9Z+iPvJ9XLc3Oj+OY/jfhC/1Ft6x7KjX79+atQ8nn5aNTtbVcStn37auYuoQsWWjIzS82sDTz/tbK7sPUSKw8qEHy7MeMKLZEtl7qOyRHqXRJITfk0kmXEJFGg8aX1MD+7n3l/g/pLUEFgGdAvx8zxwmbc9FHjK2z4W6OxtH4H79V1LLRWEkfEY6S8mCDWPcAlMgwaqmZkVFwN/yc6u7rsqS7QPMzu78vcQLZFONPxIYUZ6Ln54lRWgZIhjOJIVz/5zzMx0S2UT21SJZ7LjMpmCcCIwL7B/K3BriJ+VwJHetuB+Bh4urGUBgTBBOASI9KFWZklWri8ZOazrriufOw1+mNFKQaE5/UREJdGccUWfix9eZRLeZCfasUpK4BL1RMWqsoltuDD95xUskSVS8ktGXAZJpiCMBB4N7F8C/CXEzzPAr7ztCwAFMkP89Ac+BeppqSCs9qqS7gcaxbLFBKH6CX3BK5LoZGfHV3LIzCx7jeuuC1/1ES63528HP85EP/qnn46cIPsfZqw48M9PVFSixWU0KhpmWlr0+w0KVKRnEkts4onvaInk00+Hf39CE+FwxPOuVjSxjRVmgwaqDRuWd4v2DcR6DolmlqpaEI7A/ft1CfAA7l+tLQPH23mJ/4AQN8H9g/QJ4LYI1x+H+y9qQceOHROLDSMpxMplxUoY4wkj3EeUzCVUbKIJRKwPPlKdfEUT9opWr6Wlxa7uiGR7ZmZi9sYSmAYNot9jPETLFUerPgsukUQ/HoGsaGKbiJDHG9dNm1YuLkOp0iqjEP9NgcLAfnNgcbTqIWAw8I9YtlgJoXqpTPVQ6EedlqYlCVQwcatM20MiS7gEJN7ER8TllOP1n6rFz3WGlpAi3evTT5fGf7wJVGUSt3CliXBEu05FbAiXaCZSQohV5ZjqZx6aMaopbQj1gXVAp0CjcvcQP20CVUFTgMnedkNc76ObwoTbzlsLMBW4M5YtJgjVS6IJQ0aGSxTi6d1SHQlqMCGoaI5fpPQeqlrMKro0bVrxqr5U5IIjiXBFRCqWzX6YwWrFeG2KVDUVFNSqeNa+3TWql5ELixHA57jeRhM8t8nAOd72SGCN5+dRvz0AGAv8RGnX0pLupcA7wArgE9x/bJvGssMEIXGS0cCayEfgXyvSuWlpzqYmTVL/gUVbKvOR+/cYrdrElvDP3W+PqEy1W+jiNzRXJEz/nFjnZWbG/5yT8T4kq5dWUgWhpiwmCIkRT2+IaOcmWjT26+urO/Hx77devcjHK5M7TaSB3Zaat2RkRK67r84lGd2w4xUEm7qiDhBulKeqW0eaAmDWLDeieNu2xK+7bVvlzk8m/ucViQMHKhf2V3H9jyox6tWDgwdTF35tpEEDaN48ue9X6DdSU/j666q7lk1dEYXt22H//vLuO3fW3JfH56efYMcOtx3rhfKnk/Dx542pyMfWpIlbDmWaNAGRqrteRgY8/bQTq6q8bmVJta0ibqkpmY1UU69e1U0zYoIQhd694fe/L+t24AAMGABXXlk9NsXLffdBt24u99qxY2z/27a5jywnx4lDvIKXne2usWePm5OousjMdAukJkHKyICHH4annnL3nAqaNHH3IOKuMX16aaktnmcYSr0Ev+7s7MTvUQSGDnU5+FSQkQGtW4fPqFWEtLTk2JPM62ZmuvsL5cCB0lLouHEpFoV46pVqylKVbQg7d7pKhqOOUj14sNT9lVece5Mmqj/8kLzr7dql+vrryQtv7Fhn5/btle8rH63ONdj+UJEeKbHGG4ionnZabLujdRsVSU7PlXDtLNH8RxosV5kpExJ5htGmqogVn5V5Z2J1JKjsc6hsz6dIvd5SvcQzPUg88ZZImwLWqFw5Vq0qfQCffVbqPmSIav36zv3NN5N3vWnTXJgbNiQnvMGDXXgrV7r9cFMwVGbxR1QGiXfUbqzRxtGmfIi3T7tPovccbUqEeEYwp4JgA3+89xVuIFrwOUSLz9DnE++AwdABXsnIkATjNRnjYXy7Kvsd+N2Og/EULhPSsGF8EwjGE1eJjFY2Qagkb75Z+gDuu8+5rVjh9m+/XTU9XfWmm5J3vYkTXdjvv1/WfetW1RtvVN2zp2LhHXOMC2/8+OT3gInUFa4yvZlSRaR7j9bNNdF5goJjElJNvFOIhCZYyZzILdZEeeHOhbKDEuPpbhxuvEq4CRVjCVaiA9YqGmZoLj/e+ZbitcdKCFr1gjBjhoudVq1c1YWq6t/+5tzWr1c96yzVzp2Td71rr3VhP/lkWffHHnPuL74Y/4d98KATLD9nUlkBqEg1R7ISn2QRrXhekQQtSLTceXWR7MnQ4iFZM3ImUgoM955FK0FFy8SEGy/QsGHsaqXQXH8yiFXyS3RcgglCBB58UHXcuNj+Jk92sXPjje6F2bPHlQxEVPfvV/3LX9zxL75Q/fpr1e7dVd95J3xYf/ub6qhR0RPLkSNdeJMnlz33ppuc+9lnR693DIb70EOVFwF/ORTmm090xslIVEfiG4tkJc6JXLcmZQB8KmJXtBx9JJGpSK6/IkQrIVQmfk0QInDBBapt28b2N26calaW6jPPuFhaubLUTVV1wQLn/s47qq+95rZbtFD95JPyYfXsqdqoUfQPdtAg53bllWXPPf30iififukg1lK/fuwSRLi2gkOJRBK06kp847GrJibORvyk6t0yQYjA8OEuxx/sORSOESNU+/RxdfqgOm+e6s9+ptqrlzu+fLlzf/55V80Drj70pJPKhvP119ETXD9X2b272+/WrWwDVapmVATVYcPia6isCYldTcMSXyNVpOLdilcQ6tw4hH373KCtPXui+9u4ETp0cAtAYSF88w20bev2/T7vwdG4+fnw8cfuGj5vvBH9Ol995fr+r1zp9letcm6qLlzVCt1e3HTt6uzMz4f16911nnoqfD9p+3dtefx4O3jQre1H70ayqM53q04KArhRyNEoLHRicMQRpfubN0O7dm6/devScLZvdwNyzjwTioth8eLScF57Lfp1RFI77UEo6enQqBGceCKsWVP2WH5+5CkSqnL4vGEY1UOdEwR/BG60Ye/79rnjHTq4xPOww1yC+M03pYKQnu5GFfolhFat4KST3LH//MetH38cXn21NNz6YWaOSlUJIByNGzshaNkSOneGLVtg166yfvySTyiJjJQ1DKN2UecEIZ4SwsaNbu1XF3XoAMuXu9y/LwjgSgl+CaF1a1edlJMDH33khpdfe23ZBF+1euf7mToVsrJKBQHKlxIyMspP/ZCRAVOmVI2NhmFUH3VWEKKVEAoL3TooCMuWue2gIGRmlpYQ/Jz1gAFOECZMgB9/LBvugQPwww+Vv4eK4pdMTj8dvvuuvCD893/Dvfe6/e+/d8LVrl34OXWi8eabMHgwbNoU2c8770BeHhQUVOqWDMNIAXVu+utgCaG42E2SlZFRmlg3aRJeEPzJtPxGZXClgm3bXJi++wknwOzZka9f0WmW09OdsHTsCCNGwBNPVHym1eJit962zc3U2rIlHHOMc1u40CX4J57ohMEXyhtvhPHjy4bx44/RSzjTpsF778HZZ7sG6tAqsk2b4PzzXTXV2WfDK684W8DdX+PG7jpffBH5GmlpcPTRtWv2T8OoLdS5EkKwDeEPf3C5VYD774cuXVzu2K8yat/erX1hgPIlhGCVEZS2I0RKOOOdZTEjw4Xvi8GUKfDXv7rEOzu7NPfep0/kMIO2+vfslxAyMtx9zZzpRGr9+tL7Bnj99dLtfftg0CA49tjIDeA//ADz58Pxx7vqtR49XHwGl6FDoWlTmDfP+R8woPRY9+7OhkGDyp8XXDp3hjFj7P8AhpEK6lQJ4eDB0mqcbdvg88/h009dgrd4sSsZ7NnjGlubNi1N1H1hgPBVRvv2lVYZff45NGzoql7CEW8JYd++UvEK/YlNsPrmm2/cnPkTJ5avotq82a2bN3e58u3bSwUBXOK6YIHb3rgR1q1z2yeeCB984P6n0KIFXHIJfPihi48RI8r+OwGcUOzf7+ydNMmVllavDn9fAwe6+Fy8uLTxffduuPlm1xX2xx/hrrvgyCPDn79okaveyshwgmIYdYWLLir9dlNFnRKEYP399u0uRwquB5G/vWWLWzIyXAPx11+7hliAZs3K5vz9KqMDB5yYtGmTvJ92hPY+8scChNblt20Lv/mN8//b34YPK1gqCicIGRnOz4cfOvfrrnPbc+bA4YfDCy/APfdA376uqucXvyh/jdxcV701eLALr2/f6Pd31FFuCd7HqFGupBYqOEFGj3bjSB580JVuDKOuMHBgDREEERkOPACkAY+q6p0hx7OBGUAWsB0Yq6qF3rHLgIme1z+q6hOeez/gcaAx8DrwK29EXcoI1r1v21YqAuvXl25/+62r8ti6tbRa4ttv3fr7791PR/zqoWDiP3euS6hSSbSxANddF1kQ/DaEjRtdDtx/qY491q3z8+GRR+Df/3b755/vqqIeeMB1ue3YEW66ybUJbNnicvQ+Bw+6EsS777rSQ7gffMTDOee49o2GDaP7E3F2TZhQel+GURfwM6YpJdZQZpwIfAEcBTQElgHdQvw8D1zmbQ8FnvK2WwPrvHUrb7uVd+xjYAAgwBvAWbFsqezUFcFpJPzpoUF16tTS7ZdeCj/7YaqWVq3KuyU6137LltGvdeGFbv3Xvzr/n3zi5kpaskRLpt5o2dIde+KJ0vPuvjv6dbdvVz3vvOT+H8IwjORBEqeu6A+sVdV1qrofmA2cG+KnG/COt70gcPxM4C1V3a6qO4C3gOEi0g5orqofecY+CZwXj4BVBr+HkQisXVvq/v77pdvffpv6nH6QwYPdOj29tKH42mvL57TjGQsQbPwOpX790t47fgmhe3d46y3XAJyW5kpAfhijRrnqoowMuPrq6Ndt1QpeegmGDYvuzzCMmk08gtAe2BDYL/TcgiwDLvC2zweaiUhmlHPbe9vRwgRARMaJSIGIFBQVFcVhbmR8QTjssFK3evVcV0kfvyE21TRq5NZ+w+j555fOXRKuN1E8YwH8xDw9vax7Roar9vEbjUPrIevXL9vF1rdv5ky3tGqV0C0ahlHLSFa3098Ag0RkCTAI2AhUsMd9eFR1uqrmqWpeViUr0fw2hGBOuk+f0rYAEbj77kpdIm7GjHHrXr1cL6D2IXKYyARXRx3lEvtwYtK5s2tQhvANUzk5bh2Mm7POcj0bDMOoG8QjCBuBYCfADp5bCaq6SVUvUNU+wATP7bso5270tiOGmQr8EoKf6LVq5XrHQGlf/mDDsz/4KTMzdmNnEBHXyKvquoRmZ5dewy+d+Ndr0wb+9a+yg8ASZdIkF9Yll5QXk+AcRfEKgmEYdYt4BGEh0FlEOolIQ2A0MDfoQUTaiIgf1q24HkcA84AzRKSViLQCzgDmqepmYJeIDBARAS4FXknC/UQlVBByckoTQpHyXT1VXWL+wAOuy2m8qJYO7PJz+jfe6Kpy/DmU/BlRMzNdKSHSpHIVISvLtQeEw+8ZBSYIhmGEJ6YgqGoxcD0ucf8UmKOqK0Vksoic43kbDKwWkc+Bw4Ep3rnbgTtworIQmOy5Afw38CiwFteLKcafAypPaJVRUBAidWH86isYO7bi4wtCu4i2aOEabf3rfPKJWwcT6lRiJQTDMGIR1zgEVX0dN1Yg6HZbYPvvwN8jnDuD0hJD0L0AiJCfTQ2hJYTs7NLqnCZNIo8uToTQ6aJbtCi7/9NPrjG3efPkXTMaviA0bFi+0Rlcb6eBA6Ffv6qxxzCMmkedmssonCAsWeK2kykG4bqIBgXBn1iudeuqm6TNL4m0bBn+mjk5rrdVmzZVY49hGDWPOikIvXrBHXe43PKttyb/OuG6iAYF4dRT3bqqqougtISQ6qHvhmHUXurUXEZ+G0KTJm4yuJycsv8/jpeMDDdVc7h2hezs8F1E/aqhtm3drJ2QnIbkeAmWEAzDMMJR50oIIqVdSBP5T3BmpisBPPBAxUYT+yWEYEO2lRAMw6hJ1KkSwr59Lmfv16F37Bj9B/d+Irp9e+k/CUJz/xMmOGGJdNwnnCBUZQnBBMEwjFjUKUHYu7dsrn7KFDeIK9wcq9nZpTOgRiL03wTRqO4SQvPmbpqO0N5OhmEYPnWuyqhx49L9/PzwYgCJVSdF47DD4Mwz3XQQWVluuufTTkvuNaJRrx5cfLFNQGcYRmTqVAkhVBDAlQTCVRuFjiOoLPXrwz//Wbr/SsrHZZfnqaeq/pqGYdQe6lwJIbQheMqUxKaaNgzDONSoU4Kwd2/5EkJ+fmJTTRuGYRxq1PkqI6hY47BhGMahSp0qIWzc6H4eX6+em6KhTRu3nZMDs2ZVt3WGYRjVS50pIcya5RqP/V5FwVHGX30F48a5bSspGIZRV6kzJYQJEyJ3MQXXvjBhQtXZYxiGUdOoM4IQz7iCZI89MAzDqE3UGUGIZ1xBssceGIZh1CbqjCDEGlfQsKGNPTAMo25TZwThoovcOtIPaZo1swZlwzDqNnEJgogMF5HVIrJWRMaHOd5RRBaIyBIRWS4iIzz3fBFZGlgOikhv79i7Xpj+scOSe2tl8f97EKlhefv28O6GYRh1hZjdTkUkDZgGDAMKgYUiMldVVwW8TQTmqOpDItIN9//lHFWdBczywukJvKyqSwPn5Xv/Vk45/s9xWrcOn/hb+4FhGHWdeEoI/YG1qrpOVfcDs4FzQ/wo4P8uvgWwKUw4Y7xzqwW/hDBypM1dZBiGEY54BKE9sCGwX+i5BZkEjBWRQlzp4IYw4YwCng1xm+lVF/1eJHztvoiME5ECESkoKiqKw9zw+IIwdKjNXWQYhhGOZDUqjwEeV9UOwAjgKREpCVtETgD2quongXPyVbUncKq3XBIuYFWdrqp5qpqXlZWVsIF79rh106Yu8V+/Hg4edGsTA8MwjPgEYSNwZGC/g+cW5CpgDoCqfgikA20Cx0cTUjpQ1Y3eejfwDK5qKmXs3u3WzZql8iqGYRi1l3gEYSHQWUQ6iUhDXOI+N8TP18BpACLSFScIRd5+PeAiAu0HIlJfRNp42w2As4FPSCH/+IdbDxpkk9kZhmGEI2YvI1UtFpHrgXlAGjBDVVeKyGSgQFXnAr8GHhGRm3ENzJerlnTwHAhsUNV1gWAbAfM8MUgD5gOPJO2uQpg1C/7619J9m8zOMAyjPKLRZnyrYeTl5WlBQcV7qebkhP9NZna2a0MwDMM4lBGRRaqaF8tfnRipHGnSOpvMzjAMo5Q6IQiRBp3ZYDTDMIxS6oQgTJkC9UNaS2wwmmEYRlnqhCDk58OJJ0Jamg1GMwzDiESd+YXmYYfBccfBypXVbYlhGEbNpE6UEMANTLNBaYZhGJExQTAMwzAAEwTDMAzDo84Iwq5dJgiGYRjRqDOCXHHLAAAb1UlEQVSCYCUEwzCM6NQJQVB1gtC8eWy/hmEYdZU6IQg//gjFxVZCMAzDiEadEAT7F4JhGEZsTBAMwzAMoI4Iwq5dbm2CYBiGEZk6IQhWQjAMw4iNCYJhGIYBmCAYhmEYHnEJgogMF5HVIrJWRMaHOd5RRBaIyBIRWS4iIzz3HBHZJyJLveVvgXP6icgKL8wHRUSSd1tlMUEwDMOITUxBEJE0YBpwFtANGCMi3UK8TQTmqGofYDQQ+KU9X6hqb2+5NuD+EHAN0Nlbhid+G9HxBcEGphmGYUQmnhJCf2Ctqq5T1f3AbODcED8K+MltC2BTtABFpB3QXFU/UlUFngTOq5DlFcAXhKZNU3UFwzCM2k88gtAe2BDYL/TcgkwCxopIIfA6cEPgWCevKuk9ETk1EGZhjDABEJFxIlIgIgVFRUVxmFueXbugcePyv9E0DMMwSklWo/IY4HFV7QCMAJ4SkXrAZqCjV5X0P8AzIlKhihtVna6qeaqal5WVlZBxNrGdYRhGbOLJM28Ejgzsd/DcglyF1wagqh+KSDrQRlW/BX703BeJyBfAsd75HWKEmTRMEAzDMGITTwlhIdBZRDqJSENco/HcED9fA6cBiEhXIB0oEpEsr1EaETkK13i8TlU3A7tEZIDXu+hS4JWk3FEYTBAMwzBiE7OEoKrFInI9MA9IA2ao6koRmQwUqOpc4NfAIyJyM66B+XJVVREZCEwWkZ+Ag8C1qrrdC/q/gceBxsAb3pISRo6EPXtSFbphGMahgbhOPrWDvLw8LSgoqG4zDMMwahUiskhV82L5qxMjlQ3DMIzYmCAYhmEYgAmCYRiG4WGCYBiGYQAmCIZhGIaHCYJhGIYBmCAYhmEYHiYIhmEYBmCCYBiGYXiYIBiGYRiACYJhGIbhYYJgGIZhACYIhmEYhocJgmEYhgGYIBiGYRgeJgiGYRgGYIJgGIZheJggGIZhGECcgiAiw0VktYisFZHxYY53FJEFIrJERJaLyAjPfZiILBKRFd56aOCcd70wl3rLYcm7LcMwDKOi1I/lQUTSgGnAMKAQWCgic1V1VcDbRGCOqj4kIt2A14EcYCvwM1XdJCI9gHlA+8B5+apqP0k2DMOoAcRTQugPrFXVdaq6H5gNnBviR4Hm3nYLYBOAqi5R1U2e+0qgsYg0qrzZhmEYRrKJRxDaAxsC+4WUzeUDTALGikghrnRwQ5hwfg4sVtUfA24zveqi34uIhLu4iIwTkQIRKSgqKorDXMMwDCMRktWoPAZ4XFU7ACOAp0SkJGwR6Q7cBfwicE6+qvYETvWWS8IFrKrTVTVPVfOysrKSZK5hGIYRSjyCsBE4MrDfwXMLchUwB0BVPwTSgTYAItIBeAm4VFW/8E9Q1Y3eejfwDK5qyjAMw6gm4hGEhUBnEekkIg2B0cDcED9fA6cBiEhXnCAUiUhL4DVgvKr+P9+ziNQXEV8wGgBnA59U9mYMwzCMxIkpCKpaDFyP6yH0Ka430UoRmSwi53jefg1cIyLLgGeBy1VVvfOOAW4L6V7aCJgnIsuBpbgSxyPJvjnDMAwjfsSl27WDvLw8LSiwXqqGYRgVQUQWqWpeLH82UtkwDMMATBAMwzAMDxMEwzAMAzBBMAzDMDxMEAzDMAzABMEwDMPwMEEwDMMwABMEwzAMw8MEwTAMwwBMEAzDMAwPEwTDMAwDMEEwDMMwPEwQDMMwDMAEwTAMw/AwQTAMwzAAEwTDMAzDwwTBMAzDAEwQDMMwDA8TBMMwDAOIUxBEZLiIrBaRtSIyPszxjiKyQESWiMhyERkROHard95qETkz3jANwzCMqiWmIIhIGjANOAvoBowRkW4h3iYCc1S1DzAa+Kt3bjdvvzswHPiriKTFGaZhGIZRhcRTQugPrFXVdaq6H5gNnBviR4Hm3nYLYJO3fS4wW1V/VNUvgbVeePGEaRiGYVQh8QhCe2BDYL/QcwsyCRgrIoXA68ANMc6NJ0wARGSciBSISEFRUVEc5hqGYRiJkKxG5THA46raARgBPCUiSQlbVaerap6q5mVlZSUjSMMwDCMM9ePwsxE4MrDfwXMLchWujQBV/VBE0oE2Mc6NFaZhGIZRhcSTi18IdBaRTiLSENdIPDfEz9fAaQAi0hVIB4o8f6NFpJGIdAI6Ax/HGaZhGIZRhcQsIahqsYhcD8wD0oAZqrpSRCYDBao6F/g18IiI3IxrYL5cVRVYKSJzgFVAMfBLVT0AEC7MFNyfYRiGESfi0u3aQV5enhYUFFS3GYZhGLUKEVmkqnmx/NlIZcMwDAMwQTAMwzA8TBAMwzAMwATBMAzD8DBBMAzDMAATBMMwDMPDBMEwDMMATBAMwzAMDxMEwzAMAzBBMAzDMDxMEAzDMAzABMEwDMPwMEEwDMMwABMEwzAMw8MEwTAMwwBMEAzDMAwPEwTDMAwDMEEwDMMwPOISBBEZLiKrRWStiIwPc/x+EVnqLZ+LyHee+5CA+1IR+UFEzvOOPS4iXwaO9U7urRmGYRgVoX4sDyKSBkwDhgGFwEIRmauqq3w/qnpzwP8NQB/PfQHQ23NvDawF3gwEf4uq/j0J92EYhmFUknhKCP2Btaq6TlX3A7OBc6P4HwM8G8Z9JPCGqu6tuJmGYRhGqolHENoDGwL7hZ5bOUQkG+gEvBPm8GjKC8UUEVnuVTk1ihDmOBEpEJGCoqKiOMw1DMMwEiHZjcqjgb+r6oGgo4i0A3oC8wLOtwJdgOOB1sD/CRegqk5X1TxVzcvKykqyuYZhGIZPPIKwETgysN/BcwtHuFIAwEXAS6r6k++gqpvV8SMwE1c1ZRiGYVQTMRuVgYVAZxHphBOC0cDFoZ5EpAvQCvgwTBhjcCWCoP92qrpZRAQ4D/ikgrYbRp3lp59+orCwkB9++KG6TTFqEOnp6XTo0IEGDRokdH5MQVDVYhG5HlfdkwbMUNWVIjIZKFDVuZ7X0cBsVdXg+SKSgythvBcS9CwRyQIEWApcm9AdGEYdpLCwkGbNmpGTk4PLUxl1HVVl27ZtFBYW0qlTp4TCiKeEgKq+Drwe4nZbyP6kCOeuJ0wjtKoOjddIwzDK8sMPP5gYGGUQETIzM6lM5xsbqWwYtRQTAyOUyr4TJgiGYRgGYIJgGHWCWbMgJwfq1XPrWbMqF962bdvo3bs3vXv3pm3btrRv375kf//+/XGFccUVV7B69eqofqZNm8asyhprxE1cbQiGYdReZs2CceNgrzdHwFdfuX2A/PzEwszMzGTp0qUATJo0iaZNm/Kb3/ymjB9VRVWpVy98vnPmzJkxr/PLX/4yMQOrkeLiYurXr51Jq5UQDOMQZ8KEUjHw2bvXuSebtWvX0q1bN/Lz8+nevTubN29m3Lhx5OXl0b17dyZPnlzi95RTTmHp0qUUFxfTsmVLxo8fT25uLieeeCLffvstABMnTmTq1Kkl/sePH0///v057rjj+OCDDwD4/vvv+fnPf063bt0YOXIkeXl5JWIV5Pbbb+f444+nR48eXHvttfgdIj///HOGDh1Kbm4uffv2Zf369QD86U9/omfPnuTm5jLBiyzfZoBvvvmGY445BoBHH32U8847jyFDhnDmmWeya9cuhg4dSt++fenVqxf/+Mc/SuyYOXMmvXr1Ijc3lyuuuIKdO3dy1FFHUVxcDMCOHTvK7FclJgiGcYjz9dcVc68sn332GTfffDOrVq2iffv23HnnnRQUFLBs2TLeeustVq1aVe6cnTt3MmjQIJYtW8aJJ57IjBkzwoatqnz88cfcc889JeLyv//7v7Rt25ZVq1bx+9//niVLloQ991e/+hULFy5kxYoV7Ny5k3/+858AjBkzhptvvplly5bxwQcfcNhhh/Hqq6/yxhtv8PHHH7Ns2TJ+/etfx7zvJUuW8OKLL/L222/TuHFjXn75ZRYvXsz8+fO5+WY3/+eyZcu46667ePfdd1m2bBn33nsvLVq04OSTTy6x59lnn+XCCy+sllKGCYJhHOJ07Fgx98py9NFHk5eXV7L/7LPP0rdvX/r27cunn34aVhAaN27MWWedBUC/fv1KcumhXHDBBeX8/Pvf/2b06NEA5Obm0r1797Dnvv322/Tv35/c3Fzee+89Vq5cyY4dO9i6dSs/+9nPADewKyMjg/nz53PllVfSuHFjAFq3bh3zvs844wxatWoFOOEaP348vXr14owzzmDDhg1s3bqVd955h1GjRpWE56+vvvrqkiq0mTNncsUVV8S8XiowQTCMQ5wpUyAjo6xbRoZzTwVNmjQp2V6zZg0PPPAA77zzDsuXL2f48OFhR1c3bNiwZDstLS1idUmjRo1i+gnH3r17uf7663nppZdYvnw5V155ZUKjvOvXr8/BgwcByp0fvO8nn3ySnTt3snjxYpYuXUqbNm2iXm/QoEF8/vnnLFiwgAYNGtClS5cK25YMTBAM4xAnPx+mT4fsbBBx6+nTE29Qrgi7du2iWbNmNG/enM2bNzNv3rzYJ1WQk08+mTlz5gCwYsWKsCWQffv2Ua9ePdq0acPu3bt54YUXAGjVqhVZWVm8+uqrgEvk9+7dy7Bhw5gxYwb79u0DYPv27QDk5OSwaNEiAP7+98i/ctm5cyeHHXYY9evX56233mLjRjf929ChQ3nuuedKwvPXAGPHjiU/P7/aSgdggmAYdYL8fFi/Hg4edOuqEAOAvn370q1bN7p06cKll17KySefnPRr3HDDDWzcuJFu3brxhz/8gW7dutGiRYsyfjIzM7nsssvo1q0bZ511FieccELJsVmzZnHvvffSq1cvTjnlFIqKijj77LMZPnw4eXl59O7dm/vvvx+AW265hQceeIC+ffuyY8eOiDZdcsklfPDBB/Ts2ZPZs2fTuXNnwFVp/fa3v2XgwIH07t2bW265peSc/Px8du7cyahRo5IZPRVCQqYeqtHk5eVpQUFBdZthGNXOp59+SteuXavbjBpBcXExxcXFpKens2bNGs444wzWrFlT67p+zp49m3nz5sXVHTca4d4NEVmkqnkRTimhdsWYYRhGCHv27OG0006juLgYVeXhhx+udWJw3XXXMX/+/JKeRtVF7Yo1wzCMEFq2bFlSr19beeihh6rbBMDaEAzDMAwPEwTDMAwDMEEwDMMwPEwQDMMwDMAEwTCMBBgyZEi5QWZTp07luuuui3pe06ZNAdi0aRMjR44M62fw4MHE6l4+depU9gZm7BsxYgTfffddPKYbUYhLEERkuIisFpG1IjI+zPH7RWSpt3wuIt8Fjh0IHJsbcO8kIv/xwnxORBqGhmsYRs1kzJgxzJ49u4zb7NmzGTNmTFznH3HEEVFH+sYiVBBef/11WrZsmXB4VY2qlkyBUZOIKQgikgZMA84CugFjRKRb0I+q3qyqvVW1N/C/wIuBw/v8Y6p6TsD9LuB+VT0G2AFcVcl7MYw6yU03weDByV1uuin6NUeOHMlrr71W8jOc9evXs2nTJk499dSScQF9+/alZ8+evPLKK+XOX79+PT169ADctBKjR4+ma9eunH/++SXTRYDrn+9PnX377bcD8OCDD7Jp0yaGDBnCkCFDADelxNatWwG477776NGjBz169CiZOnv9+vV07dqVa665hu7du3PGGWeUuY7Pq6++ygknnECfPn04/fTT2bJlC+DGOlxxxRX07NmTXr16lUx98c9//pO+ffuSm5vLaaedBrj/Q/z5z38uCbNHjx6sX7+e9evXc9xxx3HppZfSo0cPNmzYEPb+ABYuXMhJJ51Ebm4u/fv3Z/fu3QwcOLDMtN6nnHIKy5Yti/6gKkg84xD6A2tVdR2AiMwGzgXKTxjiGAPcHuEYXhgCDAUu9pyeACYBNaMzrmEYUWndujX9+/fnjTfe4Nxzz2X27NlcdNFFiAjp6em89NJLNG/enK1btzJgwADOOeeciP/7feihh8jIyODTTz9l+fLl9O3bt+TYlClTaN26NQcOHOC0005j+fLl3Hjjjdx3330sWLCANm3alAlr0aJFzJw5k//85z+oKieccAKDBg2iVatWrFmzhmeffZZHHnmEiy66iBdeeIGxY8eWOf+UU07ho48+QkR49NFHufvuu7n33nu54447aNGiBStWrADcPwuKioq45ppreP/99+nUqVOZeYkisWbNGp544gkGDBgQ8f66dOnCqFGjeO655zj++OPZtWsXjRs35qqrruLxxx9n6tSpfP755/zwww/k5uZW6LnFIh5BaA9sCOwXAieE8ygi2UAn4J2Ac7qIFADFwJ2q+jKQCXynqv50hYXedcKFOQ4YB9AxVfP1GkYtxssEVzl+tZEvCI899hjgqkN+97vf8f7771OvXj02btzIli1baNu2bdhw3n//fW688UYAevXqRa9evUqOzZkzh+nTp1NcXMzmzZtZtWpVmeOh/Pvf/+b8888vmXn0ggsu4F//+hfnnHMOnTp1onfv3kDkKbYLCwsZNWoUmzdvZv/+/XTq1AmA+fPnl6kia9WqFa+++ioDBw4s8RPPFNnZ2dklYhDp/kSEdu3acfzxxwPQvHlzAC688ELuuOMO7rnnHmbMmMHll18e83oVJdmNyqOBv6vqgYBbtjeHxsXAVBE5uiIBqup0Vc1T1bysrKwKG5Tsf8kahuE499xzefvtt1m8eDF79+6lX79+gJssrqioiEWLFrF06VIOP/zwhKaa/vLLL/nzn//M22+/zfLly/mv//qvhMLx8afOhsjTZ99www1cf/31rFixgocffrjSU2RD2Wmyg1NkV/T+MjIyGDZsGK+88gpz5swhPwUzFMYjCBuBIwP7HTy3cIwGng06qOpGb70OeBfoA2wDWoqIX0KJFmbC+P+S/eorUC39l6yJgmFUnqZNmzJkyBCuvPLKMo3J/tTPDRo0YMGCBXz11VdRwxk4cCDPPPMMAJ988gnLly8H3NTZTZo0oUWLFmzZsoU33nij5JxmzZqxe/fucmGdeuqpvPzyy+zdu5fvv/+el156iVNPPTXue9q5cyft27vKiieeeKLEfdiwYUybNq1kf8eOHQwYMID333+fL7/8Eig7RfbixYsBWLx4ccnxUCLd33HHHcfmzZtZuHAhALt37y4Rr6uvvpobb7yR448/vuRnPMkkHkFYCHT2egU1xCX6c0M9iUgXoBXwYcCtlYg08rbbACcDq9RNsboA8PudXQaUb3mqJFX5L1nDqIuMGTOGZcuWlRGE/Px8CgoK6NmzJ08++WTMn71cd9117Nmzh65du3LbbbeVlDRyc3Pp06cPXbp04eKLLy4zdfa4ceMYPnx4SaOyT9++fbn88svp378/J5xwAldffTV9+vSJ+34mTZrEhRdeSL9+/cq0T0ycOJEdO3bQo0cPcnNzWbBgAVlZWUyfPp0LLriA3Nzckmmrf/7zn7N9+3a6d+/OX/7yF4499tiw14p0fw0bNuS5557jhhtuIDc3l2HDhpWUHPr160fz5s1T9s+EuKa/FpERwFQgDZihqlNEZDJQoKpzPT+TgHRVHR847yTgYeAgTnymqupj3rGjgNlAa2AJMFZVf4xmR0Wnv65Xz5UMyt+PmxfeMGorNv113WTTpk0MHjyYzz77jHr1wufnUz79taq+Drwe4nZbyP6kMOd9APSMEOY6XA+mlNGxo6smCuduGIZRm3jyySeZMGEC9913X0QxqCyH9Ejlqv6XrGEYRqq49NJL2bBhAxdeeGHKrnFIC0J1/kvWMFJNbfrboVE1VPadOOR/kJOfbwJgHHqkp6ezbds2MjMzIw74MuoWqsq2bdtIT09POIxDXhAM41CkQ4cOFBYWUlRUVN2mGDWI9PR0OnTokPD5JgiGUQtp0KBByQhZw0gWh3QbgmEYhhE/JgiGYRgGYIJgGIZheMQ1UrmmICJFQPSJUSLTBtiaRHOSRU21C2qubWZXxTC7Kk5NtS1Ru7JVNebsoLVKECqDiBTEM3S7qqmpdkHNtc3sqhhmV8Wpqbal2i6rMjIMwzAAEwTDMAzDoy4JwvTqNiACNdUuqLm2mV0Vw+yqODXVtpTaVWfaEAzDMIzo1KUSgmEYhhEFEwTDMAwDqCOCICLDRWS1iKwVkfGxz0iZHUeKyAIRWSUiK0XkV577JBHZKCJLvWVENdi2XkRWeNcv8Nxai8hbIrLGWyf/J67RbTouECdLRWSXiNxUXfElIjNE5FsR+STgFjaOxPGg984tF5G+VWzXPSLymXftl0SkpeeeIyL7AnH3tyq2K+KzE5FbvfhaLSJnVrFdzwVsWi8iSz33qoyvSOlD1b1jqnpIL7jffn4BHAU0BJYB3arJlnZAX2+7GfA50A2YBPymmuNpPdAmxO1uYLy3PR64q5qf4zdAdnXFFzAQ6At8EiuOgBHAG4AAA4D/VLFdZwD1ve27AnblBP1VQ3yFfXbed7AMaAR08r7ZtKqyK+T4vcBt1RBfkdKHKnvH6kIJoT+wVlXXqep+3H+cz60OQ1R1s6ou9rZ3A58C7avDljg5F3jC234COK8abTkN+EJVEx2pXmlU9X1ge4hzpDg6F3hSHR8BLUWkXVXZpapvqmqxt/sRkPicyEm0KwrnArNV9UdV/RJYS4p+sRvNLnE/l7gIeDYV145GlPShyt6xuiAI7YENgf1CakAiLCI5QB/gP57T9V6xb0ZVV814KPCmiCwSkXGe2+Gqutnb/gY4vBrs8hlN2Y+0uuPLJ1Ic1aT37kpcTtKnk4gsEZH3ROTUarAn3LOrKfF1KrBFVdcE3Ko8vkLShyp7x+qCINQ4RKQp8AJwk6ruAh4CjgZ6A5txRdaq5hRV7QucBfxSRAYGD6oro1ZLH2URaQicAzzvOdWE+CpHdcZRJERkAlAMzPKcNgMdVbUP8D/AMyLSvApNqpHPLsAYymY8qjy+wqQPJaT6HasLgrARODKw38FzqxZEpAHuYc9S1RcBVHWLqh5Q1YPAI6SoqBwNVd3orb8FXvJs2OIXQb31t1Vtl8dZwGJV3eLZWO3xFSBSHFX7eycilwNnA/leQoJXJbPN216Eq6s/tqpsivLsakJ81QcuAJ7z3ao6vsKlD1ThO1YXBGEh0FlEOnk5zdHA3OowxKuffAz4VFXvC7gH6/3OBz4JPTfFdjURkWb+Nq5B8hNcPF3mebsMeKUq7QpQJtdW3fEVQqQ4mgtc6vUEGQDsDBT7U46IDAd+C5yjqnsD7lkikuZtHwV0BtZVoV2Rnt1cYLSINBKRTp5dH1eVXR6nA5+paqHvUJXxFSl9oCrfsapoPa/uBdca/zlO3SdUox2n4Ip7y4Gl3jICeApY4bnPBdpVsV1H4Xp4LANW+nEEZAJvA2uA+UDraoizJsA2oEXArVriCydKm4GfcPW1V0WKI1zPj2neO7cCyKtiu9bi6pf99+xvnt+fe894KbAY+FkV2xXx2QETvPhaDZxVlXZ57o8D14b4rcr4ipQ+VNk7ZlNXGIZhGEDdqDIyDMMw4sAEwTAMwwBMEAzDMAwPEwTDMAwDMEEwDMMwPEwQDMMwDMAEwTAMw/D4/0R2a86JnmjaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PELM_s Result\n",
      "Accuracy : 0.9032258064516129\n",
      "AUC : 0.9009546041935887\n",
      "Sensitivity : 0.9596273291925466\n",
      "Specificity : 0.8422818791946308\n",
      "F1 : 0.911504424778761\n",
      "MCC : 0.8102784823781962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Plot the training accuracy model transformer\n",
    "\n",
    "accuracy = model_transformer_train.history['acc']\n",
    "val_accuracy = model_transformer_train.history['val_acc']\n",
    "loss = model_transformer_train.history['loss']\n",
    "val_loss = model_transformer_train.history['val_loss']\n",
    "epochs = range(len(accuracy))\n",
    "plt.plot(epochs, accuracy, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
    "plt.title('Transformer Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Transformer Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "y_pred = np.argmax(model_transformer.predict(valid_X), axis=1)\n",
    "y_true = np.argmax(valid_Y, axis = 1)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "f1 = f1_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
    "auc = roc_auc_score(y_true, y_pred, average='macro', sample_weight=None, max_fpr=None)\n",
    "sensi = tp/(tp+fn)\n",
    "specificity = tn/(tn+fp)\n",
    "accu = (tn + tp)/(tn + tp + fn + fp)\n",
    "mcc = ((tp*tn)-(fp*fn))/np.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n",
    "print('{} Result'.format(string_name))\n",
    "print('Accuracy :', accu)\n",
    "print('AUC :', auc)\n",
    "print('Sensitivity :', sensi)\n",
    "print('Specificity :', specificity)\n",
    "print('F1 :', f1)\n",
    "print('MCC :', mcc)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLayer(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(MyLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.kernel = self.add_weight(name='kernel', \n",
    "#                                       shape=(input_shape[1], self.output_dim),\n",
    "                                      shape=(1),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        super(MyLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, x):\n",
    "        sig = self.kernel\n",
    "        base = np.linspace(0,input_shape[1]-1,input_shape[1])\n",
    "        gauss = K.exp(-K.power(base - (input_shape[1]-1)/2, 2.) / (2 * K.power(sig, 2.)))\n",
    "        out = K.mul(x,gauss)\n",
    "        return out\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def gaussian(x, mu, sig):\n",
    "    return np.exp(-np.power(x - mu, 2.) / (2 * np.power(sig, 2.)))\n",
    "\n",
    "seq_pos = np.linspace(0,8,9)\n",
    "mu = 4\n",
    "sig = 4\n",
    "mp.plot(seq_pos, gaussian(seq_pos, mu, sig))\n",
    "\n",
    "mp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.   , 1.125, 2.25 , 3.375, 4.5  , 5.625, 6.75 , 7.875, 9.   ])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaussian(seq_pos, mu, sig)\n",
    "np.linspace(0,8,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdddf08b1d0>]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "seq_pos=(0.1,0.4,0.6,0.75,0,0.75,0.6,0.4,0.1)\n",
    "plt.plot(seq_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape must be rank 1 but is rank 0 for 'my_layer_4/random_uniform/RandomUniform' (op: 'RandomUniform') with input shapes: [].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/Documents/Env/env_PSP/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1628\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1629\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Shape must be rank 1 but is rank 0 for 'my_layer_4/random_uniform/RandomUniform' (op: 'RandomUniform') with input shapes: [].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-bb22704dfd03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#i = Dense(32)(i)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# i = Flatten()(i)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Env/env_PSP/lib/python3.5/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m                                          \u001b[0;34m'You can build it manually via: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m                                          '`layer.build(batch_input_shape)`')\n\u001b[0;32m--> 431\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-61-1bc7845a4a7a>\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m     11\u001b[0m                                       \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                                       \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'uniform'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                                       trainable=True)\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMyLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Be sure to call this at the end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Env/env_PSP/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Env/env_PSP/lib/python3.5/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         weight = K.variable(initializer(shape),\n\u001b[0m\u001b[1;32m    250\u001b[0m                             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m                             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Env/env_PSP/lib/python3.5/site-packages/keras/initializers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, shape, dtype)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         return K.random_uniform(shape, self.minval, self.maxval,\n\u001b[0;32m--> 112\u001b[0;31m                                 dtype=dtype, seed=self.seed)\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Env/env_PSP/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[0;34m(shape, minval, maxval, dtype, seed)\u001b[0m\n\u001b[1;32m   4137\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4138\u001b[0m     return tf.random_uniform(shape, minval=minval, maxval=maxval,\n\u001b[0;32m-> 4139\u001b[0;31m                              dtype=dtype, seed=seed)\n\u001b[0m\u001b[1;32m   4140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Env/env_PSP/lib/python3.5/site-packages/tensorflow/python/ops/random_ops.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[0;34m(shape, minval, maxval, dtype, seed, name)\u001b[0m\n\u001b[1;32m    241\u001b[0m           shape, minval, maxval, seed=seed1, seed2=seed2, name=name)\n\u001b[1;32m    242\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m       \u001b[0mrnd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_random_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_uniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnd\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmaxval\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mminval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Env/env_PSP/lib/python3.5/site-packages/tensorflow/python/ops/gen_random_ops.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[0;34m(shape, dtype, seed, seed2, name)\u001b[0m\n\u001b[1;32m    731\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m    732\u001b[0m         \u001b[0;34m\"RandomUniform\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m    734\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Env/env_PSP/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Env/env_PSP/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m                 instructions)\n\u001b[0;32m--> 488\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m     return tf_decorator.make_decorator(func, new_func, 'deprecated',\n\u001b[1;32m    490\u001b[0m                                        _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m~/Documents/Env/env_PSP/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3272\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3273\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3274\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3275\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3276\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Env/env_PSP/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1790\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1791\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1792\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m     \u001b[0;31m# Initialize self._outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Env/env_PSP/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape must be rank 1 but is rank 0 for 'my_layer_4/random_uniform/RandomUniform' (op: 'RandomUniform') with input shapes: []."
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "inp = Input(shape=(9,))\n",
    "i = Embedding(20, 10, input_length=9)(inp)\n",
    "i = Flatten(data_format=None)(i)\n",
    "#i = Dense(32)(i)\n",
    "i = MyLayer(32)(i)\n",
    "# i = Flatten()(i)\n",
    "\n",
    "out = Dense(2, activation='softmax')(i)\n",
    "\n",
    "model_MyLayer = Model(inputs=inp, outputs=out)\n",
    "model_MyLayer.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "model_MyLayer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2477 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "2477/2477 [==============================] - 0s 130us/step - loss: 0.5675 - acc: 0.7731 - val_loss: 0.3246 - val_acc: 0.9000\n",
      "Epoch 2/100\n",
      "2477/2477 [==============================] - 0s 58us/step - loss: 0.2526 - acc: 0.9031 - val_loss: 0.2148 - val_acc: 0.9242\n",
      "Epoch 3/100\n",
      "2477/2477 [==============================] - 0s 60us/step - loss: 0.2167 - acc: 0.9164 - val_loss: 0.2028 - val_acc: 0.9242\n",
      "Epoch 4/100\n",
      "2477/2477 [==============================] - 0s 64us/step - loss: 0.2081 - acc: 0.9168 - val_loss: 0.2031 - val_acc: 0.9258\n",
      "Epoch 5/100\n",
      "2477/2477 [==============================] - 0s 65us/step - loss: 0.2029 - acc: 0.9201 - val_loss: 0.1994 - val_acc: 0.9274\n",
      "Epoch 6/100\n",
      "2477/2477 [==============================] - 0s 64us/step - loss: 0.2011 - acc: 0.9213 - val_loss: 0.2011 - val_acc: 0.9242\n",
      "Epoch 7/100\n",
      "2477/2477 [==============================] - 0s 62us/step - loss: 0.1986 - acc: 0.9225 - val_loss: 0.1990 - val_acc: 0.9274\n",
      "Epoch 8/100\n",
      "2477/2477 [==============================] - 0s 67us/step - loss: 0.1957 - acc: 0.9225 - val_loss: 0.2007 - val_acc: 0.9258\n",
      "Epoch 9/100\n",
      "2477/2477 [==============================] - 0s 74us/step - loss: 0.1936 - acc: 0.9265 - val_loss: 0.2027 - val_acc: 0.9258\n",
      "Epoch 10/100\n",
      "2477/2477 [==============================] - 0s 63us/step - loss: 0.1918 - acc: 0.9261 - val_loss: 0.2011 - val_acc: 0.9258\n",
      "Epoch 11/100\n",
      "2477/2477 [==============================] - 0s 61us/step - loss: 0.1908 - acc: 0.9269 - val_loss: 0.2025 - val_acc: 0.9274\n",
      "Epoch 12/100\n",
      "2477/2477 [==============================] - 0s 63us/step - loss: 0.1886 - acc: 0.9277 - val_loss: 0.2044 - val_acc: 0.9290\n",
      "Epoch 13/100\n",
      "2477/2477 [==============================] - 0s 62us/step - loss: 0.1865 - acc: 0.9289 - val_loss: 0.2049 - val_acc: 0.9258\n",
      "Epoch 14/100\n",
      "2477/2477 [==============================] - 0s 61us/step - loss: 0.1844 - acc: 0.9257 - val_loss: 0.2062 - val_acc: 0.9258\n",
      "Epoch 15/100\n",
      "2477/2477 [==============================] - 0s 62us/step - loss: 0.1831 - acc: 0.9302 - val_loss: 0.2075 - val_acc: 0.9177\n",
      "Epoch 16/100\n",
      "2477/2477 [==============================] - 0s 65us/step - loss: 0.1821 - acc: 0.9294 - val_loss: 0.2086 - val_acc: 0.9226\n",
      "Epoch 17/100\n",
      "2477/2477 [==============================] - 0s 64us/step - loss: 0.1816 - acc: 0.9298 - val_loss: 0.2105 - val_acc: 0.9210\n",
      "Epoch 18/100\n",
      "2477/2477 [==============================] - 0s 75us/step - loss: 0.1817 - acc: 0.9277 - val_loss: 0.2107 - val_acc: 0.9226\n",
      "Epoch 19/100\n",
      "2477/2477 [==============================] - 0s 66us/step - loss: 0.1793 - acc: 0.9294 - val_loss: 0.2139 - val_acc: 0.9274\n",
      "Epoch 20/100\n",
      "2477/2477 [==============================] - 0s 61us/step - loss: 0.1789 - acc: 0.9298 - val_loss: 0.2149 - val_acc: 0.9226\n",
      "Epoch 21/100\n",
      "2477/2477 [==============================] - 0s 62us/step - loss: 0.1775 - acc: 0.9314 - val_loss: 0.2192 - val_acc: 0.9226\n",
      "Epoch 22/100\n",
      "2477/2477 [==============================] - 0s 65us/step - loss: 0.1757 - acc: 0.9346 - val_loss: 0.2198 - val_acc: 0.9194\n",
      "Epoch 23/100\n",
      "2477/2477 [==============================] - 0s 74us/step - loss: 0.1773 - acc: 0.9281 - val_loss: 0.2195 - val_acc: 0.9242\n",
      "Epoch 24/100\n",
      "2477/2477 [==============================] - 0s 63us/step - loss: 0.1753 - acc: 0.9354 - val_loss: 0.2176 - val_acc: 0.9194\n",
      "Epoch 25/100\n",
      "2477/2477 [==============================] - 0s 61us/step - loss: 0.1757 - acc: 0.9338 - val_loss: 0.2219 - val_acc: 0.9258\n",
      "Epoch 26/100\n",
      "2477/2477 [==============================] - 0s 62us/step - loss: 0.1743 - acc: 0.9318 - val_loss: 0.2218 - val_acc: 0.9194\n",
      "Epoch 27/100\n",
      "2477/2477 [==============================] - 0s 61us/step - loss: 0.1747 - acc: 0.9338 - val_loss: 0.2213 - val_acc: 0.9145\n",
      "Epoch 28/100\n",
      "2477/2477 [==============================] - 0s 62us/step - loss: 0.1731 - acc: 0.9318 - val_loss: 0.2235 - val_acc: 0.9194\n",
      "Epoch 29/100\n",
      "2477/2477 [==============================] - 0s 62us/step - loss: 0.1746 - acc: 0.9318 - val_loss: 0.2270 - val_acc: 0.9226\n",
      "Epoch 30/100\n",
      "2477/2477 [==============================] - 0s 65us/step - loss: 0.1726 - acc: 0.9346 - val_loss: 0.2246 - val_acc: 0.9226\n",
      "Epoch 31/100\n",
      "2477/2477 [==============================] - 0s 65us/step - loss: 0.1733 - acc: 0.9342 - val_loss: 0.2255 - val_acc: 0.9177\n",
      "Epoch 32/100\n",
      "2477/2477 [==============================] - 0s 66us/step - loss: 0.1733 - acc: 0.9338 - val_loss: 0.2269 - val_acc: 0.9177\n",
      "Epoch 33/100\n",
      "2477/2477 [==============================] - 0s 66us/step - loss: 0.1711 - acc: 0.9346 - val_loss: 0.2279 - val_acc: 0.9145\n",
      "Epoch 34/100\n",
      "2477/2477 [==============================] - 0s 67us/step - loss: 0.1737 - acc: 0.9330 - val_loss: 0.2262 - val_acc: 0.9129\n",
      "Epoch 35/100\n",
      "2477/2477 [==============================] - 0s 65us/step - loss: 0.1704 - acc: 0.9370 - val_loss: 0.2280 - val_acc: 0.9129\n",
      "Epoch 36/100\n",
      "2477/2477 [==============================] - 0s 65us/step - loss: 0.1719 - acc: 0.9314 - val_loss: 0.2274 - val_acc: 0.9145\n",
      "Epoch 37/100\n",
      "2477/2477 [==============================] - 0s 61us/step - loss: 0.1708 - acc: 0.9350 - val_loss: 0.2285 - val_acc: 0.9161\n",
      "Epoch 38/100\n",
      "2477/2477 [==============================] - 0s 58us/step - loss: 0.1710 - acc: 0.9354 - val_loss: 0.2282 - val_acc: 0.9161\n",
      "Epoch 39/100\n",
      "2477/2477 [==============================] - 0s 58us/step - loss: 0.1707 - acc: 0.9314 - val_loss: 0.2290 - val_acc: 0.9161\n",
      "Epoch 40/100\n",
      "2477/2477 [==============================] - 0s 60us/step - loss: 0.1701 - acc: 0.9334 - val_loss: 0.2314 - val_acc: 0.9129\n",
      "Epoch 41/100\n",
      "2477/2477 [==============================] - 0s 62us/step - loss: 0.1713 - acc: 0.9338 - val_loss: 0.2305 - val_acc: 0.9113\n",
      "Epoch 42/100\n",
      "2477/2477 [==============================] - 0s 58us/step - loss: 0.1706 - acc: 0.9334 - val_loss: 0.2312 - val_acc: 0.9145\n",
      "Epoch 43/100\n",
      "2477/2477 [==============================] - 0s 60us/step - loss: 0.1690 - acc: 0.9354 - val_loss: 0.2315 - val_acc: 0.9145\n",
      "Epoch 44/100\n",
      "2477/2477 [==============================] - 0s 60us/step - loss: 0.1713 - acc: 0.9318 - val_loss: 0.2283 - val_acc: 0.9161\n",
      "Epoch 45/100\n",
      "2477/2477 [==============================] - 0s 64us/step - loss: 0.1705 - acc: 0.9342 - val_loss: 0.2321 - val_acc: 0.9161\n",
      "Epoch 46/100\n",
      "2477/2477 [==============================] - 0s 64us/step - loss: 0.1696 - acc: 0.9358 - val_loss: 0.2319 - val_acc: 0.9145\n",
      "Epoch 47/100\n",
      "2477/2477 [==============================] - 0s 70us/step - loss: 0.1705 - acc: 0.9334 - val_loss: 0.2297 - val_acc: 0.9129\n",
      "Epoch 48/100\n",
      "2477/2477 [==============================] - 0s 67us/step - loss: 0.1689 - acc: 0.9318 - val_loss: 0.2303 - val_acc: 0.9145\n",
      "Epoch 49/100\n",
      "2477/2477 [==============================] - 0s 68us/step - loss: 0.1695 - acc: 0.9326 - val_loss: 0.2311 - val_acc: 0.9145\n",
      "Epoch 50/100\n",
      "2477/2477 [==============================] - 0s 62us/step - loss: 0.1689 - acc: 0.9330 - val_loss: 0.2323 - val_acc: 0.9113\n",
      "Epoch 51/100\n",
      "2477/2477 [==============================] - 0s 56us/step - loss: 0.1688 - acc: 0.9354 - val_loss: 0.2321 - val_acc: 0.9113\n",
      "Epoch 52/100\n",
      "2477/2477 [==============================] - 0s 60us/step - loss: 0.1693 - acc: 0.9334 - val_loss: 0.2331 - val_acc: 0.9097\n",
      "Epoch 53/100\n",
      "2477/2477 [==============================] - 0s 60us/step - loss: 0.1696 - acc: 0.9346 - val_loss: 0.2346 - val_acc: 0.9145\n",
      "Epoch 54/100\n",
      "2477/2477 [==============================] - 0s 66us/step - loss: 0.1691 - acc: 0.9354 - val_loss: 0.2335 - val_acc: 0.9097\n",
      "Epoch 55/100\n",
      "2477/2477 [==============================] - 0s 65us/step - loss: 0.1697 - acc: 0.9334 - val_loss: 0.2321 - val_acc: 0.9129\n",
      "Epoch 56/100\n",
      "2477/2477 [==============================] - 0s 57us/step - loss: 0.1687 - acc: 0.9342 - val_loss: 0.2342 - val_acc: 0.9097\n",
      "Epoch 57/100\n",
      "2477/2477 [==============================] - 0s 63us/step - loss: 0.1680 - acc: 0.9338 - val_loss: 0.2310 - val_acc: 0.9113\n",
      "Epoch 58/100\n",
      "2477/2477 [==============================] - 0s 69us/step - loss: 0.1694 - acc: 0.9350 - val_loss: 0.2329 - val_acc: 0.9113\n",
      "Epoch 59/100\n",
      "2477/2477 [==============================] - 0s 60us/step - loss: 0.1697 - acc: 0.9334 - val_loss: 0.2365 - val_acc: 0.9129\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2477/2477 [==============================] - 0s 66us/step - loss: 0.1682 - acc: 0.9310 - val_loss: 0.2341 - val_acc: 0.9145\n",
      "Epoch 61/100\n",
      "2477/2477 [==============================] - 0s 67us/step - loss: 0.1682 - acc: 0.9322 - val_loss: 0.2345 - val_acc: 0.9129\n",
      "Epoch 62/100\n",
      "2477/2477 [==============================] - 0s 60us/step - loss: 0.1689 - acc: 0.9350 - val_loss: 0.2343 - val_acc: 0.9113\n",
      "Epoch 63/100\n",
      "2477/2477 [==============================] - 0s 59us/step - loss: 0.1692 - acc: 0.9338 - val_loss: 0.2343 - val_acc: 0.9113\n",
      "Epoch 64/100\n",
      "2477/2477 [==============================] - 0s 57us/step - loss: 0.1677 - acc: 0.9342 - val_loss: 0.2347 - val_acc: 0.9113\n",
      "Epoch 65/100\n",
      "2477/2477 [==============================] - 0s 60us/step - loss: 0.1679 - acc: 0.9330 - val_loss: 0.2376 - val_acc: 0.9129\n",
      "Epoch 66/100\n",
      "2477/2477 [==============================] - 0s 65us/step - loss: 0.1678 - acc: 0.9330 - val_loss: 0.2347 - val_acc: 0.9097\n",
      "Epoch 67/100\n",
      "2477/2477 [==============================] - 0s 68us/step - loss: 0.1679 - acc: 0.9334 - val_loss: 0.2376 - val_acc: 0.9129\n",
      "Epoch 68/100\n",
      "2477/2477 [==============================] - 0s 68us/step - loss: 0.1683 - acc: 0.9342 - val_loss: 0.2374 - val_acc: 0.9113\n",
      "Epoch 69/100\n",
      "2477/2477 [==============================] - 0s 61us/step - loss: 0.1679 - acc: 0.9342 - val_loss: 0.2351 - val_acc: 0.9113\n",
      "Epoch 70/100\n",
      "2477/2477 [==============================] - 0s 58us/step - loss: 0.1679 - acc: 0.9338 - val_loss: 0.2366 - val_acc: 0.9129\n",
      "Epoch 71/100\n",
      "2477/2477 [==============================] - 0s 63us/step - loss: 0.1684 - acc: 0.9330 - val_loss: 0.2345 - val_acc: 0.9113\n",
      "Epoch 72/100\n",
      "2477/2477 [==============================] - 0s 68us/step - loss: 0.1668 - acc: 0.9350 - val_loss: 0.2369 - val_acc: 0.9113\n",
      "Epoch 73/100\n",
      "2477/2477 [==============================] - 0s 62us/step - loss: 0.1701 - acc: 0.9354 - val_loss: 0.2357 - val_acc: 0.9113\n",
      "Epoch 74/100\n",
      "2477/2477 [==============================] - 0s 59us/step - loss: 0.1668 - acc: 0.9342 - val_loss: 0.2384 - val_acc: 0.9161\n",
      "Epoch 75/100\n",
      "2477/2477 [==============================] - 0s 57us/step - loss: 0.1674 - acc: 0.9334 - val_loss: 0.2371 - val_acc: 0.9129\n",
      "Epoch 76/100\n",
      "2477/2477 [==============================] - 0s 58us/step - loss: 0.1676 - acc: 0.9330 - val_loss: 0.2358 - val_acc: 0.9113\n",
      "Epoch 77/100\n",
      "2477/2477 [==============================] - 0s 58us/step - loss: 0.1679 - acc: 0.9322 - val_loss: 0.2351 - val_acc: 0.9097\n",
      "Epoch 78/100\n",
      "2477/2477 [==============================] - 0s 56us/step - loss: 0.1665 - acc: 0.9334 - val_loss: 0.2362 - val_acc: 0.9081\n",
      "Epoch 79/100\n",
      "2477/2477 [==============================] - 0s 58us/step - loss: 0.1666 - acc: 0.9298 - val_loss: 0.2352 - val_acc: 0.9097\n",
      "Epoch 80/100\n",
      "2477/2477 [==============================] - 0s 57us/step - loss: 0.1678 - acc: 0.9342 - val_loss: 0.2371 - val_acc: 0.9113\n",
      "Epoch 81/100\n",
      "2477/2477 [==============================] - 0s 57us/step - loss: 0.1676 - acc: 0.9342 - val_loss: 0.2393 - val_acc: 0.9129\n",
      "Epoch 82/100\n",
      "2477/2477 [==============================] - 0s 74us/step - loss: 0.1666 - acc: 0.9366 - val_loss: 0.2356 - val_acc: 0.9145\n",
      "Epoch 83/100\n",
      "2477/2477 [==============================] - 0s 67us/step - loss: 0.1665 - acc: 0.9358 - val_loss: 0.2374 - val_acc: 0.9129\n",
      "Epoch 84/100\n",
      "2477/2477 [==============================] - 0s 70us/step - loss: 0.1674 - acc: 0.9342 - val_loss: 0.2390 - val_acc: 0.9129\n",
      "Epoch 85/100\n",
      "2477/2477 [==============================] - 0s 62us/step - loss: 0.1670 - acc: 0.9346 - val_loss: 0.2393 - val_acc: 0.9145\n",
      "Epoch 86/100\n",
      "2477/2477 [==============================] - 0s 59us/step - loss: 0.1665 - acc: 0.9358 - val_loss: 0.2364 - val_acc: 0.9097\n",
      "Epoch 87/100\n",
      "2477/2477 [==============================] - 0s 63us/step - loss: 0.1674 - acc: 0.9354 - val_loss: 0.2372 - val_acc: 0.9097\n",
      "Epoch 88/100\n",
      "2477/2477 [==============================] - 0s 62us/step - loss: 0.1660 - acc: 0.9330 - val_loss: 0.2378 - val_acc: 0.9097\n",
      "Epoch 89/100\n",
      "2477/2477 [==============================] - 0s 60us/step - loss: 0.1668 - acc: 0.9326 - val_loss: 0.2391 - val_acc: 0.9113\n",
      "Epoch 90/100\n",
      "2477/2477 [==============================] - 0s 61us/step - loss: 0.1670 - acc: 0.9334 - val_loss: 0.2379 - val_acc: 0.9113\n",
      "Epoch 91/100\n",
      "2477/2477 [==============================] - 0s 62us/step - loss: 0.1669 - acc: 0.9330 - val_loss: 0.2394 - val_acc: 0.9113\n",
      "Epoch 92/100\n",
      "2477/2477 [==============================] - 0s 76us/step - loss: 0.1659 - acc: 0.9342 - val_loss: 0.2392 - val_acc: 0.9097\n",
      "Epoch 93/100\n",
      "2477/2477 [==============================] - 0s 70us/step - loss: 0.1663 - acc: 0.9354 - val_loss: 0.2366 - val_acc: 0.9113\n",
      "Epoch 94/100\n",
      "2477/2477 [==============================] - 0s 72us/step - loss: 0.1661 - acc: 0.9346 - val_loss: 0.2377 - val_acc: 0.9081\n",
      "Epoch 95/100\n",
      "2477/2477 [==============================] - 0s 66us/step - loss: 0.1667 - acc: 0.9322 - val_loss: 0.2376 - val_acc: 0.9081\n",
      "Epoch 96/100\n",
      "2477/2477 [==============================] - 0s 66us/step - loss: 0.1663 - acc: 0.9330 - val_loss: 0.2366 - val_acc: 0.9113\n",
      "Epoch 97/100\n",
      "2477/2477 [==============================] - 0s 64us/step - loss: 0.1672 - acc: 0.9334 - val_loss: 0.2387 - val_acc: 0.9145\n",
      "Epoch 98/100\n",
      "2477/2477 [==============================] - 0s 68us/step - loss: 0.1651 - acc: 0.9354 - val_loss: 0.2370 - val_acc: 0.9145\n",
      "Epoch 99/100\n",
      "2477/2477 [==============================] - 0s 66us/step - loss: 0.1672 - acc: 0.9330 - val_loss: 0.2386 - val_acc: 0.9097\n",
      "Epoch 100/100\n",
      "2477/2477 [==============================] - 0s 58us/step - loss: 0.1664 - acc: 0.9362 - val_loss: 0.2370 - val_acc: 0.9097\n"
     ]
    }
   ],
   "source": [
    "# Training model mylayer\n",
    "\n",
    "model_MyLayer_train = model_MyLayer.fit(train_X, train_Y, epochs=epochs, batch_size=32, validation_data=(valid_X, valid_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PELM_s Result\n",
      "Accuracy : 0.9096774193548387\n",
      "AUC : 0.9091667014048105\n",
      "Sensitivity : 0.922360248447205\n",
      "Specificity : 0.8959731543624161\n",
      "F1 : 0.9138461538461539\n",
      "MCC : 0.8191020237015824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Plot the training accuracy model transformer\n",
    "\n",
    "accuracy = model_MyLayer_train.history['acc']\n",
    "val_accuracy = model_MyLayer_train.history['val_acc']\n",
    "loss = model_MyLayer_train.history['loss']\n",
    "val_loss = model_MyLayer_train.history['val_loss']\n",
    "epochs = range(len(accuracy))\n",
    "plt.plot(epochs, accuracy, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
    "plt.title('MyLayer Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('MyLayer Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "y_pred = np.argmax(model_MyLayer.predict(valid_X), axis=1)\n",
    "y_true = np.argmax(valid_Y, axis = 1)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "f1 = f1_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
    "auc = roc_auc_score(y_true, y_pred, average='macro', sample_weight=None, max_fpr=None)\n",
    "sensi = tp/(tp+fn)\n",
    "specificity = tn/(tn+fp)\n",
    "accu = (tn + tp)/(tn + tp + fn + fp)\n",
    "mcc = ((tp*tn)-(fp*fn))/np.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n",
    "print('{} Result'.format(string_name))\n",
    "print('Accuracy :', accu)\n",
    "print('AUC :', auc)\n",
    "print('Sensitivity :', sensi)\n",
    "print('Specificity :', specificity)\n",
    "print('F1 :', f1)\n",
    "print('MCC :', mcc)\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
