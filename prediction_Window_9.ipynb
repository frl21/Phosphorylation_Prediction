{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Library\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers.core import Dense, Activation, Flatten, Dropout\n",
    "from keras.layers import Conv2D, LSTM, Embedding, Bidirectional, Input, Add, BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from keras.utils import to_categorical\n",
    "from keras import regularizers\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, CSVLogger, LearningRateScheduler\n",
    "from sklearn.metrics import f1_score, roc_auc_score, recall_score, confusion_matrix\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam, SGD\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.initializers import he_uniform\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PELM Dataset, S positive shape:  (1554, 9)\n",
      "PELM Dataset, T positive shape:  (707, 9)\n",
      "PELM Dataset, Y positive shape:  (267, 9)\n",
      "PPA Dataset, S positive shape:  (307, 9)\n",
      "PPA Dataset, T positive shape:  (68, 9)\n",
      "PPA Dataset, Y positive shape:  (51, 9)\n",
      "\n",
      "PELM Dataset, S negative shape:  (1543, 9)\n",
      "PELM Dataset, T negative shape:  (453, 9)\n",
      "PELM Dataset, Y negative shape:  (226, 9)\n",
      "PPA Dataset, S negative shape:  (307, 9)\n",
      "PPA Dataset, T negative shape:  (68, 9)\n",
      "PPA Dataset, Y negative shape:  (51, 9)\n"
     ]
    }
   ],
   "source": [
    "# Read sample from Dataset\n",
    "\n",
    "with open('fixed_sequences_length_9_PELM/Group_Phos_S_pos.fasta', 'r') as f:\n",
    "    PELM_s_positif_txt = f.readlines()\n",
    "with open('fixed_sequences_length_9_PELM/Group_Phos_T_pos.fasta', 'r') as f:\n",
    "    PELM_t_positif_txt = f.readlines()\n",
    "with open('fixed_sequences_length_9_PELM/Group_Phos_Y_pos.fasta', 'r') as f:\n",
    "    PELM_y_positif_txt = f.readlines()\n",
    "with open('fixed_sequences_length_9_PPA/S_IDS_pos.fasta', 'r') as f:\n",
    "    PPA_s_positif_txt = f.readlines()\n",
    "with open('fixed_sequences_length_9_PPA/T_IDS_pos.fasta', 'r') as f:\n",
    "    PPA_t_positif_txt = f.readlines()\n",
    "with open('fixed_sequences_length_9_PPA/Y_IDS_pos.fasta', 'r') as f:\n",
    "    PPA_y_positif_txt = f.readlines()\n",
    "\n",
    "with open('fixed_sequences_length_9_PELM/Group_Phos_S_neg.fasta', 'r') as f:\n",
    "    PELM_s_negatif_txt = f.readlines()\n",
    "with open('fixed_sequences_length_9_PELM/Group_Phos_T_neg.fasta', 'r') as f:\n",
    "    PELM_t_negatif_txt = f.readlines()\n",
    "with open('fixed_sequences_length_9_PELM/Group_Phos_Y_neg.fasta', 'r') as f:\n",
    "    PELM_y_negatif_txt = f.readlines()\n",
    "with open('fixed_sequences_length_9_PPA/S_IDS_neg.fasta', 'r') as f:\n",
    "    PPA_s_negatif_txt = f.readlines()\n",
    "with open('fixed_sequences_length_9_PPA/T_IDS_neg.fasta', 'r') as f:\n",
    "    PPA_t_negatif_txt = f.readlines()\n",
    "with open('fixed_sequences_length_9_PPA/Y_IDS_neg.fasta', 'r') as f:\n",
    "    PPA_y_negatif_txt = f.readlines()\n",
    "\n",
    "# Pick the window 9\n",
    "\n",
    "PELM_s_positif = np.array([])\n",
    "for i in range(1,len(PELM_s_positif_txt),2):\n",
    "    temp = PELM_s_positif_txt[i]\n",
    "    temp1 = temp[0:9]\n",
    "    temp2 = list(temp1)\n",
    "    PELM_s_positif = np.append(PELM_s_positif, temp2)\n",
    "print('PELM Dataset, S positive shape: ', PELM_s_positif.reshape(int(len(PELM_s_positif)/9),9).shape)\n",
    "\n",
    "PELM_t_positif = np.array([])\n",
    "for i in range(1,len(PELM_t_positif_txt),2):\n",
    "    temp = PELM_t_positif_txt[i]\n",
    "    temp1 = temp[0:9]\n",
    "    temp2 = list(temp1)\n",
    "    PELM_t_positif = np.append(PELM_t_positif, temp2)\n",
    "print('PELM Dataset, T positive shape: ', PELM_t_positif.reshape(int(len(PELM_t_positif)/9),9).shape)\n",
    "    \n",
    "PELM_y_positif = np.array([])\n",
    "for i in range(1,len(PELM_y_positif_txt),2):\n",
    "    temp = PELM_y_positif_txt[i]\n",
    "    temp1 = temp[0:9]\n",
    "    temp2 = list(temp1)\n",
    "    PELM_y_positif = np.append(PELM_y_positif, temp2)\n",
    "print('PELM Dataset, Y positive shape: ', PELM_y_positif.reshape(int(len(PELM_y_positif)/9),9).shape)\n",
    "\n",
    "PPA_s_positif = np.array([])\n",
    "for i in range(1,len(PPA_s_positif_txt),2):\n",
    "    temp = PPA_s_positif_txt[i]\n",
    "    temp1 = temp[0:9]\n",
    "    temp2 = list(temp1)\n",
    "    PPA_s_positif = np.append(PPA_s_positif, temp2)\n",
    "print('PPA Dataset, S positive shape: ', PPA_s_positif.reshape(int(len(PPA_s_positif)/9),9).shape)\n",
    "\n",
    "PPA_t_positif = np.array([])\n",
    "for i in range(1,len(PPA_t_positif_txt),2):\n",
    "    temp = PPA_t_positif_txt[i]\n",
    "    temp1 = temp[0:9]\n",
    "    temp2 = list(temp1)\n",
    "    PPA_t_positif = np.append(PPA_t_positif, temp2)\n",
    "print('PPA Dataset, T positive shape: ', PPA_t_positif.reshape(int(len(PPA_t_positif)/9),9).shape)\n",
    "    \n",
    "PPA_y_positif = np.array([])\n",
    "for i in range(1,len(PPA_y_positif_txt),2):\n",
    "    temp = PPA_y_positif_txt[i]\n",
    "    temp1 = temp[0:9]\n",
    "    temp2 = list(temp1)\n",
    "    PPA_y_positif = np.append(PPA_y_positif, temp2)\n",
    "print('PPA Dataset, Y positive shape: ', PPA_y_positif.reshape(int(len(PPA_y_positif)/9),9).shape)\n",
    "\n",
    "print()\n",
    "\n",
    "PELM_s_negatif = np.array([])\n",
    "for i in range(1,len(PELM_s_negatif_txt),2):\n",
    "    temp = PELM_s_negatif_txt[i]\n",
    "    temp1 = temp[0:9]\n",
    "    temp2 = list(temp1)\n",
    "    PELM_s_negatif = np.append(PELM_s_negatif, temp2)\n",
    "print('PELM Dataset, S negative shape: ', PELM_s_negatif.reshape(int(len(PELM_s_negatif)/9),9).shape)\n",
    "\n",
    "PELM_t_negatif = np.array([])\n",
    "for i in range(1,len(PELM_t_negatif_txt),2):\n",
    "    temp = PELM_t_negatif_txt[i]\n",
    "    temp1 = temp[0:9]\n",
    "    temp2 = list(temp1)\n",
    "    PELM_t_negatif = np.append(PELM_t_negatif, temp2)\n",
    "print('PELM Dataset, T negative shape: ', PELM_t_negatif.reshape(int(len(PELM_t_negatif)/9),9).shape)\n",
    "    \n",
    "PELM_y_negatif = np.array([])\n",
    "for i in range(1,len(PELM_y_negatif_txt),2):\n",
    "    temp = PELM_y_negatif_txt[i]\n",
    "    temp1 = temp[0:9]\n",
    "    temp2 = list(temp1)\n",
    "    PELM_y_negatif = np.append(PELM_y_negatif, temp2)\n",
    "print('PELM Dataset, Y negative shape: ', PELM_y_negatif.reshape(int(len(PELM_y_negatif)/9),9).shape)\n",
    "\n",
    "PPA_s_negatif = np.array([])\n",
    "for i in range(1,len(PPA_s_negatif_txt),2):\n",
    "    temp = PPA_s_negatif_txt[i]\n",
    "    temp1 = temp[0:9]\n",
    "    temp2 = list(temp1)\n",
    "    PPA_s_negatif = np.append(PPA_s_negatif, temp2)\n",
    "print('PPA Dataset, S negative shape: ', PPA_s_negatif.reshape(int(len(PPA_s_negatif)/9),9).shape)\n",
    "\n",
    "PPA_t_negatif = np.array([])\n",
    "for i in range(1,len(PPA_t_negatif_txt),2):\n",
    "    temp = PPA_t_negatif_txt[i]\n",
    "    temp1 = temp[0:9]\n",
    "    temp2 = list(temp1)\n",
    "    PPA_t_negatif = np.append(PPA_t_negatif, temp2)\n",
    "print('PPA Dataset, T negative shape: ', PPA_t_negatif.reshape(int(len(PPA_t_negatif)/9),9).shape)\n",
    "    \n",
    "PPA_y_negatif = np.array([])\n",
    "for i in range(1,len(PPA_y_negatif_txt),2):\n",
    "    temp = PPA_y_negatif_txt[i]\n",
    "    temp1 = temp[0:9]\n",
    "    temp2 = list(temp1)\n",
    "    PPA_y_negatif = np.append(PPA_y_negatif, temp2)\n",
    "print('PPA Dataset, Y negative shape: ', PPA_y_negatif.reshape(int(len(PPA_y_negatif)/9),9).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Dataset shape:  (1554, 9)\n",
      "Positive Label shape:  (1554, 1)\n",
      "Negative Dataset shape:  (1543, 9)\n",
      "Negative Label shape:  (1543, 1)\n"
     ]
    }
   ],
   "source": [
    "# Choose Dataset to train, make sure correspond with negative dataset\n",
    "\n",
    "dataset_pos = PELM_s_positif\n",
    "dataset_neg = PELM_s_negatif\n",
    "string_name = 'PELM_s'\n",
    "\n",
    "# Expand dimension, Reshape and Create Label\n",
    "\n",
    "sequenceLP = int(len(dataset_pos)/9)\n",
    "dataset_pos = np.expand_dims(dataset_pos, axis=0)\n",
    "dataset_pos = dataset_pos.reshape(sequenceLP,9)\n",
    "label_pos = np.ones((sequenceLP,), dtype=int)\n",
    "label_pos = np.expand_dims(label_pos, axis=0)\n",
    "label_pos = label_pos.reshape(sequenceLP,1)\n",
    "\n",
    "sequenceLN = int(len(dataset_neg)/9)\n",
    "dataset_neg = np.expand_dims(dataset_neg, axis=0)\n",
    "dataset_neg = dataset_neg.reshape(sequenceLN,9)\n",
    "label_neg = np.zeros((sequenceLN,), dtype=int)\n",
    "label_neg = np.expand_dims(label_neg, axis=0)\n",
    "label_neg = label_neg.reshape(sequenceLN,1)\n",
    "\n",
    "# Validate\n",
    "\n",
    "print('Positive Dataset shape: ', dataset_pos.shape)\n",
    "print('Positive Label shape: ', label_pos.shape)\n",
    "print('Negative Dataset shape: ', dataset_neg.shape)\n",
    "print('Negative Label shape: ', label_neg.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main X shape:  (3097, 9)\n",
      "main Y shape:  (3097, 2)\n",
      "Training sample shape:  (2788, 9)\n",
      "Training label shape:  (2788, 2)\n",
      "Validation sample shape:  (309, 9)\n",
      "Validation label shape:  (309, 2)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dataset preparation\n",
    "\n",
    "dataset_X = np.concatenate((dataset_pos, dataset_neg), axis=0, out=None)\n",
    "dataset_Y = np.concatenate((label_pos, label_neg), axis=0, out=None)\n",
    "\n",
    "# Tokenizing, Unique character got its own number\n",
    "\n",
    "asam = ['A','R','N','D','C','Q','E','G','H','I','L','K','M','F','P','S','T','W','Y','V']\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(asam)\n",
    "dataset_X_token = []\n",
    "for i in range(len(dataset_X)):\n",
    "    temp = tokenizer.texts_to_sequences(dataset_X[i])\n",
    "    dataset_X_token = np.append(dataset_X_token, temp)\n",
    "\n",
    "dataset_X_token = dataset_X_token-1\n",
    "dataset_X_token = dataset_X_token.reshape(len(dataset_X),9)\n",
    "\n",
    "# Onehot\n",
    "\n",
    "dataset_X_token_onehot = to_categorical(dataset_X_token)\n",
    "dataset_X_token_onehot = np.expand_dims(dataset_X_token_onehot, axis=3)\n",
    "dataset_X_token_onehot = dataset_X_token_onehot.reshape(len(dataset_X),9,20,1)\n",
    "\n",
    "dataset_Y_onehot = to_categorical(dataset_Y)\n",
    "\n",
    "# Shuffle Dataset\n",
    "\n",
    "main_X, main_Y = shuffle(dataset_X_token, dataset_Y_onehot, random_state=13)\n",
    "\n",
    "# Validation\n",
    "\n",
    "print('main X shape: ', main_X.shape)\n",
    "print('main Y shape: ', main_Y.shape)\n",
    "\n",
    "# Divide into 10 dataset for cross validation\n",
    "\n",
    "pjg = len(main_X)\n",
    "A = int(pjg/10)\n",
    "B = int(pjg/10*2)\n",
    "C = int(pjg/10*3)\n",
    "D = int(pjg/10*4)\n",
    "E = int(pjg/10*5)\n",
    "F = int(pjg/10*6)\n",
    "G = int(pjg/10*7)\n",
    "H = int(pjg/10*8)\n",
    "I = int(pjg/10*9)\n",
    "\n",
    "train_X1 = main_X[A:pjg]\n",
    "train_Y1 = main_Y[A:pjg]\n",
    "valid_X1 = main_X[0:A]\n",
    "valid_Y1 = main_Y[0:A]\n",
    "\n",
    "train_X2 = np.vstack((main_X[0:A], main_X[B:pjg]))\n",
    "train_Y2 = np.vstack((main_Y[0:A], main_Y[B:pjg]))\n",
    "valid_X2 = main_X[A:B]\n",
    "valid_Y2 = main_Y[A:B]\n",
    "\n",
    "train_X3 = np.vstack((main_X[0:B], main_X[C:pjg]))\n",
    "train_Y3 = np.vstack((main_Y[0:B], main_Y[C:pjg]))\n",
    "valid_X3 = main_X[B:C]\n",
    "valid_Y3 = main_Y[B:C]\n",
    "\n",
    "train_X4 = np.vstack((main_X[0:C], main_X[D:pjg]))\n",
    "train_Y4 = np.vstack((main_Y[0:C], main_Y[D:pjg]))\n",
    "valid_X4 = main_X[C:D]\n",
    "valid_Y4 = main_Y[C:D]\n",
    "\n",
    "train_X5 = np.vstack((main_X[0:D], main_X[E:pjg]))\n",
    "train_Y5 = np.vstack((main_Y[0:D], main_Y[E:pjg]))\n",
    "valid_X5 = main_X[D:E]\n",
    "valid_Y5 = main_Y[D:E]\n",
    "\n",
    "train_X6 = np.vstack((main_X[0:E], main_X[F:pjg]))\n",
    "train_Y6 = np.vstack((main_Y[0:E], main_Y[F:pjg]))\n",
    "valid_X6 = main_X[E:F]\n",
    "valid_Y6 = main_Y[E:F]\n",
    "\n",
    "train_X7 = np.vstack((main_X[0:F], main_X[G:pjg]))\n",
    "train_Y7 = np.vstack((main_Y[0:F], main_Y[G:pjg]))\n",
    "valid_X7 = main_X[F:G]\n",
    "valid_Y7 = main_Y[F:G]\n",
    "\n",
    "train_X8 = np.vstack((main_X[0:G], main_X[H:pjg]))\n",
    "train_Y8 = np.vstack((main_Y[0:G], main_Y[H:pjg]))\n",
    "valid_X8 = main_X[G:H]\n",
    "valid_Y8 = main_Y[G:H]\n",
    "\n",
    "train_X9 = np.vstack((main_X[0:H], main_X[I:pjg]))\n",
    "train_Y9 = np.vstack((main_Y[0:H], main_Y[I:pjg]))\n",
    "valid_X9 = main_X[H:I]\n",
    "valid_Y9 = main_Y[H:I]\n",
    "\n",
    "train_X10 = main_X[0:I]\n",
    "train_Y10 = main_Y[0:I]\n",
    "valid_X10 = main_X[I:pjg]\n",
    "valid_Y10 = main_Y[I:pjg]\n",
    "\n",
    "# Validation\n",
    "\n",
    "print('Training sample shape: ', train_X1.shape)\n",
    "print('Training label shape: ', train_Y1.shape)\n",
    "print('Validation sample shape: ', valid_X1.shape)\n",
    "print('Validation label shape: ', valid_Y1.shape)\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 9)                 0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 9, 10)             200       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 90)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 90)                360       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2912      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "output_layer (Dense)         (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 7,218\n",
      "Trainable params: 6,782\n",
      "Non-trainable params: 436\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Neural Network Modeling\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "checkpoint1 = ModelCheckpoint('weight_best1.hdf5', monitor='val_loss', verbose=0, save_best_only=True, \n",
    "                             save_weights_only=False, mode='auto', period=1)\n",
    "checkpoint2 = ModelCheckpoint('weight_best2.hdf5', monitor='val_loss', verbose=0, save_best_only=True, \n",
    "                             save_weights_only=False, mode='auto', period=1)\n",
    "checkpoint3 = ModelCheckpoint('weight_best3.hdf5', monitor='val_loss', verbose=0, save_best_only=True, \n",
    "                             save_weights_only=False, mode='auto', period=1)\n",
    "checkpoint4 = ModelCheckpoint('weight_best4.hdf5', monitor='val_loss', verbose=0, save_best_only=True, \n",
    "                             save_weights_only=False, mode='auto', period=1)\n",
    "checkpoint5 = ModelCheckpoint('weight_best5.hdf5', monitor='val_loss', verbose=0, save_best_only=True, \n",
    "                             save_weights_only=False, mode='auto', period=1)\n",
    "checkpoint6 = ModelCheckpoint('weight_best6.hdf5', monitor='val_loss', verbose=0, save_best_only=True, \n",
    "                             save_weights_only=False, mode='auto', period=1)\n",
    "checkpoint7 = ModelCheckpoint('weight_best7.hdf5', monitor='val_loss', verbose=0, save_best_only=True, \n",
    "                             save_weights_only=False, mode='auto', period=1)\n",
    "checkpoint8 = ModelCheckpoint('weight_best8.hdf5', monitor='val_loss', verbose=0, save_best_only=True, \n",
    "                             save_weights_only=False, mode='auto', period=1)\n",
    "checkpoint9 = ModelCheckpoint('weight_best9.hdf5', monitor='val_loss', verbose=0, save_best_only=True, \n",
    "                             save_weights_only=False, mode='auto', period=1)\n",
    "checkpoint10 = ModelCheckpoint('weight_best10.hdf5', monitor='val_loss', verbose=0, save_best_only=True, \n",
    "                             save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "def scheduler(epoch):\n",
    "    if epoch > 70:\n",
    "        return 0.0001\n",
    "    elif epoch > 40:\n",
    "        return 0.0002\n",
    "    elif epoch > 10:\n",
    "        return 0.0005\n",
    "    else:\n",
    "        return 0.001 \n",
    "\n",
    "lr_schedule= LearningRateScheduler(scheduler)\n",
    "callback_list1 = [checkpoint1, lr_schedule]\n",
    "callback_list2 = [checkpoint2, lr_schedule]\n",
    "callback_list3 = [checkpoint3, lr_schedule]\n",
    "callback_list4 = [checkpoint4, lr_schedule]\n",
    "callback_list5 = [checkpoint5, lr_schedule]\n",
    "callback_list6 = [checkpoint6, lr_schedule]\n",
    "callback_list7 = [checkpoint7, lr_schedule]\n",
    "callback_list8 = [checkpoint8, lr_schedule]\n",
    "callback_list9 = [checkpoint9, lr_schedule]\n",
    "callback_list10 = [checkpoint10, lr_schedule]\n",
    "\n",
    "# Input layer\n",
    "\n",
    "NODES = 13\n",
    "DROPOUT = 0.1\n",
    "BIAS = 0.001\n",
    "KERNEL = 0.001\n",
    "LR = 0.01\n",
    "\n",
    "inp = Input(shape=(9,))\n",
    "x = inp\n",
    "opt = Adam(lr=LR)\n",
    "\n",
    "# Hidden layers\n",
    "x = Embedding(20, 10, input_length=9)(x)\n",
    "x = Flatten(data_format=None)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(32, activation='elu', kernel_initializer='he_uniform', bias_regularizer=regularizers.l2(BIAS), kernel_regularizer=regularizers.l2(KERNEL))(x)\n",
    "x = Dropout(DROPOUT, noise_shape=None, seed=None)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(32, activation='elu', kernel_initializer='he_uniform', bias_regularizer=regularizers.l2(BIAS), kernel_regularizer=regularizers.l2(KERNEL))(x)\n",
    "x = Dropout(DROPOUT, noise_shape=None, seed=None)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(32, activation='elu', kernel_initializer='he_uniform', bias_regularizer=regularizers.l2(BIAS), kernel_regularizer=regularizers.l2(KERNEL))(x)\n",
    "x = Dropout(DROPOUT, noise_shape=None, seed=None)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(32, activation='elu', kernel_initializer='he_uniform', bias_regularizer=regularizers.l2(BIAS), kernel_regularizer=regularizers.l2(KERNEL))(x)\n",
    "x = Dropout(DROPOUT, noise_shape=None, seed=None)(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "\n",
    "output = Dense(2, activation='softmax', bias_regularizer=regularizers.l2(BIAS), kernel_regularizer=regularizers.l2(KERNEL), name='output_layer')(x)\n",
    "\n",
    "model = Model(inputs=inp, outputs=output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "model.save_weights('model.hdf5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Train 1\n",
      "Train on 2788 samples, validate on 309 samples\n",
      "Epoch 1/100\n",
      "2788/2788 [==============================] - 2s 660us/step - loss: 0.8489 - acc: 0.7457 - val_loss: 0.5193 - val_acc: 0.8964\n",
      "Epoch 2/100\n",
      "2788/2788 [==============================] - 1s 202us/step - loss: 0.5664 - acc: 0.8730 - val_loss: 0.4564 - val_acc: 0.9288\n",
      "Epoch 3/100\n",
      "2788/2788 [==============================] - 1s 202us/step - loss: 0.5216 - acc: 0.8877 - val_loss: 0.4393 - val_acc: 0.9159\n",
      "Epoch 4/100\n",
      "2788/2788 [==============================] - 1s 203us/step - loss: 0.5015 - acc: 0.8913 - val_loss: 0.4221 - val_acc: 0.9320\n",
      "Epoch 5/100\n",
      "2788/2788 [==============================] - 1s 202us/step - loss: 0.4793 - acc: 0.9021 - val_loss: 0.4177 - val_acc: 0.9417\n",
      "Epoch 6/100\n",
      "2788/2788 [==============================] - 1s 203us/step - loss: 0.4591 - acc: 0.8999 - val_loss: 0.4089 - val_acc: 0.9353\n",
      "Epoch 7/100\n",
      "2788/2788 [==============================] - 1s 205us/step - loss: 0.4572 - acc: 0.9028 - val_loss: 0.4109 - val_acc: 0.9482\n",
      "Epoch 8/100\n",
      "2788/2788 [==============================] - 1s 200us/step - loss: 0.4492 - acc: 0.9060 - val_loss: 0.4046 - val_acc: 0.9482\n",
      "Epoch 9/100\n",
      "2788/2788 [==============================] - 1s 200us/step - loss: 0.4393 - acc: 0.9057 - val_loss: 0.4004 - val_acc: 0.9482\n",
      "Epoch 10/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.4279 - acc: 0.9078 - val_loss: 0.3955 - val_acc: 0.9417\n",
      "Epoch 11/100\n",
      "2788/2788 [==============================] - 1s 201us/step - loss: 0.4260 - acc: 0.9035 - val_loss: 0.3898 - val_acc: 0.9417\n",
      "Epoch 12/100\n",
      "2788/2788 [==============================] - 1s 201us/step - loss: 0.4058 - acc: 0.9132 - val_loss: 0.3854 - val_acc: 0.9320\n",
      "Epoch 13/100\n",
      "2788/2788 [==============================] - 1s 202us/step - loss: 0.3986 - acc: 0.9125 - val_loss: 0.3835 - val_acc: 0.9353\n",
      "Epoch 14/100\n",
      "2788/2788 [==============================] - 1s 200us/step - loss: 0.3938 - acc: 0.9150 - val_loss: 0.3820 - val_acc: 0.9417\n",
      "Epoch 15/100\n",
      "2788/2788 [==============================] - 1s 200us/step - loss: 0.3778 - acc: 0.9229 - val_loss: 0.3769 - val_acc: 0.9450\n",
      "Epoch 16/100\n",
      "2788/2788 [==============================] - 1s 201us/step - loss: 0.3882 - acc: 0.9143 - val_loss: 0.3703 - val_acc: 0.9450\n",
      "Epoch 17/100\n",
      "2788/2788 [==============================] - 1s 201us/step - loss: 0.3730 - acc: 0.9229 - val_loss: 0.3726 - val_acc: 0.9450\n",
      "Epoch 18/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.3817 - acc: 0.9211 - val_loss: 0.3741 - val_acc: 0.9385\n",
      "Epoch 19/100\n",
      "2788/2788 [==============================] - 1s 202us/step - loss: 0.3722 - acc: 0.9175 - val_loss: 0.3690 - val_acc: 0.9353\n",
      "Epoch 20/100\n",
      "2788/2788 [==============================] - 1s 202us/step - loss: 0.3747 - acc: 0.9179 - val_loss: 0.3709 - val_acc: 0.9385\n",
      "Epoch 21/100\n",
      "2788/2788 [==============================] - 1s 206us/step - loss: 0.3547 - acc: 0.9232 - val_loss: 0.3609 - val_acc: 0.9353\n",
      "Epoch 22/100\n",
      "2788/2788 [==============================] - 1s 201us/step - loss: 0.3667 - acc: 0.9157 - val_loss: 0.3675 - val_acc: 0.9417\n",
      "Epoch 23/100\n",
      "2788/2788 [==============================] - 1s 200us/step - loss: 0.3623 - acc: 0.9182 - val_loss: 0.3558 - val_acc: 0.9385\n",
      "Epoch 24/100\n",
      "2788/2788 [==============================] - 1s 200us/step - loss: 0.3506 - acc: 0.9232 - val_loss: 0.3514 - val_acc: 0.9417\n",
      "Epoch 25/100\n",
      "2788/2788 [==============================] - 1s 200us/step - loss: 0.3590 - acc: 0.9189 - val_loss: 0.3515 - val_acc: 0.9417\n",
      "Epoch 26/100\n",
      "2788/2788 [==============================] - 1s 201us/step - loss: 0.3417 - acc: 0.9258 - val_loss: 0.3475 - val_acc: 0.9450\n",
      "Epoch 27/100\n",
      "2788/2788 [==============================] - 1s 201us/step - loss: 0.3505 - acc: 0.9197 - val_loss: 0.3495 - val_acc: 0.9353\n",
      "Epoch 28/100\n",
      "2788/2788 [==============================] - 1s 203us/step - loss: 0.3256 - acc: 0.9319 - val_loss: 0.3451 - val_acc: 0.9417\n",
      "Epoch 29/100\n",
      "2788/2788 [==============================] - 1s 200us/step - loss: 0.3324 - acc: 0.9189 - val_loss: 0.3413 - val_acc: 0.9353\n",
      "Epoch 30/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.3236 - acc: 0.9279 - val_loss: 0.3421 - val_acc: 0.9385\n",
      "Epoch 31/100\n",
      "2788/2788 [==============================] - 1s 200us/step - loss: 0.3263 - acc: 0.9193 - val_loss: 0.3479 - val_acc: 0.9320\n",
      "Epoch 32/100\n",
      "2788/2788 [==============================] - 1s 202us/step - loss: 0.3270 - acc: 0.9254 - val_loss: 0.3449 - val_acc: 0.9385\n",
      "Epoch 33/100\n",
      "2788/2788 [==============================] - 1s 200us/step - loss: 0.3522 - acc: 0.9096 - val_loss: 0.3414 - val_acc: 0.9385\n",
      "Epoch 34/100\n",
      "2788/2788 [==============================] - 1s 202us/step - loss: 0.3178 - acc: 0.9236 - val_loss: 0.3437 - val_acc: 0.9353\n",
      "Epoch 35/100\n",
      "2788/2788 [==============================] - 1s 200us/step - loss: 0.3154 - acc: 0.9265 - val_loss: 0.3372 - val_acc: 0.9353\n",
      "Epoch 36/100\n",
      "2788/2788 [==============================] - 1s 203us/step - loss: 0.2998 - acc: 0.9322 - val_loss: 0.3323 - val_acc: 0.9353\n",
      "Epoch 37/100\n",
      "2788/2788 [==============================] - 1s 208us/step - loss: 0.2988 - acc: 0.9315 - val_loss: 0.3316 - val_acc: 0.9417\n",
      "Epoch 38/100\n",
      "2788/2788 [==============================] - 1s 200us/step - loss: 0.3029 - acc: 0.9319 - val_loss: 0.3336 - val_acc: 0.9320\n",
      "Epoch 39/100\n",
      "2788/2788 [==============================] - 1s 206us/step - loss: 0.3126 - acc: 0.9204 - val_loss: 0.3356 - val_acc: 0.9353\n",
      "Epoch 40/100\n",
      "2788/2788 [==============================] - 1s 203us/step - loss: 0.3023 - acc: 0.9315 - val_loss: 0.3318 - val_acc: 0.9256\n",
      "Epoch 41/100\n",
      "2788/2788 [==============================] - 1s 200us/step - loss: 0.3039 - acc: 0.9240 - val_loss: 0.3255 - val_acc: 0.9417\n",
      "Epoch 42/100\n",
      "2788/2788 [==============================] - 1s 201us/step - loss: 0.2803 - acc: 0.9362 - val_loss: 0.3234 - val_acc: 0.9450\n",
      "Epoch 43/100\n",
      "2788/2788 [==============================] - 1s 200us/step - loss: 0.2837 - acc: 0.9319 - val_loss: 0.3218 - val_acc: 0.9417\n",
      "Epoch 44/100\n",
      "2788/2788 [==============================] - 1s 201us/step - loss: 0.2783 - acc: 0.9362 - val_loss: 0.3247 - val_acc: 0.9353\n",
      "Epoch 45/100\n",
      "2788/2788 [==============================] - 1s 201us/step - loss: 0.2896 - acc: 0.9301 - val_loss: 0.3237 - val_acc: 0.9385\n",
      "Epoch 46/100\n",
      "2788/2788 [==============================] - 1s 201us/step - loss: 0.2856 - acc: 0.9304 - val_loss: 0.3235 - val_acc: 0.9385\n",
      "Epoch 47/100\n",
      "2788/2788 [==============================] - 1s 204us/step - loss: 0.2911 - acc: 0.9268 - val_loss: 0.3222 - val_acc: 0.9385\n",
      "Epoch 48/100\n",
      "2788/2788 [==============================] - 1s 209us/step - loss: 0.2900 - acc: 0.9283 - val_loss: 0.3248 - val_acc: 0.9353\n",
      "Epoch 49/100\n",
      "2788/2788 [==============================] - 1s 218us/step - loss: 0.2819 - acc: 0.9326 - val_loss: 0.3209 - val_acc: 0.9385\n",
      "Epoch 50/100\n",
      "2788/2788 [==============================] - 1s 205us/step - loss: 0.2785 - acc: 0.9293 - val_loss: 0.3176 - val_acc: 0.9450\n",
      "Epoch 51/100\n",
      "2788/2788 [==============================] - 1s 203us/step - loss: 0.2766 - acc: 0.9372 - val_loss: 0.3179 - val_acc: 0.9385\n",
      "Epoch 52/100\n",
      "2788/2788 [==============================] - 1s 201us/step - loss: 0.2673 - acc: 0.9365 - val_loss: 0.3153 - val_acc: 0.9450\n",
      "Epoch 53/100\n",
      "2788/2788 [==============================] - 1s 202us/step - loss: 0.2710 - acc: 0.9390 - val_loss: 0.3160 - val_acc: 0.9450\n",
      "Epoch 54/100\n",
      "2788/2788 [==============================] - 1s 200us/step - loss: 0.2763 - acc: 0.9369 - val_loss: 0.3137 - val_acc: 0.9417\n",
      "Epoch 55/100\n",
      "2788/2788 [==============================] - 1s 204us/step - loss: 0.2719 - acc: 0.9397 - val_loss: 0.3104 - val_acc: 0.9450\n",
      "Epoch 56/100\n",
      "2788/2788 [==============================] - 1s 201us/step - loss: 0.2807 - acc: 0.9297 - val_loss: 0.3106 - val_acc: 0.9417\n",
      "Epoch 57/100\n",
      "2788/2788 [==============================] - 1s 201us/step - loss: 0.2745 - acc: 0.9376 - val_loss: 0.3121 - val_acc: 0.9385\n",
      "Epoch 58/100\n",
      "2788/2788 [==============================] - 1s 202us/step - loss: 0.2651 - acc: 0.9408 - val_loss: 0.3106 - val_acc: 0.9385\n",
      "Epoch 59/100\n",
      "2788/2788 [==============================] - 1s 200us/step - loss: 0.2702 - acc: 0.9340 - val_loss: 0.3154 - val_acc: 0.9482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2686 - acc: 0.9344 - val_loss: 0.3148 - val_acc: 0.9450\n",
      "Epoch 61/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2673 - acc: 0.9376 - val_loss: 0.3151 - val_acc: 0.9417\n",
      "Epoch 62/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2651 - acc: 0.9397 - val_loss: 0.3123 - val_acc: 0.9417\n",
      "Epoch 63/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2706 - acc: 0.9344 - val_loss: 0.3130 - val_acc: 0.9353\n",
      "Epoch 64/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2614 - acc: 0.9372 - val_loss: 0.3134 - val_acc: 0.9353\n",
      "Epoch 65/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2681 - acc: 0.9372 - val_loss: 0.3131 - val_acc: 0.9385\n",
      "Epoch 66/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2529 - acc: 0.9433 - val_loss: 0.3114 - val_acc: 0.9417\n",
      "Epoch 67/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2719 - acc: 0.9340 - val_loss: 0.3105 - val_acc: 0.9482\n",
      "Epoch 68/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2657 - acc: 0.9376 - val_loss: 0.3122 - val_acc: 0.9515\n",
      "Epoch 69/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2709 - acc: 0.9336 - val_loss: 0.3129 - val_acc: 0.9450\n",
      "Epoch 70/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2678 - acc: 0.9326 - val_loss: 0.3106 - val_acc: 0.9450\n",
      "Epoch 71/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2610 - acc: 0.9383 - val_loss: 0.3068 - val_acc: 0.9482\n",
      "Epoch 72/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2580 - acc: 0.9405 - val_loss: 0.3071 - val_acc: 0.9482\n",
      "Epoch 73/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2668 - acc: 0.9315 - val_loss: 0.3031 - val_acc: 0.9482\n",
      "Epoch 74/100\n",
      "2788/2788 [==============================] - 1s 197us/step - loss: 0.2410 - acc: 0.9415 - val_loss: 0.3028 - val_acc: 0.9482\n",
      "Epoch 75/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2639 - acc: 0.9308 - val_loss: 0.3021 - val_acc: 0.9547\n",
      "Epoch 76/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2543 - acc: 0.9362 - val_loss: 0.3018 - val_acc: 0.9385\n",
      "Epoch 77/100\n",
      "2788/2788 [==============================] - 1s 197us/step - loss: 0.2511 - acc: 0.9358 - val_loss: 0.3013 - val_acc: 0.9482\n",
      "Epoch 78/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2522 - acc: 0.9397 - val_loss: 0.3036 - val_acc: 0.9385\n",
      "Epoch 79/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2489 - acc: 0.9372 - val_loss: 0.3039 - val_acc: 0.9417\n",
      "Epoch 80/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2513 - acc: 0.9426 - val_loss: 0.3044 - val_acc: 0.9385\n",
      "Epoch 81/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2476 - acc: 0.9415 - val_loss: 0.3063 - val_acc: 0.9385\n",
      "Epoch 82/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2819 - acc: 0.9315 - val_loss: 0.3025 - val_acc: 0.9417\n",
      "Epoch 83/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2445 - acc: 0.9466 - val_loss: 0.3016 - val_acc: 0.9385\n",
      "Epoch 84/100\n",
      "2788/2788 [==============================] - 1s 201us/step - loss: 0.2431 - acc: 0.9415 - val_loss: 0.3037 - val_acc: 0.9353\n",
      "Epoch 85/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2546 - acc: 0.9387 - val_loss: 0.3033 - val_acc: 0.9385\n",
      "Epoch 86/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2419 - acc: 0.9426 - val_loss: 0.3034 - val_acc: 0.9417\n",
      "Epoch 87/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2495 - acc: 0.9315 - val_loss: 0.3024 - val_acc: 0.9450\n",
      "Epoch 88/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2448 - acc: 0.9383 - val_loss: 0.3042 - val_acc: 0.9482\n",
      "Epoch 89/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2617 - acc: 0.9354 - val_loss: 0.3055 - val_acc: 0.9417\n",
      "Epoch 90/100\n",
      "2788/2788 [==============================] - 1s 208us/step - loss: 0.2371 - acc: 0.9451 - val_loss: 0.3051 - val_acc: 0.9385\n",
      "Epoch 91/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2371 - acc: 0.9480 - val_loss: 0.3062 - val_acc: 0.9417\n",
      "Epoch 92/100\n",
      "2788/2788 [==============================] - 1s 200us/step - loss: 0.2494 - acc: 0.9369 - val_loss: 0.3058 - val_acc: 0.9417\n",
      "Epoch 93/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2526 - acc: 0.9415 - val_loss: 0.3054 - val_acc: 0.9385\n",
      "Epoch 94/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2532 - acc: 0.9433 - val_loss: 0.3036 - val_acc: 0.9450\n",
      "Epoch 95/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2519 - acc: 0.9430 - val_loss: 0.3027 - val_acc: 0.9450\n",
      "Epoch 96/100\n",
      "2788/2788 [==============================] - 1s 197us/step - loss: 0.2420 - acc: 0.9455 - val_loss: 0.3055 - val_acc: 0.9417\n",
      "Epoch 97/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2401 - acc: 0.9437 - val_loss: 0.3048 - val_acc: 0.9482\n",
      "Epoch 98/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2438 - acc: 0.9466 - val_loss: 0.3040 - val_acc: 0.9417\n",
      "Epoch 99/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2408 - acc: 0.9426 - val_loss: 0.3042 - val_acc: 0.9450\n",
      "Epoch 100/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2514 - acc: 0.9433 - val_loss: 0.3037 - val_acc: 0.9515\n",
      "Model Train 2\n",
      "Train on 2787 samples, validate on 310 samples\n",
      "Epoch 1/100\n",
      "2787/2787 [==============================] - 1s 200us/step - loss: 0.7828 - acc: 0.7693 - val_loss: 0.6026 - val_acc: 0.8613\n",
      "Epoch 2/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.5309 - acc: 0.8888 - val_loss: 0.5153 - val_acc: 0.8903\n",
      "Epoch 3/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.4904 - acc: 0.9024 - val_loss: 0.4893 - val_acc: 0.9065\n",
      "Epoch 4/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.4897 - acc: 0.8959 - val_loss: 0.4687 - val_acc: 0.9226\n",
      "Epoch 5/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.4763 - acc: 0.9060 - val_loss: 0.4547 - val_acc: 0.9097\n",
      "Epoch 6/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.4558 - acc: 0.9085 - val_loss: 0.4551 - val_acc: 0.9258\n",
      "Epoch 7/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.4506 - acc: 0.9135 - val_loss: 0.4458 - val_acc: 0.9161\n",
      "Epoch 8/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.4365 - acc: 0.9107 - val_loss: 0.4405 - val_acc: 0.9258\n",
      "Epoch 9/100\n",
      "2787/2787 [==============================] - 1s 203us/step - loss: 0.4460 - acc: 0.9049 - val_loss: 0.4288 - val_acc: 0.9226\n",
      "Epoch 10/100\n",
      "2787/2787 [==============================] - 1s 200us/step - loss: 0.4120 - acc: 0.9203 - val_loss: 0.4387 - val_acc: 0.9161\n",
      "Epoch 11/100\n",
      "2787/2787 [==============================] - 1s 200us/step - loss: 0.4243 - acc: 0.9128 - val_loss: 0.4163 - val_acc: 0.9161\n",
      "Epoch 12/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.4014 - acc: 0.9142 - val_loss: 0.4164 - val_acc: 0.9226\n",
      "Epoch 13/100\n",
      "2787/2787 [==============================] - 1s 200us/step - loss: 0.4061 - acc: 0.9196 - val_loss: 0.4152 - val_acc: 0.9226\n",
      "Epoch 14/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.4076 - acc: 0.9121 - val_loss: 0.4111 - val_acc: 0.9226\n",
      "Epoch 15/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3912 - acc: 0.9203 - val_loss: 0.4126 - val_acc: 0.9194\n",
      "Epoch 16/100\n",
      "2787/2787 [==============================] - 1s 200us/step - loss: 0.3846 - acc: 0.9243 - val_loss: 0.4074 - val_acc: 0.9226\n",
      "Epoch 17/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3882 - acc: 0.9164 - val_loss: 0.4072 - val_acc: 0.9194\n",
      "Epoch 18/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3821 - acc: 0.9182 - val_loss: 0.4070 - val_acc: 0.9194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3800 - acc: 0.9207 - val_loss: 0.4048 - val_acc: 0.9129\n",
      "Epoch 20/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3844 - acc: 0.9146 - val_loss: 0.3979 - val_acc: 0.9194\n",
      "Epoch 21/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3638 - acc: 0.9211 - val_loss: 0.3949 - val_acc: 0.9161\n",
      "Epoch 22/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3556 - acc: 0.9236 - val_loss: 0.3897 - val_acc: 0.9161\n",
      "Epoch 23/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3472 - acc: 0.9286 - val_loss: 0.3905 - val_acc: 0.9097\n",
      "Epoch 24/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3591 - acc: 0.9200 - val_loss: 0.3876 - val_acc: 0.9161\n",
      "Epoch 25/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3673 - acc: 0.9142 - val_loss: 0.3921 - val_acc: 0.9194\n",
      "Epoch 26/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3507 - acc: 0.9229 - val_loss: 0.3909 - val_acc: 0.9161\n",
      "Epoch 27/100\n",
      "2787/2787 [==============================] - 1s 197us/step - loss: 0.3567 - acc: 0.9196 - val_loss: 0.3913 - val_acc: 0.9161\n",
      "Epoch 28/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3369 - acc: 0.9243 - val_loss: 0.3835 - val_acc: 0.9194\n",
      "Epoch 29/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3336 - acc: 0.9279 - val_loss: 0.3818 - val_acc: 0.9129\n",
      "Epoch 30/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3521 - acc: 0.9132 - val_loss: 0.3775 - val_acc: 0.9097\n",
      "Epoch 31/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3361 - acc: 0.9250 - val_loss: 0.3770 - val_acc: 0.9129\n",
      "Epoch 32/100\n",
      "2787/2787 [==============================] - 1s 206us/step - loss: 0.3395 - acc: 0.9203 - val_loss: 0.3683 - val_acc: 0.9161\n",
      "Epoch 33/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3303 - acc: 0.9211 - val_loss: 0.3632 - val_acc: 0.9129\n",
      "Epoch 34/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3136 - acc: 0.9268 - val_loss: 0.3653 - val_acc: 0.9129\n",
      "Epoch 35/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3332 - acc: 0.9196 - val_loss: 0.3586 - val_acc: 0.9226\n",
      "Epoch 36/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3145 - acc: 0.9243 - val_loss: 0.3547 - val_acc: 0.9226\n",
      "Epoch 37/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3123 - acc: 0.9250 - val_loss: 0.3557 - val_acc: 0.9226\n",
      "Epoch 38/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2998 - acc: 0.9264 - val_loss: 0.3564 - val_acc: 0.9161\n",
      "Epoch 39/100\n",
      "2787/2787 [==============================] - 1s 201us/step - loss: 0.3216 - acc: 0.9200 - val_loss: 0.3650 - val_acc: 0.9226\n",
      "Epoch 40/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3056 - acc: 0.9243 - val_loss: 0.3568 - val_acc: 0.9194\n",
      "Epoch 41/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3051 - acc: 0.9293 - val_loss: 0.3507 - val_acc: 0.9129\n",
      "Epoch 42/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2935 - acc: 0.9376 - val_loss: 0.3523 - val_acc: 0.9161\n",
      "Epoch 43/100\n",
      "2787/2787 [==============================] - 1s 204us/step - loss: 0.2927 - acc: 0.9304 - val_loss: 0.3510 - val_acc: 0.9161\n",
      "Epoch 44/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2884 - acc: 0.9354 - val_loss: 0.3486 - val_acc: 0.9194\n",
      "Epoch 45/100\n",
      "2787/2787 [==============================] - 1s 201us/step - loss: 0.2990 - acc: 0.9297 - val_loss: 0.3486 - val_acc: 0.9290\n",
      "Epoch 46/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3058 - acc: 0.9279 - val_loss: 0.3486 - val_acc: 0.9226\n",
      "Epoch 47/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2865 - acc: 0.9397 - val_loss: 0.3498 - val_acc: 0.9226\n",
      "Epoch 48/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2744 - acc: 0.9412 - val_loss: 0.3474 - val_acc: 0.9258\n",
      "Epoch 49/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2903 - acc: 0.9318 - val_loss: 0.3500 - val_acc: 0.9226\n",
      "Epoch 50/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2980 - acc: 0.9236 - val_loss: 0.3455 - val_acc: 0.9194\n",
      "Epoch 51/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2861 - acc: 0.9368 - val_loss: 0.3452 - val_acc: 0.9226\n",
      "Epoch 52/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2797 - acc: 0.9372 - val_loss: 0.3409 - val_acc: 0.9161\n",
      "Epoch 53/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2892 - acc: 0.9311 - val_loss: 0.3442 - val_acc: 0.9161\n",
      "Epoch 54/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2889 - acc: 0.9307 - val_loss: 0.3445 - val_acc: 0.9194\n",
      "Epoch 55/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2961 - acc: 0.9300 - val_loss: 0.3428 - val_acc: 0.9226\n",
      "Epoch 56/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2694 - acc: 0.9379 - val_loss: 0.3436 - val_acc: 0.9194\n",
      "Epoch 57/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2831 - acc: 0.9333 - val_loss: 0.3443 - val_acc: 0.9194\n",
      "Epoch 58/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2825 - acc: 0.9322 - val_loss: 0.3433 - val_acc: 0.9129\n",
      "Epoch 59/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2763 - acc: 0.9329 - val_loss: 0.3402 - val_acc: 0.9161\n",
      "Epoch 60/100\n",
      "2787/2787 [==============================] - 1s 200us/step - loss: 0.2794 - acc: 0.9333 - val_loss: 0.3419 - val_acc: 0.9161\n",
      "Epoch 61/100\n",
      "2787/2787 [==============================] - 1s 200us/step - loss: 0.2675 - acc: 0.9354 - val_loss: 0.3430 - val_acc: 0.9129\n",
      "Epoch 62/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2875 - acc: 0.9304 - val_loss: 0.3437 - val_acc: 0.9161\n",
      "Epoch 63/100\n",
      "2787/2787 [==============================] - 1s 200us/step - loss: 0.2662 - acc: 0.9433 - val_loss: 0.3427 - val_acc: 0.9194\n",
      "Epoch 64/100\n",
      "2787/2787 [==============================] - 1s 200us/step - loss: 0.2892 - acc: 0.9257 - val_loss: 0.3406 - val_acc: 0.9226\n",
      "Epoch 65/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2729 - acc: 0.9368 - val_loss: 0.3369 - val_acc: 0.9226\n",
      "Epoch 66/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2793 - acc: 0.9290 - val_loss: 0.3345 - val_acc: 0.9258\n",
      "Epoch 67/100\n",
      "2787/2787 [==============================] - 1s 197us/step - loss: 0.2715 - acc: 0.9376 - val_loss: 0.3450 - val_acc: 0.9161\n",
      "Epoch 68/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2598 - acc: 0.9401 - val_loss: 0.3397 - val_acc: 0.9129\n",
      "Epoch 69/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2738 - acc: 0.9322 - val_loss: 0.3396 - val_acc: 0.9161\n",
      "Epoch 70/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2742 - acc: 0.9318 - val_loss: 0.3389 - val_acc: 0.9161\n",
      "Epoch 71/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2752 - acc: 0.9336 - val_loss: 0.3400 - val_acc: 0.9194\n",
      "Epoch 72/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2567 - acc: 0.9422 - val_loss: 0.3413 - val_acc: 0.9194\n",
      "Epoch 73/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2789 - acc: 0.9257 - val_loss: 0.3381 - val_acc: 0.9194\n",
      "Epoch 74/100\n",
      "2787/2787 [==============================] - 1s 201us/step - loss: 0.2724 - acc: 0.9347 - val_loss: 0.3403 - val_acc: 0.9194\n",
      "Epoch 75/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2610 - acc: 0.9401 - val_loss: 0.3409 - val_acc: 0.9129\n",
      "Epoch 76/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2664 - acc: 0.9351 - val_loss: 0.3406 - val_acc: 0.9129\n",
      "Epoch 77/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2661 - acc: 0.9372 - val_loss: 0.3388 - val_acc: 0.9161\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2703 - acc: 0.9336 - val_loss: 0.3374 - val_acc: 0.9161\n",
      "Epoch 79/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2686 - acc: 0.9365 - val_loss: 0.3379 - val_acc: 0.9161\n",
      "Epoch 80/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2671 - acc: 0.9376 - val_loss: 0.3366 - val_acc: 0.9161\n",
      "Epoch 81/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2646 - acc: 0.9361 - val_loss: 0.3367 - val_acc: 0.9129\n",
      "Epoch 82/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2643 - acc: 0.9347 - val_loss: 0.3356 - val_acc: 0.9161\n",
      "Epoch 83/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2541 - acc: 0.9376 - val_loss: 0.3367 - val_acc: 0.9129\n",
      "Epoch 84/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2757 - acc: 0.9304 - val_loss: 0.3378 - val_acc: 0.9161\n",
      "Epoch 85/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2671 - acc: 0.9383 - val_loss: 0.3361 - val_acc: 0.9097\n",
      "Epoch 86/100\n",
      "2787/2787 [==============================] - 1s 197us/step - loss: 0.2577 - acc: 0.9379 - val_loss: 0.3379 - val_acc: 0.9097\n",
      "Epoch 87/100\n",
      "2787/2787 [==============================] - 1s 200us/step - loss: 0.2626 - acc: 0.9333 - val_loss: 0.3380 - val_acc: 0.9129\n",
      "Epoch 88/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2547 - acc: 0.9408 - val_loss: 0.3384 - val_acc: 0.9129\n",
      "Epoch 89/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2582 - acc: 0.9419 - val_loss: 0.3374 - val_acc: 0.9129\n",
      "Epoch 90/100\n",
      "2787/2787 [==============================] - 1s 197us/step - loss: 0.2694 - acc: 0.9329 - val_loss: 0.3383 - val_acc: 0.9129\n",
      "Epoch 91/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2714 - acc: 0.9304 - val_loss: 0.3377 - val_acc: 0.9129\n",
      "Epoch 92/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2486 - acc: 0.9429 - val_loss: 0.3386 - val_acc: 0.9129\n",
      "Epoch 93/100\n",
      "2787/2787 [==============================] - 1s 201us/step - loss: 0.2562 - acc: 0.9390 - val_loss: 0.3404 - val_acc: 0.9161\n",
      "Epoch 94/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2566 - acc: 0.9386 - val_loss: 0.3388 - val_acc: 0.9161\n",
      "Epoch 95/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2517 - acc: 0.9368 - val_loss: 0.3379 - val_acc: 0.9097\n",
      "Epoch 96/100\n",
      "2787/2787 [==============================] - 1s 204us/step - loss: 0.2547 - acc: 0.9412 - val_loss: 0.3366 - val_acc: 0.9065\n",
      "Epoch 97/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2562 - acc: 0.9354 - val_loss: 0.3354 - val_acc: 0.9129\n",
      "Epoch 98/100\n",
      "2787/2787 [==============================] - 1s 200us/step - loss: 0.2605 - acc: 0.9379 - val_loss: 0.3338 - val_acc: 0.9129\n",
      "Epoch 99/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2445 - acc: 0.9419 - val_loss: 0.3357 - val_acc: 0.9129\n",
      "Epoch 100/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2535 - acc: 0.9390 - val_loss: 0.3362 - val_acc: 0.9097\n",
      "Model Train 3\n",
      "Train on 2787 samples, validate on 310 samples\n",
      "Epoch 1/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.7885 - acc: 0.7679 - val_loss: 0.4859 - val_acc: 0.9129\n",
      "Epoch 2/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.5349 - acc: 0.8888 - val_loss: 0.4543 - val_acc: 0.9258\n",
      "Epoch 3/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.5052 - acc: 0.8916 - val_loss: 0.4405 - val_acc: 0.9258\n",
      "Epoch 4/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.4813 - acc: 0.8985 - val_loss: 0.4286 - val_acc: 0.9290\n",
      "Epoch 5/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.4845 - acc: 0.8985 - val_loss: 0.4235 - val_acc: 0.9290\n",
      "Epoch 6/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.4586 - acc: 0.9046 - val_loss: 0.4149 - val_acc: 0.9290\n",
      "Epoch 7/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.4395 - acc: 0.9171 - val_loss: 0.4167 - val_acc: 0.9194\n",
      "Epoch 8/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.4482 - acc: 0.9089 - val_loss: 0.4065 - val_acc: 0.9258\n",
      "Epoch 9/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.4266 - acc: 0.9067 - val_loss: 0.4014 - val_acc: 0.9161\n",
      "Epoch 10/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.4296 - acc: 0.9046 - val_loss: 0.4030 - val_acc: 0.9129\n",
      "Epoch 11/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.4103 - acc: 0.9117 - val_loss: 0.3825 - val_acc: 0.9226\n",
      "Epoch 12/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.4023 - acc: 0.9186 - val_loss: 0.3835 - val_acc: 0.9194\n",
      "Epoch 13/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3968 - acc: 0.9196 - val_loss: 0.3784 - val_acc: 0.9194\n",
      "Epoch 14/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3963 - acc: 0.9203 - val_loss: 0.3798 - val_acc: 0.9258\n",
      "Epoch 15/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3998 - acc: 0.9128 - val_loss: 0.3782 - val_acc: 0.9290\n",
      "Epoch 16/100\n",
      "2787/2787 [==============================] - 1s 206us/step - loss: 0.3903 - acc: 0.9132 - val_loss: 0.3768 - val_acc: 0.9194\n",
      "Epoch 17/100\n",
      "2787/2787 [==============================] - 1s 200us/step - loss: 0.3851 - acc: 0.9175 - val_loss: 0.3752 - val_acc: 0.9226\n",
      "Epoch 18/100\n",
      "2787/2787 [==============================] - 1s 200us/step - loss: 0.3801 - acc: 0.9186 - val_loss: 0.3683 - val_acc: 0.9226\n",
      "Epoch 19/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3896 - acc: 0.9085 - val_loss: 0.3677 - val_acc: 0.9194\n",
      "Epoch 20/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3607 - acc: 0.9275 - val_loss: 0.3681 - val_acc: 0.9226\n",
      "Epoch 21/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3775 - acc: 0.9168 - val_loss: 0.3764 - val_acc: 0.9065\n",
      "Epoch 22/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3776 - acc: 0.9142 - val_loss: 0.3698 - val_acc: 0.9194\n",
      "Epoch 23/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3633 - acc: 0.9257 - val_loss: 0.3680 - val_acc: 0.9161\n",
      "Epoch 24/100\n",
      "2787/2787 [==============================] - 1s 202us/step - loss: 0.3639 - acc: 0.9229 - val_loss: 0.3634 - val_acc: 0.9161\n",
      "Epoch 25/100\n",
      "2787/2787 [==============================] - 1s 201us/step - loss: 0.3580 - acc: 0.9236 - val_loss: 0.3595 - val_acc: 0.9194\n",
      "Epoch 26/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3482 - acc: 0.9264 - val_loss: 0.3548 - val_acc: 0.9258\n",
      "Epoch 27/100\n",
      "2787/2787 [==============================] - 1s 200us/step - loss: 0.3588 - acc: 0.9182 - val_loss: 0.3516 - val_acc: 0.9258\n",
      "Epoch 28/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3482 - acc: 0.9250 - val_loss: 0.3476 - val_acc: 0.9161\n",
      "Epoch 29/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3586 - acc: 0.9157 - val_loss: 0.3458 - val_acc: 0.9161\n",
      "Epoch 30/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3379 - acc: 0.9225 - val_loss: 0.3535 - val_acc: 0.9129\n",
      "Epoch 31/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3407 - acc: 0.9243 - val_loss: 0.3467 - val_acc: 0.9161\n",
      "Epoch 32/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3394 - acc: 0.9218 - val_loss: 0.3483 - val_acc: 0.9097\n",
      "Epoch 33/100\n",
      "2787/2787 [==============================] - 1s 204us/step - loss: 0.3407 - acc: 0.9232 - val_loss: 0.3486 - val_acc: 0.9065\n",
      "Epoch 34/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3248 - acc: 0.9293 - val_loss: 0.3400 - val_acc: 0.9129\n",
      "Epoch 35/100\n",
      "2787/2787 [==============================] - 1s 200us/step - loss: 0.3287 - acc: 0.9164 - val_loss: 0.3389 - val_acc: 0.9161\n",
      "Epoch 36/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3181 - acc: 0.9272 - val_loss: 0.3369 - val_acc: 0.9161\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3292 - acc: 0.9257 - val_loss: 0.3408 - val_acc: 0.9161\n",
      "Epoch 38/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3131 - acc: 0.9275 - val_loss: 0.3373 - val_acc: 0.9129\n",
      "Epoch 39/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3065 - acc: 0.9290 - val_loss: 0.3310 - val_acc: 0.9129\n",
      "Epoch 40/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3058 - acc: 0.9297 - val_loss: 0.3282 - val_acc: 0.9161\n",
      "Epoch 41/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2974 - acc: 0.9325 - val_loss: 0.3233 - val_acc: 0.9194\n",
      "Epoch 42/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3047 - acc: 0.9318 - val_loss: 0.3231 - val_acc: 0.9161\n",
      "Epoch 43/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3080 - acc: 0.9268 - val_loss: 0.3246 - val_acc: 0.9161\n",
      "Epoch 44/100\n",
      "2787/2787 [==============================] - 1s 201us/step - loss: 0.3176 - acc: 0.9254 - val_loss: 0.3243 - val_acc: 0.9161\n",
      "Epoch 45/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2986 - acc: 0.9239 - val_loss: 0.3256 - val_acc: 0.9129\n",
      "Epoch 46/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3090 - acc: 0.9264 - val_loss: 0.3275 - val_acc: 0.9097\n",
      "Epoch 47/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2923 - acc: 0.9340 - val_loss: 0.3228 - val_acc: 0.9097\n",
      "Epoch 48/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2992 - acc: 0.9282 - val_loss: 0.3222 - val_acc: 0.9097\n",
      "Epoch 49/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3060 - acc: 0.9268 - val_loss: 0.3235 - val_acc: 0.9097\n",
      "Epoch 50/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3037 - acc: 0.9232 - val_loss: 0.3251 - val_acc: 0.9065\n",
      "Epoch 51/100\n",
      "2787/2787 [==============================] - 1s 202us/step - loss: 0.3003 - acc: 0.9264 - val_loss: 0.3224 - val_acc: 0.9065\n",
      "Epoch 52/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2807 - acc: 0.9343 - val_loss: 0.3213 - val_acc: 0.9129\n",
      "Epoch 53/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3027 - acc: 0.9268 - val_loss: 0.3244 - val_acc: 0.9032\n",
      "Epoch 54/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2828 - acc: 0.9336 - val_loss: 0.3221 - val_acc: 0.9129\n",
      "Epoch 55/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2872 - acc: 0.9340 - val_loss: 0.3200 - val_acc: 0.9194\n",
      "Epoch 56/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2894 - acc: 0.9329 - val_loss: 0.3118 - val_acc: 0.9194\n",
      "Epoch 57/100\n",
      "2787/2787 [==============================] - 1s 197us/step - loss: 0.2815 - acc: 0.9372 - val_loss: 0.3177 - val_acc: 0.9194\n",
      "Epoch 58/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2787 - acc: 0.9304 - val_loss: 0.3158 - val_acc: 0.9194\n",
      "Epoch 59/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2784 - acc: 0.9365 - val_loss: 0.3139 - val_acc: 0.9194\n",
      "Epoch 60/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2856 - acc: 0.9300 - val_loss: 0.3162 - val_acc: 0.9194\n",
      "Epoch 61/100\n",
      "2787/2787 [==============================] - 1s 200us/step - loss: 0.2775 - acc: 0.9404 - val_loss: 0.3137 - val_acc: 0.9194\n",
      "Epoch 62/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2853 - acc: 0.9297 - val_loss: 0.3156 - val_acc: 0.9194\n",
      "Epoch 63/100\n",
      "2787/2787 [==============================] - 1s 202us/step - loss: 0.2750 - acc: 0.9365 - val_loss: 0.3189 - val_acc: 0.9194\n",
      "Epoch 64/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2830 - acc: 0.9347 - val_loss: 0.3190 - val_acc: 0.9161\n",
      "Epoch 65/100\n",
      "2787/2787 [==============================] - 1s 200us/step - loss: 0.2828 - acc: 0.9297 - val_loss: 0.3195 - val_acc: 0.9161\n",
      "Epoch 66/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2791 - acc: 0.9325 - val_loss: 0.3203 - val_acc: 0.9129\n",
      "Epoch 67/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2684 - acc: 0.9412 - val_loss: 0.3187 - val_acc: 0.9129\n",
      "Epoch 68/100\n",
      "2787/2787 [==============================] - 1s 200us/step - loss: 0.2826 - acc: 0.9318 - val_loss: 0.3197 - val_acc: 0.9129\n",
      "Epoch 69/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2764 - acc: 0.9297 - val_loss: 0.3167 - val_acc: 0.9129\n",
      "Epoch 70/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2728 - acc: 0.9361 - val_loss: 0.3153 - val_acc: 0.9129\n",
      "Epoch 71/100\n",
      "2787/2787 [==============================] - 1s 200us/step - loss: 0.2609 - acc: 0.9383 - val_loss: 0.3184 - val_acc: 0.9161\n",
      "Epoch 72/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2983 - acc: 0.9186 - val_loss: 0.3172 - val_acc: 0.9161\n",
      "Epoch 73/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2710 - acc: 0.9365 - val_loss: 0.3155 - val_acc: 0.9161\n",
      "Epoch 74/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2678 - acc: 0.9361 - val_loss: 0.3128 - val_acc: 0.9129\n",
      "Epoch 75/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2695 - acc: 0.9351 - val_loss: 0.3129 - val_acc: 0.9161\n",
      "Epoch 76/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2741 - acc: 0.9397 - val_loss: 0.3131 - val_acc: 0.9161\n",
      "Epoch 77/100\n",
      "2787/2787 [==============================] - 1s 200us/step - loss: 0.2875 - acc: 0.9304 - val_loss: 0.3141 - val_acc: 0.9161\n",
      "Epoch 78/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2600 - acc: 0.9408 - val_loss: 0.3094 - val_acc: 0.9194\n",
      "Epoch 79/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2884 - acc: 0.9282 - val_loss: 0.3097 - val_acc: 0.9161\n",
      "Epoch 80/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2893 - acc: 0.9211 - val_loss: 0.3088 - val_acc: 0.9129\n",
      "Epoch 81/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2849 - acc: 0.9236 - val_loss: 0.3051 - val_acc: 0.9161\n",
      "Epoch 82/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2690 - acc: 0.9347 - val_loss: 0.3094 - val_acc: 0.9129\n",
      "Epoch 83/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2570 - acc: 0.9390 - val_loss: 0.3067 - val_acc: 0.9194\n",
      "Epoch 84/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2763 - acc: 0.9300 - val_loss: 0.3047 - val_acc: 0.9161\n",
      "Epoch 85/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2699 - acc: 0.9361 - val_loss: 0.3066 - val_acc: 0.9226\n",
      "Epoch 86/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2755 - acc: 0.9329 - val_loss: 0.3044 - val_acc: 0.9194\n",
      "Epoch 87/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2741 - acc: 0.9333 - val_loss: 0.3048 - val_acc: 0.9194\n",
      "Epoch 88/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2571 - acc: 0.9383 - val_loss: 0.3048 - val_acc: 0.9194\n",
      "Epoch 89/100\n",
      "2787/2787 [==============================] - 1s 200us/step - loss: 0.2627 - acc: 0.9376 - val_loss: 0.3079 - val_acc: 0.9226\n",
      "Epoch 90/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2693 - acc: 0.9325 - val_loss: 0.3074 - val_acc: 0.9226\n",
      "Epoch 91/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2699 - acc: 0.9329 - val_loss: 0.3054 - val_acc: 0.9258\n",
      "Epoch 92/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2819 - acc: 0.9261 - val_loss: 0.3077 - val_acc: 0.9258\n",
      "Epoch 93/100\n",
      "2787/2787 [==============================] - 1s 200us/step - loss: 0.2701 - acc: 0.9365 - val_loss: 0.3071 - val_acc: 0.9226\n",
      "Epoch 94/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2664 - acc: 0.9368 - val_loss: 0.3080 - val_acc: 0.9194\n",
      "Epoch 95/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2747 - acc: 0.9325 - val_loss: 0.3092 - val_acc: 0.9226\n",
      "Epoch 96/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2551 - acc: 0.9444 - val_loss: 0.3093 - val_acc: 0.9226\n",
      "Epoch 97/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2572 - acc: 0.9422 - val_loss: 0.3062 - val_acc: 0.9226\n",
      "Epoch 98/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2770 - acc: 0.9315 - val_loss: 0.3046 - val_acc: 0.9226\n",
      "Epoch 99/100\n",
      "2787/2787 [==============================] - 1s 197us/step - loss: 0.2714 - acc: 0.9340 - val_loss: 0.3052 - val_acc: 0.9258\n",
      "Epoch 100/100\n",
      "2787/2787 [==============================] - 1s 197us/step - loss: 0.2525 - acc: 0.9444 - val_loss: 0.3073 - val_acc: 0.9226\n",
      "Model Train 4\n",
      "Train on 2788 samples, validate on 309 samples\n",
      "Epoch 1/100\n",
      "2788/2788 [==============================] - 1s 197us/step - loss: 0.8322 - acc: 0.7482 - val_loss: 0.5121 - val_acc: 0.8932\n",
      "Epoch 2/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.5607 - acc: 0.8712 - val_loss: 0.4878 - val_acc: 0.9126\n",
      "Epoch 3/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.5093 - acc: 0.8902 - val_loss: 0.4750 - val_acc: 0.9061\n",
      "Epoch 4/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.5056 - acc: 0.8949 - val_loss: 0.4638 - val_acc: 0.9094\n",
      "Epoch 5/100\n",
      "2788/2788 [==============================] - 1s 197us/step - loss: 0.4696 - acc: 0.9032 - val_loss: 0.4508 - val_acc: 0.9159\n",
      "Epoch 6/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.4602 - acc: 0.9082 - val_loss: 0.4429 - val_acc: 0.9191\n",
      "Epoch 7/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.4592 - acc: 0.9093 - val_loss: 0.4406 - val_acc: 0.9126\n",
      "Epoch 8/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.4453 - acc: 0.9118 - val_loss: 0.4327 - val_acc: 0.9126\n",
      "Epoch 9/100\n",
      "2788/2788 [==============================] - 1s 197us/step - loss: 0.4324 - acc: 0.9118 - val_loss: 0.4212 - val_acc: 0.9223\n",
      "Epoch 10/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.4440 - acc: 0.9082 - val_loss: 0.4202 - val_acc: 0.9223\n",
      "Epoch 11/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.4145 - acc: 0.9171 - val_loss: 0.4102 - val_acc: 0.9223\n",
      "Epoch 12/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.4206 - acc: 0.9164 - val_loss: 0.4024 - val_acc: 0.9191\n",
      "Epoch 13/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.4061 - acc: 0.9193 - val_loss: 0.4018 - val_acc: 0.9159\n",
      "Epoch 14/100\n",
      "2788/2788 [==============================] - 1s 197us/step - loss: 0.4098 - acc: 0.9128 - val_loss: 0.3998 - val_acc: 0.9223\n",
      "Epoch 15/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.3972 - acc: 0.9222 - val_loss: 0.3996 - val_acc: 0.9191\n",
      "Epoch 16/100\n",
      "2788/2788 [==============================] - 1s 201us/step - loss: 0.3996 - acc: 0.9204 - val_loss: 0.4001 - val_acc: 0.9223\n",
      "Epoch 17/100\n",
      "2788/2788 [==============================] - 1s 200us/step - loss: 0.4088 - acc: 0.9114 - val_loss: 0.3974 - val_acc: 0.9191\n",
      "Epoch 18/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.3915 - acc: 0.9154 - val_loss: 0.3925 - val_acc: 0.9191\n",
      "Epoch 19/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.3865 - acc: 0.9193 - val_loss: 0.3895 - val_acc: 0.9094\n",
      "Epoch 20/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.3788 - acc: 0.9222 - val_loss: 0.3870 - val_acc: 0.9126\n",
      "Epoch 21/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.3847 - acc: 0.9125 - val_loss: 0.3845 - val_acc: 0.9126\n",
      "Epoch 22/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.3769 - acc: 0.9204 - val_loss: 0.3817 - val_acc: 0.9126\n",
      "Epoch 23/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.3847 - acc: 0.9136 - val_loss: 0.3837 - val_acc: 0.9159\n",
      "Epoch 24/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.3796 - acc: 0.9168 - val_loss: 0.3786 - val_acc: 0.9191\n",
      "Epoch 25/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.3608 - acc: 0.9211 - val_loss: 0.3741 - val_acc: 0.9223\n",
      "Epoch 26/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.3552 - acc: 0.9283 - val_loss: 0.3716 - val_acc: 0.9256\n",
      "Epoch 27/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.3610 - acc: 0.9207 - val_loss: 0.3667 - val_acc: 0.9223\n",
      "Epoch 28/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.3546 - acc: 0.9247 - val_loss: 0.3609 - val_acc: 0.9288\n",
      "Epoch 29/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.3468 - acc: 0.9225 - val_loss: 0.3602 - val_acc: 0.9256\n",
      "Epoch 30/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.3404 - acc: 0.9207 - val_loss: 0.3662 - val_acc: 0.9191\n",
      "Epoch 31/100\n",
      "2788/2788 [==============================] - 1s 197us/step - loss: 0.3355 - acc: 0.9297 - val_loss: 0.3604 - val_acc: 0.9094\n",
      "Epoch 32/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.3407 - acc: 0.9222 - val_loss: 0.3563 - val_acc: 0.9223\n",
      "Epoch 33/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.3317 - acc: 0.9243 - val_loss: 0.3599 - val_acc: 0.9191\n",
      "Epoch 34/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.3316 - acc: 0.9211 - val_loss: 0.3495 - val_acc: 0.9223\n",
      "Epoch 35/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.3282 - acc: 0.9236 - val_loss: 0.3517 - val_acc: 0.9191\n",
      "Epoch 36/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.3284 - acc: 0.9240 - val_loss: 0.3433 - val_acc: 0.9191\n",
      "Epoch 37/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.3316 - acc: 0.9171 - val_loss: 0.3435 - val_acc: 0.9191\n",
      "Epoch 38/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.3235 - acc: 0.9232 - val_loss: 0.3383 - val_acc: 0.9256\n",
      "Epoch 39/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.3171 - acc: 0.9211 - val_loss: 0.3425 - val_acc: 0.9223\n",
      "Epoch 40/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.3022 - acc: 0.9311 - val_loss: 0.3318 - val_acc: 0.9223\n",
      "Epoch 41/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.3178 - acc: 0.9265 - val_loss: 0.3338 - val_acc: 0.9288\n",
      "Epoch 42/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.3089 - acc: 0.9301 - val_loss: 0.3369 - val_acc: 0.9223\n",
      "Epoch 43/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2888 - acc: 0.9394 - val_loss: 0.3337 - val_acc: 0.9191\n",
      "Epoch 44/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2973 - acc: 0.9279 - val_loss: 0.3296 - val_acc: 0.9320\n",
      "Epoch 45/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2931 - acc: 0.9376 - val_loss: 0.3266 - val_acc: 0.9288\n",
      "Epoch 46/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.3011 - acc: 0.9336 - val_loss: 0.3264 - val_acc: 0.9191\n",
      "Epoch 47/100\n",
      "2788/2788 [==============================] - 1s 197us/step - loss: 0.2957 - acc: 0.9351 - val_loss: 0.3272 - val_acc: 0.9191\n",
      "Epoch 48/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2850 - acc: 0.9362 - val_loss: 0.3266 - val_acc: 0.9223\n",
      "Epoch 49/100\n",
      "2788/2788 [==============================] - 1s 200us/step - loss: 0.3064 - acc: 0.9279 - val_loss: 0.3290 - val_acc: 0.9223\n",
      "Epoch 50/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2852 - acc: 0.9333 - val_loss: 0.3290 - val_acc: 0.9191\n",
      "Epoch 51/100\n",
      "2788/2788 [==============================] - 1s 200us/step - loss: 0.2874 - acc: 0.9304 - val_loss: 0.3255 - val_acc: 0.9256\n",
      "Epoch 52/100\n",
      "2788/2788 [==============================] - 1s 200us/step - loss: 0.2764 - acc: 0.9405 - val_loss: 0.3259 - val_acc: 0.9223\n",
      "Epoch 53/100\n",
      "2788/2788 [==============================] - 1s 212us/step - loss: 0.2810 - acc: 0.9390 - val_loss: 0.3252 - val_acc: 0.9256\n",
      "Epoch 54/100\n",
      "2788/2788 [==============================] - 1s 200us/step - loss: 0.2822 - acc: 0.9340 - val_loss: 0.3243 - val_acc: 0.9256\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2788/2788 [==============================] - 1s 203us/step - loss: 0.2914 - acc: 0.9293 - val_loss: 0.3266 - val_acc: 0.9223\n",
      "Epoch 56/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2725 - acc: 0.9405 - val_loss: 0.3300 - val_acc: 0.9223\n",
      "Epoch 57/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2678 - acc: 0.9426 - val_loss: 0.3272 - val_acc: 0.9191\n",
      "Epoch 58/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2779 - acc: 0.9336 - val_loss: 0.3271 - val_acc: 0.9159\n",
      "Epoch 59/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2757 - acc: 0.9358 - val_loss: 0.3278 - val_acc: 0.9223\n",
      "Epoch 60/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2694 - acc: 0.9369 - val_loss: 0.3256 - val_acc: 0.9126\n",
      "Epoch 61/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2843 - acc: 0.9279 - val_loss: 0.3236 - val_acc: 0.9191\n",
      "Epoch 62/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2822 - acc: 0.9347 - val_loss: 0.3203 - val_acc: 0.9159\n",
      "Epoch 63/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2759 - acc: 0.9319 - val_loss: 0.3194 - val_acc: 0.9223\n",
      "Epoch 64/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2738 - acc: 0.9365 - val_loss: 0.3188 - val_acc: 0.9256\n",
      "Epoch 65/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2779 - acc: 0.9344 - val_loss: 0.3174 - val_acc: 0.9256\n",
      "Epoch 66/100\n",
      "2788/2788 [==============================] - 1s 197us/step - loss: 0.2667 - acc: 0.9419 - val_loss: 0.3172 - val_acc: 0.9256\n",
      "Epoch 67/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2662 - acc: 0.9405 - val_loss: 0.3190 - val_acc: 0.9223\n",
      "Epoch 68/100\n",
      "2788/2788 [==============================] - 1s 197us/step - loss: 0.2774 - acc: 0.9297 - val_loss: 0.3151 - val_acc: 0.9191\n",
      "Epoch 69/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2672 - acc: 0.9354 - val_loss: 0.3179 - val_acc: 0.9191\n",
      "Epoch 70/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2622 - acc: 0.9397 - val_loss: 0.3125 - val_acc: 0.9223\n",
      "Epoch 71/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2637 - acc: 0.9387 - val_loss: 0.3113 - val_acc: 0.9191\n",
      "Epoch 72/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2573 - acc: 0.9379 - val_loss: 0.3152 - val_acc: 0.9191\n",
      "Epoch 73/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2578 - acc: 0.9390 - val_loss: 0.3130 - val_acc: 0.9191\n",
      "Epoch 74/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2697 - acc: 0.9383 - val_loss: 0.3107 - val_acc: 0.9223\n",
      "Epoch 75/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2609 - acc: 0.9415 - val_loss: 0.3113 - val_acc: 0.9191\n",
      "Epoch 76/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2651 - acc: 0.9347 - val_loss: 0.3125 - val_acc: 0.9223\n",
      "Epoch 77/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2645 - acc: 0.9344 - val_loss: 0.3158 - val_acc: 0.9191\n",
      "Epoch 78/100\n",
      "2788/2788 [==============================] - 1s 200us/step - loss: 0.2688 - acc: 0.9369 - val_loss: 0.3177 - val_acc: 0.9191\n",
      "Epoch 79/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2584 - acc: 0.9401 - val_loss: 0.3170 - val_acc: 0.9191\n",
      "Epoch 80/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2557 - acc: 0.9412 - val_loss: 0.3117 - val_acc: 0.9223\n",
      "Epoch 81/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2644 - acc: 0.9379 - val_loss: 0.3151 - val_acc: 0.9191\n",
      "Epoch 82/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2647 - acc: 0.9390 - val_loss: 0.3128 - val_acc: 0.9256\n",
      "Epoch 83/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2512 - acc: 0.9426 - val_loss: 0.3125 - val_acc: 0.9159\n",
      "Epoch 84/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2598 - acc: 0.9376 - val_loss: 0.3149 - val_acc: 0.9191\n",
      "Epoch 85/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2583 - acc: 0.9412 - val_loss: 0.3146 - val_acc: 0.9159\n",
      "Epoch 86/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2581 - acc: 0.9390 - val_loss: 0.3126 - val_acc: 0.9223\n",
      "Epoch 87/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2693 - acc: 0.9301 - val_loss: 0.3108 - val_acc: 0.9191\n",
      "Epoch 88/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2661 - acc: 0.9372 - val_loss: 0.3103 - val_acc: 0.9159\n",
      "Epoch 89/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2688 - acc: 0.9301 - val_loss: 0.3098 - val_acc: 0.9126\n",
      "Epoch 90/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2503 - acc: 0.9437 - val_loss: 0.3098 - val_acc: 0.9159\n",
      "Epoch 91/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2605 - acc: 0.9387 - val_loss: 0.3098 - val_acc: 0.9159\n",
      "Epoch 92/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2452 - acc: 0.9412 - val_loss: 0.3043 - val_acc: 0.9191\n",
      "Epoch 93/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2619 - acc: 0.9354 - val_loss: 0.3065 - val_acc: 0.9191\n",
      "Epoch 94/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2562 - acc: 0.9351 - val_loss: 0.3074 - val_acc: 0.9159\n",
      "Epoch 95/100\n",
      "2788/2788 [==============================] - 1s 200us/step - loss: 0.2475 - acc: 0.9394 - val_loss: 0.3083 - val_acc: 0.9159\n",
      "Epoch 96/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2489 - acc: 0.9397 - val_loss: 0.3096 - val_acc: 0.9159\n",
      "Epoch 97/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2544 - acc: 0.9347 - val_loss: 0.3114 - val_acc: 0.9126\n",
      "Epoch 98/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2495 - acc: 0.9365 - val_loss: 0.3096 - val_acc: 0.9159\n",
      "Epoch 99/100\n",
      "2788/2788 [==============================] - 1s 201us/step - loss: 0.2595 - acc: 0.9351 - val_loss: 0.3130 - val_acc: 0.9159\n",
      "Epoch 100/100\n",
      "2788/2788 [==============================] - 1s 203us/step - loss: 0.2493 - acc: 0.9405 - val_loss: 0.3106 - val_acc: 0.9159\n",
      "Model Train 5\n",
      "Train on 2787 samples, validate on 310 samples\n",
      "Epoch 1/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.7355 - acc: 0.7937 - val_loss: 0.5084 - val_acc: 0.8968\n",
      "Epoch 2/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.5295 - acc: 0.8906 - val_loss: 0.4615 - val_acc: 0.9194\n",
      "Epoch 3/100\n",
      "2787/2787 [==============================] - 1s 203us/step - loss: 0.5052 - acc: 0.8924 - val_loss: 0.4436 - val_acc: 0.9161\n",
      "Epoch 4/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.4978 - acc: 0.8963 - val_loss: 0.4444 - val_acc: 0.9226\n",
      "Epoch 5/100\n",
      "2787/2787 [==============================] - 1s 200us/step - loss: 0.4758 - acc: 0.9013 - val_loss: 0.4257 - val_acc: 0.9226\n",
      "Epoch 6/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.4727 - acc: 0.8995 - val_loss: 0.4275 - val_acc: 0.9194\n",
      "Epoch 7/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.4491 - acc: 0.9089 - val_loss: 0.4166 - val_acc: 0.9290\n",
      "Epoch 8/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.4427 - acc: 0.9024 - val_loss: 0.4029 - val_acc: 0.9323\n",
      "Epoch 9/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.4417 - acc: 0.9024 - val_loss: 0.4044 - val_acc: 0.9258\n",
      "Epoch 10/100\n",
      "2787/2787 [==============================] - 1s 201us/step - loss: 0.4297 - acc: 0.9042 - val_loss: 0.3971 - val_acc: 0.9258\n",
      "Epoch 11/100\n",
      "2787/2787 [==============================] - 1s 200us/step - loss: 0.4094 - acc: 0.9132 - val_loss: 0.3907 - val_acc: 0.9290\n",
      "Epoch 12/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3958 - acc: 0.9142 - val_loss: 0.3870 - val_acc: 0.9323\n",
      "Epoch 13/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.4023 - acc: 0.9168 - val_loss: 0.3852 - val_acc: 0.9194\n",
      "Epoch 14/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3982 - acc: 0.9150 - val_loss: 0.3892 - val_acc: 0.9194\n",
      "Epoch 15/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3881 - acc: 0.9157 - val_loss: 0.3846 - val_acc: 0.9290\n",
      "Epoch 16/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3900 - acc: 0.9146 - val_loss: 0.3805 - val_acc: 0.9194\n",
      "Epoch 17/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3917 - acc: 0.9157 - val_loss: 0.3798 - val_acc: 0.9258\n",
      "Epoch 18/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3787 - acc: 0.9186 - val_loss: 0.3732 - val_acc: 0.9097\n",
      "Epoch 19/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3894 - acc: 0.9139 - val_loss: 0.3650 - val_acc: 0.9258\n",
      "Epoch 20/100\n",
      "2787/2787 [==============================] - 1s 197us/step - loss: 0.3787 - acc: 0.9125 - val_loss: 0.3705 - val_acc: 0.9161\n",
      "Epoch 21/100\n",
      "2787/2787 [==============================] - 1s 197us/step - loss: 0.3747 - acc: 0.9182 - val_loss: 0.3730 - val_acc: 0.9129\n",
      "Epoch 22/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3678 - acc: 0.9175 - val_loss: 0.3665 - val_acc: 0.9226\n",
      "Epoch 23/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3688 - acc: 0.9128 - val_loss: 0.3679 - val_acc: 0.9065\n",
      "Epoch 24/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3663 - acc: 0.9171 - val_loss: 0.3654 - val_acc: 0.9097\n",
      "Epoch 25/100\n",
      "2787/2787 [==============================] - 1s 197us/step - loss: 0.3600 - acc: 0.9153 - val_loss: 0.3622 - val_acc: 0.9097\n",
      "Epoch 26/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3554 - acc: 0.9236 - val_loss: 0.3605 - val_acc: 0.9161\n",
      "Epoch 27/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3486 - acc: 0.9207 - val_loss: 0.3586 - val_acc: 0.9097\n",
      "Epoch 28/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3408 - acc: 0.9229 - val_loss: 0.3607 - val_acc: 0.9065\n",
      "Epoch 29/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3486 - acc: 0.9142 - val_loss: 0.3598 - val_acc: 0.9129\n",
      "Epoch 30/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3451 - acc: 0.9193 - val_loss: 0.3544 - val_acc: 0.9000\n",
      "Epoch 31/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3458 - acc: 0.9189 - val_loss: 0.3548 - val_acc: 0.9129\n",
      "Epoch 32/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3301 - acc: 0.9200 - val_loss: 0.3527 - val_acc: 0.9065\n",
      "Epoch 33/100\n",
      "2787/2787 [==============================] - 1s 197us/step - loss: 0.3430 - acc: 0.9139 - val_loss: 0.3458 - val_acc: 0.9161\n",
      "Epoch 34/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3311 - acc: 0.9236 - val_loss: 0.3441 - val_acc: 0.9129\n",
      "Epoch 35/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3289 - acc: 0.9257 - val_loss: 0.3406 - val_acc: 0.9032\n",
      "Epoch 36/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3120 - acc: 0.9333 - val_loss: 0.3460 - val_acc: 0.9065\n",
      "Epoch 37/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3209 - acc: 0.9211 - val_loss: 0.3345 - val_acc: 0.9161\n",
      "Epoch 38/100\n",
      "2787/2787 [==============================] - 1s 206us/step - loss: 0.3262 - acc: 0.9236 - val_loss: 0.3281 - val_acc: 0.9194\n",
      "Epoch 39/100\n",
      "2787/2787 [==============================] - 1s 200us/step - loss: 0.3197 - acc: 0.9175 - val_loss: 0.3325 - val_acc: 0.9065\n",
      "Epoch 40/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3088 - acc: 0.9243 - val_loss: 0.3275 - val_acc: 0.9161\n",
      "Epoch 41/100\n",
      "2787/2787 [==============================] - 1s 197us/step - loss: 0.2950 - acc: 0.9272 - val_loss: 0.3276 - val_acc: 0.9161\n",
      "Epoch 42/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3009 - acc: 0.9250 - val_loss: 0.3246 - val_acc: 0.9065\n",
      "Epoch 43/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3122 - acc: 0.9229 - val_loss: 0.3245 - val_acc: 0.9097\n",
      "Epoch 44/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3000 - acc: 0.9264 - val_loss: 0.3250 - val_acc: 0.9129\n",
      "Epoch 45/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2957 - acc: 0.9300 - val_loss: 0.3278 - val_acc: 0.9161\n",
      "Epoch 46/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2966 - acc: 0.9297 - val_loss: 0.3250 - val_acc: 0.9129\n",
      "Epoch 47/100\n",
      "2787/2787 [==============================] - 1s 197us/step - loss: 0.2905 - acc: 0.9286 - val_loss: 0.3295 - val_acc: 0.9097\n",
      "Epoch 48/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3090 - acc: 0.9247 - val_loss: 0.3235 - val_acc: 0.9032\n",
      "Epoch 49/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3017 - acc: 0.9193 - val_loss: 0.3238 - val_acc: 0.9032\n",
      "Epoch 50/100\n",
      "2787/2787 [==============================] - 1s 200us/step - loss: 0.3019 - acc: 0.9264 - val_loss: 0.3262 - val_acc: 0.9032\n",
      "Epoch 51/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2920 - acc: 0.9268 - val_loss: 0.3247 - val_acc: 0.9000\n",
      "Epoch 52/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2996 - acc: 0.9286 - val_loss: 0.3254 - val_acc: 0.9032\n",
      "Epoch 53/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2949 - acc: 0.9243 - val_loss: 0.3338 - val_acc: 0.9032\n",
      "Epoch 54/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2795 - acc: 0.9354 - val_loss: 0.3314 - val_acc: 0.9065\n",
      "Epoch 55/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2769 - acc: 0.9343 - val_loss: 0.3278 - val_acc: 0.9097\n",
      "Epoch 56/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2761 - acc: 0.9365 - val_loss: 0.3254 - val_acc: 0.9129\n",
      "Epoch 57/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2853 - acc: 0.9304 - val_loss: 0.3239 - val_acc: 0.9129\n",
      "Epoch 58/100\n",
      "2787/2787 [==============================] - 1s 200us/step - loss: 0.2931 - acc: 0.9272 - val_loss: 0.3232 - val_acc: 0.9161\n",
      "Epoch 59/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2796 - acc: 0.9286 - val_loss: 0.3273 - val_acc: 0.9065\n",
      "Epoch 60/100\n",
      "2787/2787 [==============================] - 1s 200us/step - loss: 0.2849 - acc: 0.9300 - val_loss: 0.3234 - val_acc: 0.9000\n",
      "Epoch 61/100\n",
      "2787/2787 [==============================] - 1s 201us/step - loss: 0.2969 - acc: 0.9297 - val_loss: 0.3217 - val_acc: 0.9000\n",
      "Epoch 62/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2590 - acc: 0.9473 - val_loss: 0.3213 - val_acc: 0.9097\n",
      "Epoch 63/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2690 - acc: 0.9315 - val_loss: 0.3212 - val_acc: 0.9065\n",
      "Epoch 64/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2983 - acc: 0.9189 - val_loss: 0.3215 - val_acc: 0.9097\n",
      "Epoch 65/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2760 - acc: 0.9336 - val_loss: 0.3232 - val_acc: 0.9194\n",
      "Epoch 66/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2651 - acc: 0.9372 - val_loss: 0.3229 - val_acc: 0.9226\n",
      "Epoch 67/100\n",
      "2787/2787 [==============================] - 1s 205us/step - loss: 0.2779 - acc: 0.9376 - val_loss: 0.3216 - val_acc: 0.9194\n",
      "Epoch 68/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2839 - acc: 0.9297 - val_loss: 0.3200 - val_acc: 0.9226\n",
      "Epoch 69/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2795 - acc: 0.9318 - val_loss: 0.3209 - val_acc: 0.9226\n",
      "Epoch 70/100\n",
      "2787/2787 [==============================] - 1s 203us/step - loss: 0.2806 - acc: 0.9297 - val_loss: 0.3222 - val_acc: 0.9226\n",
      "Epoch 71/100\n",
      "2787/2787 [==============================] - 1s 207us/step - loss: 0.2594 - acc: 0.9379 - val_loss: 0.3209 - val_acc: 0.9226\n",
      "Epoch 72/100\n",
      "2787/2787 [==============================] - 1s 207us/step - loss: 0.2720 - acc: 0.9333 - val_loss: 0.3208 - val_acc: 0.9194\n",
      "Epoch 73/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2787/2787 [==============================] - 1s 203us/step - loss: 0.2720 - acc: 0.9336 - val_loss: 0.3205 - val_acc: 0.9161\n",
      "Epoch 74/100\n",
      "2787/2787 [==============================] - 1s 202us/step - loss: 0.2812 - acc: 0.9275 - val_loss: 0.3192 - val_acc: 0.9161\n",
      "Epoch 75/100\n",
      "2787/2787 [==============================] - 1s 203us/step - loss: 0.2739 - acc: 0.9343 - val_loss: 0.3190 - val_acc: 0.9129\n",
      "Epoch 76/100\n",
      "2787/2787 [==============================] - 1s 206us/step - loss: 0.2741 - acc: 0.9307 - val_loss: 0.3186 - val_acc: 0.9161\n",
      "Epoch 77/100\n",
      "2787/2787 [==============================] - 1s 202us/step - loss: 0.2728 - acc: 0.9293 - val_loss: 0.3188 - val_acc: 0.9194\n",
      "Epoch 78/100\n",
      "2787/2787 [==============================] - 1s 204us/step - loss: 0.2808 - acc: 0.9275 - val_loss: 0.3203 - val_acc: 0.9194\n",
      "Epoch 79/100\n",
      "2787/2787 [==============================] - 1s 202us/step - loss: 0.2660 - acc: 0.9336 - val_loss: 0.3208 - val_acc: 0.9194\n",
      "Epoch 80/100\n",
      "2787/2787 [==============================] - 1s 203us/step - loss: 0.2557 - acc: 0.9383 - val_loss: 0.3204 - val_acc: 0.9161\n",
      "Epoch 81/100\n",
      "2787/2787 [==============================] - 1s 201us/step - loss: 0.2740 - acc: 0.9300 - val_loss: 0.3204 - val_acc: 0.9161\n",
      "Epoch 82/100\n",
      "2787/2787 [==============================] - 1s 204us/step - loss: 0.2762 - acc: 0.9250 - val_loss: 0.3203 - val_acc: 0.9161\n",
      "Epoch 83/100\n",
      "2787/2787 [==============================] - 1s 206us/step - loss: 0.2612 - acc: 0.9408 - val_loss: 0.3203 - val_acc: 0.9129\n",
      "Epoch 84/100\n",
      "2787/2787 [==============================] - 1s 203us/step - loss: 0.2680 - acc: 0.9293 - val_loss: 0.3192 - val_acc: 0.9161\n",
      "Epoch 85/100\n",
      "2787/2787 [==============================] - 1s 202us/step - loss: 0.2662 - acc: 0.9354 - val_loss: 0.3177 - val_acc: 0.9161\n",
      "Epoch 86/100\n",
      "2787/2787 [==============================] - 1s 205us/step - loss: 0.2654 - acc: 0.9325 - val_loss: 0.3204 - val_acc: 0.9161\n",
      "Epoch 87/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2816 - acc: 0.9264 - val_loss: 0.3194 - val_acc: 0.9097\n",
      "Epoch 88/100\n",
      "2787/2787 [==============================] - 1s 206us/step - loss: 0.2785 - acc: 0.9225 - val_loss: 0.3171 - val_acc: 0.9129\n",
      "Epoch 89/100\n",
      "2787/2787 [==============================] - 1s 206us/step - loss: 0.2631 - acc: 0.9354 - val_loss: 0.3183 - val_acc: 0.9129\n",
      "Epoch 90/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2632 - acc: 0.9358 - val_loss: 0.3180 - val_acc: 0.9161\n",
      "Epoch 91/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2590 - acc: 0.9422 - val_loss: 0.3185 - val_acc: 0.9129\n",
      "Epoch 92/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2660 - acc: 0.9325 - val_loss: 0.3185 - val_acc: 0.9129\n",
      "Epoch 93/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2567 - acc: 0.9365 - val_loss: 0.3181 - val_acc: 0.9161\n",
      "Epoch 94/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2654 - acc: 0.9311 - val_loss: 0.3174 - val_acc: 0.9161\n",
      "Epoch 95/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2747 - acc: 0.9232 - val_loss: 0.3179 - val_acc: 0.9129\n",
      "Epoch 96/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2561 - acc: 0.9336 - val_loss: 0.3156 - val_acc: 0.9129\n",
      "Epoch 97/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2723 - acc: 0.9279 - val_loss: 0.3168 - val_acc: 0.9129\n",
      "Epoch 98/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2676 - acc: 0.9329 - val_loss: 0.3188 - val_acc: 0.9129\n",
      "Epoch 99/100\n",
      "2787/2787 [==============================] - 1s 197us/step - loss: 0.2702 - acc: 0.9325 - val_loss: 0.3169 - val_acc: 0.9097\n",
      "Epoch 100/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2553 - acc: 0.9376 - val_loss: 0.3178 - val_acc: 0.9097\n",
      "Model Train 6\n",
      "Train on 2787 samples, validate on 310 samples\n",
      "Epoch 1/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.7713 - acc: 0.7822 - val_loss: 0.4546 - val_acc: 0.9129\n",
      "Epoch 2/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.5179 - acc: 0.8992 - val_loss: 0.4493 - val_acc: 0.9161\n",
      "Epoch 3/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.4989 - acc: 0.8956 - val_loss: 0.4374 - val_acc: 0.9194\n",
      "Epoch 4/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.4827 - acc: 0.9031 - val_loss: 0.4302 - val_acc: 0.9258\n",
      "Epoch 5/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.4707 - acc: 0.9078 - val_loss: 0.4151 - val_acc: 0.9290\n",
      "Epoch 6/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.4620 - acc: 0.9017 - val_loss: 0.4020 - val_acc: 0.9226\n",
      "Epoch 7/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.4576 - acc: 0.9010 - val_loss: 0.3908 - val_acc: 0.9323\n",
      "Epoch 8/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.4363 - acc: 0.9110 - val_loss: 0.3946 - val_acc: 0.9258\n",
      "Epoch 9/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.4238 - acc: 0.9189 - val_loss: 0.3899 - val_acc: 0.9226\n",
      "Epoch 10/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.4162 - acc: 0.9135 - val_loss: 0.3763 - val_acc: 0.9355\n",
      "Epoch 11/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.4209 - acc: 0.9078 - val_loss: 0.3872 - val_acc: 0.9323\n",
      "Epoch 12/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.4057 - acc: 0.9132 - val_loss: 0.3807 - val_acc: 0.9194\n",
      "Epoch 13/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3881 - acc: 0.9239 - val_loss: 0.3713 - val_acc: 0.9161\n",
      "Epoch 14/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3884 - acc: 0.9193 - val_loss: 0.3670 - val_acc: 0.9194\n",
      "Epoch 15/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3877 - acc: 0.9150 - val_loss: 0.3641 - val_acc: 0.9194\n",
      "Epoch 16/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3765 - acc: 0.9261 - val_loss: 0.3653 - val_acc: 0.9161\n",
      "Epoch 17/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3764 - acc: 0.9221 - val_loss: 0.3615 - val_acc: 0.9194\n",
      "Epoch 18/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3725 - acc: 0.9254 - val_loss: 0.3619 - val_acc: 0.9194\n",
      "Epoch 19/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3923 - acc: 0.9182 - val_loss: 0.3591 - val_acc: 0.9129\n",
      "Epoch 20/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3740 - acc: 0.9175 - val_loss: 0.3580 - val_acc: 0.9129\n",
      "Epoch 21/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3795 - acc: 0.9203 - val_loss: 0.3559 - val_acc: 0.9161\n",
      "Epoch 22/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3778 - acc: 0.9135 - val_loss: 0.3536 - val_acc: 0.9194\n",
      "Epoch 23/100\n",
      "2787/2787 [==============================] - 1s 204us/step - loss: 0.3671 - acc: 0.9186 - val_loss: 0.3470 - val_acc: 0.9161\n",
      "Epoch 24/100\n",
      "2787/2787 [==============================] - 1s 200us/step - loss: 0.3599 - acc: 0.9268 - val_loss: 0.3479 - val_acc: 0.9226\n",
      "Epoch 25/100\n",
      "2787/2787 [==============================] - 1s 200us/step - loss: 0.3663 - acc: 0.9164 - val_loss: 0.3403 - val_acc: 0.9226\n",
      "Epoch 26/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3518 - acc: 0.9236 - val_loss: 0.3380 - val_acc: 0.9226\n",
      "Epoch 27/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3363 - acc: 0.9264 - val_loss: 0.3403 - val_acc: 0.9097\n",
      "Epoch 28/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3547 - acc: 0.9200 - val_loss: 0.3357 - val_acc: 0.9129\n",
      "Epoch 29/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3316 - acc: 0.9207 - val_loss: 0.3402 - val_acc: 0.9129\n",
      "Epoch 30/100\n",
      "2787/2787 [==============================] - 1s 200us/step - loss: 0.3511 - acc: 0.9225 - val_loss: 0.3321 - val_acc: 0.9161\n",
      "Epoch 31/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3332 - acc: 0.9340 - val_loss: 0.3318 - val_acc: 0.9226\n",
      "Epoch 32/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3356 - acc: 0.9214 - val_loss: 0.3262 - val_acc: 0.9258\n",
      "Epoch 33/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3416 - acc: 0.9225 - val_loss: 0.3262 - val_acc: 0.9258\n",
      "Epoch 34/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3300 - acc: 0.9171 - val_loss: 0.3205 - val_acc: 0.9226\n",
      "Epoch 35/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3270 - acc: 0.9247 - val_loss: 0.3146 - val_acc: 0.9226\n",
      "Epoch 36/100\n",
      "2787/2787 [==============================] - 1s 197us/step - loss: 0.3156 - acc: 0.9297 - val_loss: 0.3219 - val_acc: 0.9065\n",
      "Epoch 37/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3171 - acc: 0.9232 - val_loss: 0.3157 - val_acc: 0.9129\n",
      "Epoch 38/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3134 - acc: 0.9282 - val_loss: 0.3101 - val_acc: 0.9323\n",
      "Epoch 39/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3073 - acc: 0.9272 - val_loss: 0.3185 - val_acc: 0.9194\n",
      "Epoch 40/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3019 - acc: 0.9300 - val_loss: 0.3133 - val_acc: 0.9226\n",
      "Epoch 41/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3002 - acc: 0.9318 - val_loss: 0.3118 - val_acc: 0.9194\n",
      "Epoch 42/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3043 - acc: 0.9275 - val_loss: 0.3116 - val_acc: 0.9226\n",
      "Epoch 43/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2993 - acc: 0.9329 - val_loss: 0.3084 - val_acc: 0.9258\n",
      "Epoch 44/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3035 - acc: 0.9329 - val_loss: 0.3095 - val_acc: 0.9226\n",
      "Epoch 45/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3025 - acc: 0.9261 - val_loss: 0.3115 - val_acc: 0.9226\n",
      "Epoch 46/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3011 - acc: 0.9275 - val_loss: 0.3094 - val_acc: 0.9129\n",
      "Epoch 47/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2948 - acc: 0.9347 - val_loss: 0.3096 - val_acc: 0.9161\n",
      "Epoch 48/100\n",
      "2787/2787 [==============================] - 1s 197us/step - loss: 0.2784 - acc: 0.9379 - val_loss: 0.3085 - val_acc: 0.9226\n",
      "Epoch 49/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2969 - acc: 0.9279 - val_loss: 0.3059 - val_acc: 0.9129\n",
      "Epoch 50/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2919 - acc: 0.9261 - val_loss: 0.3035 - val_acc: 0.9226\n",
      "Epoch 51/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2966 - acc: 0.9275 - val_loss: 0.3041 - val_acc: 0.9226\n",
      "Epoch 52/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2834 - acc: 0.9354 - val_loss: 0.3057 - val_acc: 0.9226\n",
      "Epoch 53/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2843 - acc: 0.9351 - val_loss: 0.3054 - val_acc: 0.9161\n",
      "Epoch 54/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3048 - acc: 0.9239 - val_loss: 0.3038 - val_acc: 0.9129\n",
      "Epoch 55/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2998 - acc: 0.9232 - val_loss: 0.3046 - val_acc: 0.9129\n",
      "Epoch 56/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2757 - acc: 0.9358 - val_loss: 0.3019 - val_acc: 0.9226\n",
      "Epoch 57/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2900 - acc: 0.9315 - val_loss: 0.3036 - val_acc: 0.9194\n",
      "Epoch 58/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2888 - acc: 0.9293 - val_loss: 0.3060 - val_acc: 0.9129\n",
      "Epoch 59/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2916 - acc: 0.9325 - val_loss: 0.3052 - val_acc: 0.9129\n",
      "Epoch 60/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2983 - acc: 0.9250 - val_loss: 0.2998 - val_acc: 0.9194\n",
      "Epoch 61/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2835 - acc: 0.9318 - val_loss: 0.3019 - val_acc: 0.9194\n",
      "Epoch 62/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2840 - acc: 0.9293 - val_loss: 0.3001 - val_acc: 0.9161\n",
      "Epoch 63/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2963 - acc: 0.9214 - val_loss: 0.3051 - val_acc: 0.9097\n",
      "Epoch 64/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2743 - acc: 0.9383 - val_loss: 0.2989 - val_acc: 0.9194\n",
      "Epoch 65/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2736 - acc: 0.9343 - val_loss: 0.2998 - val_acc: 0.9129\n",
      "Epoch 66/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2812 - acc: 0.9307 - val_loss: 0.3016 - val_acc: 0.9129\n",
      "Epoch 67/100\n",
      "2787/2787 [==============================] - 1s 209us/step - loss: 0.2770 - acc: 0.9340 - val_loss: 0.2952 - val_acc: 0.9226\n",
      "Epoch 68/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2625 - acc: 0.9408 - val_loss: 0.2977 - val_acc: 0.9194\n",
      "Epoch 69/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2785 - acc: 0.9329 - val_loss: 0.2994 - val_acc: 0.9129\n",
      "Epoch 70/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2587 - acc: 0.9372 - val_loss: 0.2986 - val_acc: 0.9129\n",
      "Epoch 71/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2713 - acc: 0.9361 - val_loss: 0.2962 - val_acc: 0.9129\n",
      "Epoch 72/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2642 - acc: 0.9343 - val_loss: 0.2966 - val_acc: 0.9129\n",
      "Epoch 73/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2565 - acc: 0.9412 - val_loss: 0.2957 - val_acc: 0.9161\n",
      "Epoch 74/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2632 - acc: 0.9343 - val_loss: 0.2946 - val_acc: 0.9129\n",
      "Epoch 75/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2690 - acc: 0.9361 - val_loss: 0.2933 - val_acc: 0.9129\n",
      "Epoch 76/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2659 - acc: 0.9372 - val_loss: 0.2939 - val_acc: 0.9129\n",
      "Epoch 77/100\n",
      "2787/2787 [==============================] - 1s 200us/step - loss: 0.2605 - acc: 0.9401 - val_loss: 0.2942 - val_acc: 0.9097\n",
      "Epoch 78/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2629 - acc: 0.9419 - val_loss: 0.2964 - val_acc: 0.9097\n",
      "Epoch 79/100\n",
      "2787/2787 [==============================] - 1s 206us/step - loss: 0.2478 - acc: 0.9390 - val_loss: 0.2965 - val_acc: 0.9097\n",
      "Epoch 80/100\n",
      "2787/2787 [==============================] - 1s 201us/step - loss: 0.2571 - acc: 0.9429 - val_loss: 0.2969 - val_acc: 0.9129\n",
      "Epoch 81/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2594 - acc: 0.9404 - val_loss: 0.2969 - val_acc: 0.9161\n",
      "Epoch 82/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2606 - acc: 0.9307 - val_loss: 0.2967 - val_acc: 0.9129\n",
      "Epoch 83/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2734 - acc: 0.9329 - val_loss: 0.2914 - val_acc: 0.9161\n",
      "Epoch 84/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2762 - acc: 0.9318 - val_loss: 0.2921 - val_acc: 0.9129\n",
      "Epoch 85/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2776 - acc: 0.9229 - val_loss: 0.2930 - val_acc: 0.9129\n",
      "Epoch 86/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2534 - acc: 0.9386 - val_loss: 0.2918 - val_acc: 0.9097\n",
      "Epoch 87/100\n",
      "2787/2787 [==============================] - 1s 202us/step - loss: 0.2527 - acc: 0.9426 - val_loss: 0.2912 - val_acc: 0.9161\n",
      "Epoch 88/100\n",
      "2787/2787 [==============================] - 1s 200us/step - loss: 0.2558 - acc: 0.9368 - val_loss: 0.2906 - val_acc: 0.9129\n",
      "Epoch 89/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2476 - acc: 0.9412 - val_loss: 0.2907 - val_acc: 0.9226\n",
      "Epoch 90/100\n",
      "2787/2787 [==============================] - 1s 200us/step - loss: 0.2626 - acc: 0.9329 - val_loss: 0.2927 - val_acc: 0.9129\n",
      "Epoch 91/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2512 - acc: 0.9390 - val_loss: 0.2943 - val_acc: 0.9129\n",
      "Epoch 92/100\n",
      "2787/2787 [==============================] - 1s 197us/step - loss: 0.2445 - acc: 0.9433 - val_loss: 0.2949 - val_acc: 0.9194\n",
      "Epoch 93/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2457 - acc: 0.9412 - val_loss: 0.2935 - val_acc: 0.9194\n",
      "Epoch 94/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2531 - acc: 0.9390 - val_loss: 0.2962 - val_acc: 0.9097\n",
      "Epoch 95/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2495 - acc: 0.9383 - val_loss: 0.2940 - val_acc: 0.9161\n",
      "Epoch 96/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2540 - acc: 0.9426 - val_loss: 0.2945 - val_acc: 0.9161\n",
      "Epoch 97/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2525 - acc: 0.9390 - val_loss: 0.2935 - val_acc: 0.9194\n",
      "Epoch 98/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2596 - acc: 0.9368 - val_loss: 0.2921 - val_acc: 0.9129\n",
      "Epoch 99/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2542 - acc: 0.9412 - val_loss: 0.2901 - val_acc: 0.9097\n",
      "Epoch 100/100\n",
      "2787/2787 [==============================] - 1s 197us/step - loss: 0.2498 - acc: 0.9390 - val_loss: 0.2950 - val_acc: 0.9032\n",
      "Model Train 7\n",
      "Train on 2788 samples, validate on 309 samples\n",
      "Epoch 1/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.7727 - acc: 0.7719 - val_loss: 0.5928 - val_acc: 0.8738\n",
      "Epoch 2/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.5319 - acc: 0.8913 - val_loss: 0.5453 - val_acc: 0.8997\n",
      "Epoch 3/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.5173 - acc: 0.8945 - val_loss: 0.5156 - val_acc: 0.9029\n",
      "Epoch 4/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.4955 - acc: 0.9003 - val_loss: 0.5185 - val_acc: 0.8932\n",
      "Epoch 5/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.4828 - acc: 0.9003 - val_loss: 0.5133 - val_acc: 0.8932\n",
      "Epoch 6/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.4664 - acc: 0.9060 - val_loss: 0.5037 - val_acc: 0.8900\n",
      "Epoch 7/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.4438 - acc: 0.9121 - val_loss: 0.4955 - val_acc: 0.8932\n",
      "Epoch 8/100\n",
      "2788/2788 [==============================] - 1s 203us/step - loss: 0.4430 - acc: 0.9132 - val_loss: 0.4915 - val_acc: 0.8900\n",
      "Epoch 9/100\n",
      "2788/2788 [==============================] - 1s 197us/step - loss: 0.4328 - acc: 0.9107 - val_loss: 0.4893 - val_acc: 0.8964\n",
      "Epoch 10/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.4350 - acc: 0.9093 - val_loss: 0.4816 - val_acc: 0.8964\n",
      "Epoch 11/100\n",
      "2788/2788 [==============================] - 1s 202us/step - loss: 0.4159 - acc: 0.9136 - val_loss: 0.4780 - val_acc: 0.8900\n",
      "Epoch 12/100\n",
      "2788/2788 [==============================] - 1s 197us/step - loss: 0.4045 - acc: 0.9189 - val_loss: 0.4783 - val_acc: 0.8964\n",
      "Epoch 13/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.4049 - acc: 0.9171 - val_loss: 0.4736 - val_acc: 0.8964\n",
      "Epoch 14/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.4000 - acc: 0.9150 - val_loss: 0.4713 - val_acc: 0.8932\n",
      "Epoch 15/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.3960 - acc: 0.9175 - val_loss: 0.4623 - val_acc: 0.8997\n",
      "Epoch 16/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.3836 - acc: 0.9261 - val_loss: 0.4615 - val_acc: 0.8932\n",
      "Epoch 17/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.3972 - acc: 0.9247 - val_loss: 0.4535 - val_acc: 0.8932\n",
      "Epoch 18/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.3910 - acc: 0.9211 - val_loss: 0.4566 - val_acc: 0.8932\n",
      "Epoch 19/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.3661 - acc: 0.9297 - val_loss: 0.4557 - val_acc: 0.8900\n",
      "Epoch 20/100\n",
      "2788/2788 [==============================] - 1s 197us/step - loss: 0.3827 - acc: 0.9182 - val_loss: 0.4530 - val_acc: 0.8835\n",
      "Epoch 21/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.3823 - acc: 0.9189 - val_loss: 0.4510 - val_acc: 0.8867\n",
      "Epoch 22/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.3627 - acc: 0.9232 - val_loss: 0.4499 - val_acc: 0.8932\n",
      "Epoch 23/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.3520 - acc: 0.9261 - val_loss: 0.4477 - val_acc: 0.8964\n",
      "Epoch 24/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.3537 - acc: 0.9268 - val_loss: 0.4439 - val_acc: 0.8867\n",
      "Epoch 25/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.3508 - acc: 0.9254 - val_loss: 0.4456 - val_acc: 0.8932\n",
      "Epoch 26/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.3516 - acc: 0.9243 - val_loss: 0.4438 - val_acc: 0.8867\n",
      "Epoch 27/100\n",
      "2788/2788 [==============================] - 1s 197us/step - loss: 0.3450 - acc: 0.9286 - val_loss: 0.4450 - val_acc: 0.8932\n",
      "Epoch 28/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.3335 - acc: 0.9304 - val_loss: 0.4425 - val_acc: 0.8867\n",
      "Epoch 29/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.3482 - acc: 0.9236 - val_loss: 0.4397 - val_acc: 0.8900\n",
      "Epoch 30/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.3239 - acc: 0.9369 - val_loss: 0.4393 - val_acc: 0.8867\n",
      "Epoch 31/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.3394 - acc: 0.9232 - val_loss: 0.4386 - val_acc: 0.8964\n",
      "Epoch 32/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.3205 - acc: 0.9336 - val_loss: 0.4399 - val_acc: 0.8964\n",
      "Epoch 33/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.3344 - acc: 0.9225 - val_loss: 0.4297 - val_acc: 0.8964\n",
      "Epoch 34/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.3229 - acc: 0.9297 - val_loss: 0.4280 - val_acc: 0.8964\n",
      "Epoch 35/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.3118 - acc: 0.9315 - val_loss: 0.4359 - val_acc: 0.8835\n",
      "Epoch 36/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.3338 - acc: 0.9279 - val_loss: 0.4297 - val_acc: 0.8867\n",
      "Epoch 37/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.3179 - acc: 0.9265 - val_loss: 0.4210 - val_acc: 0.8932\n",
      "Epoch 38/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.3114 - acc: 0.9275 - val_loss: 0.4174 - val_acc: 0.8900\n",
      "Epoch 39/100\n",
      "2788/2788 [==============================] - 1s 201us/step - loss: 0.3182 - acc: 0.9275 - val_loss: 0.4122 - val_acc: 0.9029\n",
      "Epoch 40/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.3110 - acc: 0.9258 - val_loss: 0.4140 - val_acc: 0.9029\n",
      "Epoch 41/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.3187 - acc: 0.9243 - val_loss: 0.4095 - val_acc: 0.8997\n",
      "Epoch 42/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2993 - acc: 0.9336 - val_loss: 0.4069 - val_acc: 0.9029\n",
      "Epoch 43/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2850 - acc: 0.9397 - val_loss: 0.4116 - val_acc: 0.8997\n",
      "Epoch 44/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2936 - acc: 0.9347 - val_loss: 0.4129 - val_acc: 0.8964\n",
      "Epoch 45/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2836 - acc: 0.9379 - val_loss: 0.4099 - val_acc: 0.8932\n",
      "Epoch 46/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2988 - acc: 0.9340 - val_loss: 0.4068 - val_acc: 0.9029\n",
      "Epoch 47/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2890 - acc: 0.9362 - val_loss: 0.4090 - val_acc: 0.8997\n",
      "Epoch 48/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2903 - acc: 0.9322 - val_loss: 0.4127 - val_acc: 0.8997\n",
      "Epoch 49/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2768 - acc: 0.9383 - val_loss: 0.4109 - val_acc: 0.8997\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2833 - acc: 0.9326 - val_loss: 0.4124 - val_acc: 0.8997\n",
      "Epoch 51/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2923 - acc: 0.9333 - val_loss: 0.4075 - val_acc: 0.9029\n",
      "Epoch 52/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2729 - acc: 0.9401 - val_loss: 0.4141 - val_acc: 0.8964\n",
      "Epoch 53/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2774 - acc: 0.9376 - val_loss: 0.4138 - val_acc: 0.8964\n",
      "Epoch 54/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2767 - acc: 0.9369 - val_loss: 0.4201 - val_acc: 0.8964\n",
      "Epoch 55/100\n",
      "2788/2788 [==============================] - 1s 197us/step - loss: 0.2827 - acc: 0.9319 - val_loss: 0.4188 - val_acc: 0.8997\n",
      "Epoch 56/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2802 - acc: 0.9333 - val_loss: 0.4160 - val_acc: 0.8997\n",
      "Epoch 57/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2829 - acc: 0.9347 - val_loss: 0.4197 - val_acc: 0.8997\n",
      "Epoch 58/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2886 - acc: 0.9322 - val_loss: 0.4137 - val_acc: 0.8964\n",
      "Epoch 59/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2735 - acc: 0.9412 - val_loss: 0.4114 - val_acc: 0.8932\n",
      "Epoch 60/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2719 - acc: 0.9405 - val_loss: 0.4111 - val_acc: 0.8964\n",
      "Epoch 61/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2686 - acc: 0.9379 - val_loss: 0.4190 - val_acc: 0.9029\n",
      "Epoch 62/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2786 - acc: 0.9387 - val_loss: 0.4154 - val_acc: 0.8997\n",
      "Epoch 63/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2721 - acc: 0.9369 - val_loss: 0.4118 - val_acc: 0.8997\n",
      "Epoch 64/100\n",
      "2788/2788 [==============================] - 1s 197us/step - loss: 0.2739 - acc: 0.9336 - val_loss: 0.4133 - val_acc: 0.8997\n",
      "Epoch 65/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2730 - acc: 0.9308 - val_loss: 0.4162 - val_acc: 0.9061\n",
      "Epoch 66/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2665 - acc: 0.9369 - val_loss: 0.4130 - val_acc: 0.9029\n",
      "Epoch 67/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2706 - acc: 0.9426 - val_loss: 0.4148 - val_acc: 0.9029\n",
      "Epoch 68/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2636 - acc: 0.9347 - val_loss: 0.4120 - val_acc: 0.9029\n",
      "Epoch 69/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2515 - acc: 0.9394 - val_loss: 0.4192 - val_acc: 0.9029\n",
      "Epoch 70/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2620 - acc: 0.9401 - val_loss: 0.4181 - val_acc: 0.9029\n",
      "Epoch 71/100\n",
      "2788/2788 [==============================] - 1s 197us/step - loss: 0.2513 - acc: 0.9405 - val_loss: 0.4197 - val_acc: 0.8997\n",
      "Epoch 72/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2603 - acc: 0.9387 - val_loss: 0.4182 - val_acc: 0.8997\n",
      "Epoch 73/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2483 - acc: 0.9430 - val_loss: 0.4199 - val_acc: 0.8997\n",
      "Epoch 74/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2512 - acc: 0.9415 - val_loss: 0.4203 - val_acc: 0.8997\n",
      "Epoch 75/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2607 - acc: 0.9369 - val_loss: 0.4154 - val_acc: 0.8964\n",
      "Epoch 76/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2544 - acc: 0.9344 - val_loss: 0.4153 - val_acc: 0.8964\n",
      "Epoch 77/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2401 - acc: 0.9498 - val_loss: 0.4170 - val_acc: 0.8932\n",
      "Epoch 78/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2558 - acc: 0.9405 - val_loss: 0.4152 - val_acc: 0.8964\n",
      "Epoch 79/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2544 - acc: 0.9451 - val_loss: 0.4138 - val_acc: 0.8964\n",
      "Epoch 80/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2551 - acc: 0.9455 - val_loss: 0.4146 - val_acc: 0.8964\n",
      "Epoch 81/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2477 - acc: 0.9440 - val_loss: 0.4175 - val_acc: 0.8932\n",
      "Epoch 82/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2492 - acc: 0.9383 - val_loss: 0.4185 - val_acc: 0.8997\n",
      "Epoch 83/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2601 - acc: 0.9326 - val_loss: 0.4175 - val_acc: 0.8997\n",
      "Epoch 84/100\n",
      "2788/2788 [==============================] - 1s 200us/step - loss: 0.2390 - acc: 0.9487 - val_loss: 0.4157 - val_acc: 0.8964\n",
      "Epoch 85/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2520 - acc: 0.9437 - val_loss: 0.4174 - val_acc: 0.8964\n",
      "Epoch 86/100\n",
      "2788/2788 [==============================] - 1s 200us/step - loss: 0.2418 - acc: 0.9469 - val_loss: 0.4156 - val_acc: 0.8964\n",
      "Epoch 87/100\n",
      "2788/2788 [==============================] - 1s 197us/step - loss: 0.2574 - acc: 0.9365 - val_loss: 0.4139 - val_acc: 0.8964\n",
      "Epoch 88/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2567 - acc: 0.9408 - val_loss: 0.4134 - val_acc: 0.8964\n",
      "Epoch 89/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2432 - acc: 0.9469 - val_loss: 0.4130 - val_acc: 0.8964\n",
      "Epoch 90/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2453 - acc: 0.9480 - val_loss: 0.4123 - val_acc: 0.8964\n",
      "Epoch 91/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2467 - acc: 0.9394 - val_loss: 0.4090 - val_acc: 0.8964\n",
      "Epoch 92/100\n",
      "2788/2788 [==============================] - 1s 204us/step - loss: 0.2474 - acc: 0.9469 - val_loss: 0.4081 - val_acc: 0.8964\n",
      "Epoch 93/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2581 - acc: 0.9369 - val_loss: 0.4098 - val_acc: 0.8997\n",
      "Epoch 94/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2508 - acc: 0.9433 - val_loss: 0.4085 - val_acc: 0.8964\n",
      "Epoch 95/100\n",
      "2788/2788 [==============================] - 1s 200us/step - loss: 0.2425 - acc: 0.9458 - val_loss: 0.4115 - val_acc: 0.8997\n",
      "Epoch 96/100\n",
      "2788/2788 [==============================] - 1s 197us/step - loss: 0.2412 - acc: 0.9458 - val_loss: 0.4109 - val_acc: 0.8964\n",
      "Epoch 97/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2529 - acc: 0.9405 - val_loss: 0.4124 - val_acc: 0.8932\n",
      "Epoch 98/100\n",
      "2788/2788 [==============================] - 1s 199us/step - loss: 0.2370 - acc: 0.9440 - val_loss: 0.4119 - val_acc: 0.8997\n",
      "Epoch 99/100\n",
      "2788/2788 [==============================] - 1s 198us/step - loss: 0.2557 - acc: 0.9344 - val_loss: 0.4150 - val_acc: 0.8932\n",
      "Epoch 100/100\n",
      "2788/2788 [==============================] - 1s 203us/step - loss: 0.2431 - acc: 0.9405 - val_loss: 0.4136 - val_acc: 0.8997\n",
      "Model Train 8\n",
      "Train on 2787 samples, validate on 310 samples\n",
      "Epoch 1/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.7618 - acc: 0.7801 - val_loss: 0.5023 - val_acc: 0.9129\n",
      "Epoch 2/100\n",
      "2787/2787 [==============================] - 1s 200us/step - loss: 0.5254 - acc: 0.8913 - val_loss: 0.5107 - val_acc: 0.9161\n",
      "Epoch 3/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.4951 - acc: 0.8977 - val_loss: 0.4903 - val_acc: 0.9226\n",
      "Epoch 4/100\n",
      "2787/2787 [==============================] - 1s 200us/step - loss: 0.4829 - acc: 0.9017 - val_loss: 0.4716 - val_acc: 0.9161\n",
      "Epoch 5/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.4690 - acc: 0.8988 - val_loss: 0.4649 - val_acc: 0.9129\n",
      "Epoch 6/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.4384 - acc: 0.9160 - val_loss: 0.4509 - val_acc: 0.9032\n",
      "Epoch 7/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.4510 - acc: 0.9046 - val_loss: 0.4410 - val_acc: 0.9065\n",
      "Epoch 8/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.4407 - acc: 0.9114 - val_loss: 0.4362 - val_acc: 0.9097\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.4314 - acc: 0.9110 - val_loss: 0.4324 - val_acc: 0.9097\n",
      "Epoch 10/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.4168 - acc: 0.9182 - val_loss: 0.4277 - val_acc: 0.9097\n",
      "Epoch 11/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.4097 - acc: 0.9153 - val_loss: 0.4214 - val_acc: 0.9161\n",
      "Epoch 12/100\n",
      "2787/2787 [==============================] - 1s 197us/step - loss: 0.4005 - acc: 0.9142 - val_loss: 0.4143 - val_acc: 0.9129\n",
      "Epoch 13/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3890 - acc: 0.9175 - val_loss: 0.4171 - val_acc: 0.9065\n",
      "Epoch 14/100\n",
      "2787/2787 [==============================] - 1s 197us/step - loss: 0.3877 - acc: 0.9168 - val_loss: 0.4136 - val_acc: 0.9129\n",
      "Epoch 15/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3902 - acc: 0.9146 - val_loss: 0.4115 - val_acc: 0.9097\n",
      "Epoch 16/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3887 - acc: 0.9132 - val_loss: 0.4066 - val_acc: 0.9161\n",
      "Epoch 17/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3721 - acc: 0.9264 - val_loss: 0.4117 - val_acc: 0.9097\n",
      "Epoch 18/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3774 - acc: 0.9175 - val_loss: 0.4023 - val_acc: 0.9194\n",
      "Epoch 19/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3708 - acc: 0.9225 - val_loss: 0.4037 - val_acc: 0.9226\n",
      "Epoch 20/100\n",
      "2787/2787 [==============================] - 1s 197us/step - loss: 0.3613 - acc: 0.9268 - val_loss: 0.4003 - val_acc: 0.9129\n",
      "Epoch 21/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3529 - acc: 0.9268 - val_loss: 0.3925 - val_acc: 0.9129\n",
      "Epoch 22/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3678 - acc: 0.9207 - val_loss: 0.3869 - val_acc: 0.9129\n",
      "Epoch 23/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3373 - acc: 0.9347 - val_loss: 0.3884 - val_acc: 0.9097\n",
      "Epoch 24/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3674 - acc: 0.9099 - val_loss: 0.3794 - val_acc: 0.9129\n",
      "Epoch 25/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3538 - acc: 0.9186 - val_loss: 0.3774 - val_acc: 0.9161\n",
      "Epoch 26/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3274 - acc: 0.9286 - val_loss: 0.3775 - val_acc: 0.9194\n",
      "Epoch 27/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3327 - acc: 0.9304 - val_loss: 0.3738 - val_acc: 0.9194\n",
      "Epoch 28/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3356 - acc: 0.9218 - val_loss: 0.3747 - val_acc: 0.9194\n",
      "Epoch 29/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3389 - acc: 0.9254 - val_loss: 0.3727 - val_acc: 0.9194\n",
      "Epoch 30/100\n",
      "2787/2787 [==============================] - 1s 197us/step - loss: 0.3361 - acc: 0.9221 - val_loss: 0.3725 - val_acc: 0.9161\n",
      "Epoch 31/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3327 - acc: 0.9275 - val_loss: 0.3776 - val_acc: 0.9194\n",
      "Epoch 32/100\n",
      "2787/2787 [==============================] - 1s 202us/step - loss: 0.3243 - acc: 0.9272 - val_loss: 0.3746 - val_acc: 0.9194\n",
      "Epoch 33/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3300 - acc: 0.9232 - val_loss: 0.3741 - val_acc: 0.9258\n",
      "Epoch 34/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3132 - acc: 0.9286 - val_loss: 0.3711 - val_acc: 0.9194\n",
      "Epoch 35/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3175 - acc: 0.9232 - val_loss: 0.3642 - val_acc: 0.9226\n",
      "Epoch 36/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3054 - acc: 0.9322 - val_loss: 0.3605 - val_acc: 0.9226\n",
      "Epoch 37/100\n",
      "2787/2787 [==============================] - 1s 197us/step - loss: 0.2936 - acc: 0.9336 - val_loss: 0.3591 - val_acc: 0.9258\n",
      "Epoch 38/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3067 - acc: 0.9250 - val_loss: 0.3547 - val_acc: 0.9258\n",
      "Epoch 39/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2963 - acc: 0.9325 - val_loss: 0.3587 - val_acc: 0.9226\n",
      "Epoch 40/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3038 - acc: 0.9300 - val_loss: 0.3551 - val_acc: 0.9194\n",
      "Epoch 41/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3143 - acc: 0.9189 - val_loss: 0.3554 - val_acc: 0.9129\n",
      "Epoch 42/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2937 - acc: 0.9333 - val_loss: 0.3576 - val_acc: 0.9194\n",
      "Epoch 43/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2745 - acc: 0.9390 - val_loss: 0.3601 - val_acc: 0.9161\n",
      "Epoch 44/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2942 - acc: 0.9243 - val_loss: 0.3589 - val_acc: 0.9194\n",
      "Epoch 45/100\n",
      "2787/2787 [==============================] - 1s 200us/step - loss: 0.2974 - acc: 0.9218 - val_loss: 0.3565 - val_acc: 0.9194\n",
      "Epoch 46/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2780 - acc: 0.9386 - val_loss: 0.3577 - val_acc: 0.9194\n",
      "Epoch 47/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3046 - acc: 0.9257 - val_loss: 0.3500 - val_acc: 0.9194\n",
      "Epoch 48/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2946 - acc: 0.9322 - val_loss: 0.3483 - val_acc: 0.9194\n",
      "Epoch 49/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2809 - acc: 0.9358 - val_loss: 0.3518 - val_acc: 0.9194\n",
      "Epoch 50/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2923 - acc: 0.9315 - val_loss: 0.3498 - val_acc: 0.9194\n",
      "Epoch 51/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2914 - acc: 0.9279 - val_loss: 0.3533 - val_acc: 0.9194\n",
      "Epoch 52/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2771 - acc: 0.9383 - val_loss: 0.3502 - val_acc: 0.9194\n",
      "Epoch 53/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2801 - acc: 0.9419 - val_loss: 0.3506 - val_acc: 0.9129\n",
      "Epoch 54/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2827 - acc: 0.9340 - val_loss: 0.3543 - val_acc: 0.9161\n",
      "Epoch 55/100\n",
      "2787/2787 [==============================] - 1s 197us/step - loss: 0.2764 - acc: 0.9386 - val_loss: 0.3572 - val_acc: 0.9161\n",
      "Epoch 56/100\n",
      "2787/2787 [==============================] - 1s 200us/step - loss: 0.2752 - acc: 0.9347 - val_loss: 0.3559 - val_acc: 0.9097\n",
      "Epoch 57/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2938 - acc: 0.9297 - val_loss: 0.3554 - val_acc: 0.9097\n",
      "Epoch 58/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2719 - acc: 0.9333 - val_loss: 0.3532 - val_acc: 0.9097\n",
      "Epoch 59/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2723 - acc: 0.9368 - val_loss: 0.3538 - val_acc: 0.9097\n",
      "Epoch 60/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2688 - acc: 0.9401 - val_loss: 0.3568 - val_acc: 0.9161\n",
      "Epoch 61/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2839 - acc: 0.9351 - val_loss: 0.3546 - val_acc: 0.9161\n",
      "Epoch 62/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2917 - acc: 0.9250 - val_loss: 0.3509 - val_acc: 0.9129\n",
      "Epoch 63/100\n",
      "2787/2787 [==============================] - 1s 200us/step - loss: 0.2855 - acc: 0.9282 - val_loss: 0.3480 - val_acc: 0.9129\n",
      "Epoch 64/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2792 - acc: 0.9343 - val_loss: 0.3474 - val_acc: 0.9161\n",
      "Epoch 65/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2589 - acc: 0.9394 - val_loss: 0.3504 - val_acc: 0.9129\n",
      "Epoch 66/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2607 - acc: 0.9422 - val_loss: 0.3502 - val_acc: 0.9097\n",
      "Epoch 67/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2708 - acc: 0.9315 - val_loss: 0.3517 - val_acc: 0.9129\n",
      "Epoch 68/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2787/2787 [==============================] - 1s 197us/step - loss: 0.2787 - acc: 0.9318 - val_loss: 0.3467 - val_acc: 0.9129\n",
      "Epoch 69/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2633 - acc: 0.9365 - val_loss: 0.3482 - val_acc: 0.9161\n",
      "Epoch 70/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2580 - acc: 0.9351 - val_loss: 0.3489 - val_acc: 0.9097\n",
      "Epoch 71/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2534 - acc: 0.9408 - val_loss: 0.3508 - val_acc: 0.9065\n",
      "Epoch 72/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2675 - acc: 0.9372 - val_loss: 0.3515 - val_acc: 0.9097\n",
      "Epoch 73/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2544 - acc: 0.9426 - val_loss: 0.3496 - val_acc: 0.9129\n",
      "Epoch 74/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2580 - acc: 0.9404 - val_loss: 0.3516 - val_acc: 0.9129\n",
      "Epoch 75/100\n",
      "2787/2787 [==============================] - 1s 197us/step - loss: 0.2622 - acc: 0.9347 - val_loss: 0.3501 - val_acc: 0.9129\n",
      "Epoch 76/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2587 - acc: 0.9386 - val_loss: 0.3514 - val_acc: 0.9161\n",
      "Epoch 77/100\n",
      "2787/2787 [==============================] - 1s 197us/step - loss: 0.2573 - acc: 0.9440 - val_loss: 0.3468 - val_acc: 0.9097\n",
      "Epoch 78/100\n",
      "2787/2787 [==============================] - 1s 197us/step - loss: 0.2573 - acc: 0.9415 - val_loss: 0.3468 - val_acc: 0.9097\n",
      "Epoch 79/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2704 - acc: 0.9386 - val_loss: 0.3464 - val_acc: 0.9097\n",
      "Epoch 80/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2591 - acc: 0.9311 - val_loss: 0.3486 - val_acc: 0.9129\n",
      "Epoch 81/100\n",
      "2787/2787 [==============================] - 1s 200us/step - loss: 0.2703 - acc: 0.9333 - val_loss: 0.3467 - val_acc: 0.9129\n",
      "Epoch 82/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2628 - acc: 0.9336 - val_loss: 0.3441 - val_acc: 0.9194\n",
      "Epoch 83/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2727 - acc: 0.9297 - val_loss: 0.3414 - val_acc: 0.9097\n",
      "Epoch 84/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2718 - acc: 0.9347 - val_loss: 0.3430 - val_acc: 0.9097\n",
      "Epoch 85/100\n",
      "2787/2787 [==============================] - 1s 197us/step - loss: 0.2712 - acc: 0.9329 - val_loss: 0.3434 - val_acc: 0.9129\n",
      "Epoch 86/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2544 - acc: 0.9372 - val_loss: 0.3458 - val_acc: 0.9129\n",
      "Epoch 87/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2584 - acc: 0.9419 - val_loss: 0.3473 - val_acc: 0.9161\n",
      "Epoch 88/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2490 - acc: 0.9426 - val_loss: 0.3477 - val_acc: 0.9129\n",
      "Epoch 89/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2640 - acc: 0.9372 - val_loss: 0.3528 - val_acc: 0.9097\n",
      "Epoch 90/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2473 - acc: 0.9412 - val_loss: 0.3496 - val_acc: 0.9097\n",
      "Epoch 91/100\n",
      "2787/2787 [==============================] - 1s 200us/step - loss: 0.2637 - acc: 0.9379 - val_loss: 0.3512 - val_acc: 0.9097\n",
      "Epoch 92/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2627 - acc: 0.9340 - val_loss: 0.3483 - val_acc: 0.9129\n",
      "Epoch 93/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2507 - acc: 0.9404 - val_loss: 0.3483 - val_acc: 0.9097\n",
      "Epoch 94/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2542 - acc: 0.9397 - val_loss: 0.3474 - val_acc: 0.9161\n",
      "Epoch 95/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2528 - acc: 0.9383 - val_loss: 0.3489 - val_acc: 0.9097\n",
      "Epoch 96/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2437 - acc: 0.9408 - val_loss: 0.3504 - val_acc: 0.9097\n",
      "Epoch 97/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2382 - acc: 0.9447 - val_loss: 0.3523 - val_acc: 0.9129\n",
      "Epoch 98/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2644 - acc: 0.9329 - val_loss: 0.3492 - val_acc: 0.9129\n",
      "Epoch 99/100\n",
      "2787/2787 [==============================] - 1s 200us/step - loss: 0.2574 - acc: 0.9386 - val_loss: 0.3482 - val_acc: 0.9129\n",
      "Epoch 100/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2419 - acc: 0.9429 - val_loss: 0.3501 - val_acc: 0.9097\n",
      "Model Train 9\n",
      "Train on 2787 samples, validate on 310 samples\n",
      "Epoch 1/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.7612 - acc: 0.7786 - val_loss: 0.4530 - val_acc: 0.9226\n",
      "Epoch 2/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.5588 - acc: 0.8715 - val_loss: 0.4156 - val_acc: 0.9323\n",
      "Epoch 3/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.5082 - acc: 0.8974 - val_loss: 0.4019 - val_acc: 0.9387\n",
      "Epoch 4/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.4879 - acc: 0.8949 - val_loss: 0.4008 - val_acc: 0.9355\n",
      "Epoch 5/100\n",
      "2787/2787 [==============================] - 1s 203us/step - loss: 0.4709 - acc: 0.9125 - val_loss: 0.3904 - val_acc: 0.9290\n",
      "Epoch 6/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.4649 - acc: 0.9042 - val_loss: 0.3823 - val_acc: 0.9258\n",
      "Epoch 7/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.4494 - acc: 0.9096 - val_loss: 0.3807 - val_acc: 0.9226\n",
      "Epoch 8/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.4363 - acc: 0.9164 - val_loss: 0.3683 - val_acc: 0.9290\n",
      "Epoch 9/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.4416 - acc: 0.9096 - val_loss: 0.3733 - val_acc: 0.9290\n",
      "Epoch 10/100\n",
      "2787/2787 [==============================] - 1s 200us/step - loss: 0.4122 - acc: 0.9139 - val_loss: 0.3626 - val_acc: 0.9323\n",
      "Epoch 11/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.4269 - acc: 0.9132 - val_loss: 0.3539 - val_acc: 0.9226\n",
      "Epoch 12/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.4158 - acc: 0.9135 - val_loss: 0.3506 - val_acc: 0.9323\n",
      "Epoch 13/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.4083 - acc: 0.9128 - val_loss: 0.3542 - val_acc: 0.9290\n",
      "Epoch 14/100\n",
      "2787/2787 [==============================] - 1s 200us/step - loss: 0.4065 - acc: 0.9175 - val_loss: 0.3482 - val_acc: 0.9226\n",
      "Epoch 15/100\n",
      "2787/2787 [==============================] - 1s 197us/step - loss: 0.3944 - acc: 0.9193 - val_loss: 0.3469 - val_acc: 0.9161\n",
      "Epoch 16/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.4102 - acc: 0.9060 - val_loss: 0.3475 - val_acc: 0.9161\n",
      "Epoch 17/100\n",
      "2787/2787 [==============================] - 1s 204us/step - loss: 0.3813 - acc: 0.9207 - val_loss: 0.3389 - val_acc: 0.9194\n",
      "Epoch 18/100\n",
      "2787/2787 [==============================] - 1s 200us/step - loss: 0.3763 - acc: 0.9229 - val_loss: 0.3377 - val_acc: 0.9161\n",
      "Epoch 19/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3757 - acc: 0.9247 - val_loss: 0.3304 - val_acc: 0.9161\n",
      "Epoch 20/100\n",
      "2787/2787 [==============================] - 1s 200us/step - loss: 0.3687 - acc: 0.9268 - val_loss: 0.3301 - val_acc: 0.9161\n",
      "Epoch 21/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3649 - acc: 0.9229 - val_loss: 0.3249 - val_acc: 0.9226\n",
      "Epoch 22/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3686 - acc: 0.9193 - val_loss: 0.3199 - val_acc: 0.9258\n",
      "Epoch 23/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3560 - acc: 0.9250 - val_loss: 0.3166 - val_acc: 0.9194\n",
      "Epoch 24/100\n",
      "2787/2787 [==============================] - 1s 202us/step - loss: 0.3456 - acc: 0.9286 - val_loss: 0.3180 - val_acc: 0.9194\n",
      "Epoch 25/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3589 - acc: 0.9218 - val_loss: 0.3096 - val_acc: 0.9290\n",
      "Epoch 26/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3562 - acc: 0.9232 - val_loss: 0.3114 - val_acc: 0.9097\n",
      "Epoch 27/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3516 - acc: 0.9236 - val_loss: 0.3132 - val_acc: 0.9129\n",
      "Epoch 28/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3604 - acc: 0.9186 - val_loss: 0.3084 - val_acc: 0.9194\n",
      "Epoch 29/100\n",
      "2787/2787 [==============================] - 1s 197us/step - loss: 0.3368 - acc: 0.9239 - val_loss: 0.3096 - val_acc: 0.9226\n",
      "Epoch 30/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3539 - acc: 0.9128 - val_loss: 0.3041 - val_acc: 0.9258\n",
      "Epoch 31/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3419 - acc: 0.9221 - val_loss: 0.2993 - val_acc: 0.9226\n",
      "Epoch 32/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3266 - acc: 0.9272 - val_loss: 0.2977 - val_acc: 0.9290\n",
      "Epoch 33/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3164 - acc: 0.9300 - val_loss: 0.2991 - val_acc: 0.9258\n",
      "Epoch 34/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3312 - acc: 0.9178 - val_loss: 0.2957 - val_acc: 0.9226\n",
      "Epoch 35/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3224 - acc: 0.9268 - val_loss: 0.2910 - val_acc: 0.9258\n",
      "Epoch 36/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3170 - acc: 0.9268 - val_loss: 0.2871 - val_acc: 0.9258\n",
      "Epoch 37/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3036 - acc: 0.9286 - val_loss: 0.2858 - val_acc: 0.9290\n",
      "Epoch 38/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3068 - acc: 0.9275 - val_loss: 0.2867 - val_acc: 0.9194\n",
      "Epoch 39/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3123 - acc: 0.9325 - val_loss: 0.2854 - val_acc: 0.9161\n",
      "Epoch 40/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3062 - acc: 0.9272 - val_loss: 0.2806 - val_acc: 0.9226\n",
      "Epoch 41/100\n",
      "2787/2787 [==============================] - 1s 201us/step - loss: 0.3042 - acc: 0.9286 - val_loss: 0.2875 - val_acc: 0.9226\n",
      "Epoch 42/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3025 - acc: 0.9272 - val_loss: 0.2821 - val_acc: 0.9226\n",
      "Epoch 43/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3027 - acc: 0.9293 - val_loss: 0.2765 - val_acc: 0.9290\n",
      "Epoch 44/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3030 - acc: 0.9311 - val_loss: 0.2750 - val_acc: 0.9258\n",
      "Epoch 45/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3017 - acc: 0.9250 - val_loss: 0.2756 - val_acc: 0.9226\n",
      "Epoch 46/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2892 - acc: 0.9322 - val_loss: 0.2755 - val_acc: 0.9226\n",
      "Epoch 47/100\n",
      "2787/2787 [==============================] - 1s 197us/step - loss: 0.2898 - acc: 0.9300 - val_loss: 0.2725 - val_acc: 0.9258\n",
      "Epoch 48/100\n",
      "2787/2787 [==============================] - 1s 197us/step - loss: 0.2986 - acc: 0.9272 - val_loss: 0.2727 - val_acc: 0.9258\n",
      "Epoch 49/100\n",
      "2787/2787 [==============================] - 1s 200us/step - loss: 0.2925 - acc: 0.9322 - val_loss: 0.2742 - val_acc: 0.9290\n",
      "Epoch 50/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3032 - acc: 0.9275 - val_loss: 0.2750 - val_acc: 0.9323\n",
      "Epoch 51/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2818 - acc: 0.9282 - val_loss: 0.2780 - val_acc: 0.9194\n",
      "Epoch 52/100\n",
      "2787/2787 [==============================] - 1s 197us/step - loss: 0.2908 - acc: 0.9322 - val_loss: 0.2713 - val_acc: 0.9258\n",
      "Epoch 53/100\n",
      "2787/2787 [==============================] - 1s 197us/step - loss: 0.2869 - acc: 0.9340 - val_loss: 0.2735 - val_acc: 0.9258\n",
      "Epoch 54/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2936 - acc: 0.9293 - val_loss: 0.2746 - val_acc: 0.9290\n",
      "Epoch 55/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3083 - acc: 0.9221 - val_loss: 0.2716 - val_acc: 0.9290\n",
      "Epoch 56/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3000 - acc: 0.9232 - val_loss: 0.2691 - val_acc: 0.9290\n",
      "Epoch 57/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2962 - acc: 0.9236 - val_loss: 0.2681 - val_acc: 0.9290\n",
      "Epoch 58/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2711 - acc: 0.9415 - val_loss: 0.2680 - val_acc: 0.9226\n",
      "Epoch 59/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2836 - acc: 0.9354 - val_loss: 0.2680 - val_acc: 0.9258\n",
      "Epoch 60/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3038 - acc: 0.9250 - val_loss: 0.2702 - val_acc: 0.9323\n",
      "Epoch 61/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2868 - acc: 0.9275 - val_loss: 0.2685 - val_acc: 0.9226\n",
      "Epoch 62/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2709 - acc: 0.9386 - val_loss: 0.2682 - val_acc: 0.9258\n",
      "Epoch 63/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2894 - acc: 0.9290 - val_loss: 0.2655 - val_acc: 0.9226\n",
      "Epoch 64/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2763 - acc: 0.9307 - val_loss: 0.2622 - val_acc: 0.9323\n",
      "Epoch 65/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2766 - acc: 0.9351 - val_loss: 0.2627 - val_acc: 0.9290\n",
      "Epoch 66/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2771 - acc: 0.9347 - val_loss: 0.2649 - val_acc: 0.9226\n",
      "Epoch 67/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2838 - acc: 0.9286 - val_loss: 0.2660 - val_acc: 0.9290\n",
      "Epoch 68/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2824 - acc: 0.9257 - val_loss: 0.2638 - val_acc: 0.9290\n",
      "Epoch 69/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2860 - acc: 0.9279 - val_loss: 0.2614 - val_acc: 0.9258\n",
      "Epoch 70/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2886 - acc: 0.9272 - val_loss: 0.2608 - val_acc: 0.9258\n",
      "Epoch 71/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2764 - acc: 0.9325 - val_loss: 0.2634 - val_acc: 0.9226\n",
      "Epoch 72/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2716 - acc: 0.9340 - val_loss: 0.2624 - val_acc: 0.9258\n",
      "Epoch 73/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2675 - acc: 0.9354 - val_loss: 0.2629 - val_acc: 0.9258\n",
      "Epoch 74/100\n",
      "2787/2787 [==============================] - 1s 200us/step - loss: 0.2766 - acc: 0.9358 - val_loss: 0.2616 - val_acc: 0.9323\n",
      "Epoch 75/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2876 - acc: 0.9279 - val_loss: 0.2620 - val_acc: 0.9323\n",
      "Epoch 76/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2619 - acc: 0.9408 - val_loss: 0.2638 - val_acc: 0.9290\n",
      "Epoch 77/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2741 - acc: 0.9390 - val_loss: 0.2630 - val_acc: 0.9290\n",
      "Epoch 78/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2658 - acc: 0.9379 - val_loss: 0.2621 - val_acc: 0.9323\n",
      "Epoch 79/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2666 - acc: 0.9354 - val_loss: 0.2600 - val_acc: 0.9323\n",
      "Epoch 80/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2731 - acc: 0.9333 - val_loss: 0.2586 - val_acc: 0.9290\n",
      "Epoch 81/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2667 - acc: 0.9365 - val_loss: 0.2580 - val_acc: 0.9323\n",
      "Epoch 82/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2697 - acc: 0.9322 - val_loss: 0.2585 - val_acc: 0.9323\n",
      "Epoch 83/100\n",
      "2787/2787 [==============================] - 1s 202us/step - loss: 0.2694 - acc: 0.9307 - val_loss: 0.2587 - val_acc: 0.9355\n",
      "Epoch 84/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2577 - acc: 0.9408 - val_loss: 0.2602 - val_acc: 0.9323\n",
      "Epoch 85/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2754 - acc: 0.9358 - val_loss: 0.2596 - val_acc: 0.9355\n",
      "Epoch 86/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2613 - acc: 0.9343 - val_loss: 0.2586 - val_acc: 0.9323\n",
      "Epoch 87/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2710 - acc: 0.9361 - val_loss: 0.2592 - val_acc: 0.9323\n",
      "Epoch 88/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2797 - acc: 0.9297 - val_loss: 0.2599 - val_acc: 0.9290\n",
      "Epoch 89/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2778 - acc: 0.9376 - val_loss: 0.2599 - val_acc: 0.9323\n",
      "Epoch 90/100\n",
      "2787/2787 [==============================] - 1s 197us/step - loss: 0.2686 - acc: 0.9365 - val_loss: 0.2608 - val_acc: 0.9290\n",
      "Epoch 91/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2769 - acc: 0.9318 - val_loss: 0.2593 - val_acc: 0.9290\n",
      "Epoch 92/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2551 - acc: 0.9401 - val_loss: 0.2598 - val_acc: 0.9323\n",
      "Epoch 93/100\n",
      "2787/2787 [==============================] - 1s 202us/step - loss: 0.2650 - acc: 0.9343 - val_loss: 0.2590 - val_acc: 0.9290\n",
      "Epoch 94/100\n",
      "2787/2787 [==============================] - 1s 200us/step - loss: 0.2686 - acc: 0.9397 - val_loss: 0.2591 - val_acc: 0.9290\n",
      "Epoch 95/100\n",
      "2787/2787 [==============================] - 1s 197us/step - loss: 0.2753 - acc: 0.9351 - val_loss: 0.2601 - val_acc: 0.9323\n",
      "Epoch 96/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2669 - acc: 0.9322 - val_loss: 0.2602 - val_acc: 0.9323\n",
      "Epoch 97/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2643 - acc: 0.9368 - val_loss: 0.2598 - val_acc: 0.9323\n",
      "Epoch 98/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2593 - acc: 0.9379 - val_loss: 0.2602 - val_acc: 0.9323\n",
      "Epoch 99/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2653 - acc: 0.9365 - val_loss: 0.2594 - val_acc: 0.9323\n",
      "Epoch 100/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2766 - acc: 0.9282 - val_loss: 0.2603 - val_acc: 0.9323\n",
      "Model Train 10\n",
      "Train on 2787 samples, validate on 310 samples\n",
      "Epoch 1/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.7443 - acc: 0.7872 - val_loss: 0.5310 - val_acc: 0.8903\n",
      "Epoch 2/100\n",
      "2787/2787 [==============================] - 1s 201us/step - loss: 0.5197 - acc: 0.8924 - val_loss: 0.4705 - val_acc: 0.9161\n",
      "Epoch 3/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.5094 - acc: 0.8909 - val_loss: 0.4583 - val_acc: 0.9161\n",
      "Epoch 4/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.4790 - acc: 0.9067 - val_loss: 0.4591 - val_acc: 0.9097\n",
      "Epoch 5/100\n",
      "2787/2787 [==============================] - 1s 200us/step - loss: 0.4800 - acc: 0.9042 - val_loss: 0.4315 - val_acc: 0.9161\n",
      "Epoch 6/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.4704 - acc: 0.9006 - val_loss: 0.4235 - val_acc: 0.9290\n",
      "Epoch 7/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.4526 - acc: 0.9078 - val_loss: 0.4200 - val_acc: 0.9161\n",
      "Epoch 8/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.4346 - acc: 0.9074 - val_loss: 0.4083 - val_acc: 0.9258\n",
      "Epoch 9/100\n",
      "2787/2787 [==============================] - 1s 197us/step - loss: 0.4297 - acc: 0.9121 - val_loss: 0.3995 - val_acc: 0.9258\n",
      "Epoch 10/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.4278 - acc: 0.9117 - val_loss: 0.3996 - val_acc: 0.9323\n",
      "Epoch 11/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.4057 - acc: 0.9186 - val_loss: 0.3863 - val_acc: 0.9355\n",
      "Epoch 12/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.4100 - acc: 0.9128 - val_loss: 0.3885 - val_acc: 0.9290\n",
      "Epoch 13/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.4038 - acc: 0.9142 - val_loss: 0.3906 - val_acc: 0.9161\n",
      "Epoch 14/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3959 - acc: 0.9193 - val_loss: 0.3925 - val_acc: 0.9258\n",
      "Epoch 15/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3953 - acc: 0.9207 - val_loss: 0.3924 - val_acc: 0.9258\n",
      "Epoch 16/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3883 - acc: 0.9164 - val_loss: 0.3867 - val_acc: 0.9258\n",
      "Epoch 17/100\n",
      "2787/2787 [==============================] - 1s 203us/step - loss: 0.4058 - acc: 0.9114 - val_loss: 0.3833 - val_acc: 0.9258\n",
      "Epoch 18/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3935 - acc: 0.9121 - val_loss: 0.3816 - val_acc: 0.9355\n",
      "Epoch 19/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3757 - acc: 0.9229 - val_loss: 0.3774 - val_acc: 0.9355\n",
      "Epoch 20/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3960 - acc: 0.9114 - val_loss: 0.3766 - val_acc: 0.9355\n",
      "Epoch 21/100\n",
      "2787/2787 [==============================] - 1s 201us/step - loss: 0.3670 - acc: 0.9229 - val_loss: 0.3765 - val_acc: 0.9258\n",
      "Epoch 22/100\n",
      "2787/2787 [==============================] - 1s 200us/step - loss: 0.3526 - acc: 0.9279 - val_loss: 0.3795 - val_acc: 0.9258\n",
      "Epoch 23/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3809 - acc: 0.9117 - val_loss: 0.3775 - val_acc: 0.9290\n",
      "Epoch 24/100\n",
      "2787/2787 [==============================] - 1s 201us/step - loss: 0.3637 - acc: 0.9229 - val_loss: 0.3758 - val_acc: 0.9258\n",
      "Epoch 25/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3696 - acc: 0.9193 - val_loss: 0.3718 - val_acc: 0.9258\n",
      "Epoch 26/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3619 - acc: 0.9232 - val_loss: 0.3730 - val_acc: 0.9258\n",
      "Epoch 27/100\n",
      "2787/2787 [==============================] - 1s 200us/step - loss: 0.3502 - acc: 0.9214 - val_loss: 0.3661 - val_acc: 0.9323\n",
      "Epoch 28/100\n",
      "2787/2787 [==============================] - 1s 200us/step - loss: 0.3608 - acc: 0.9200 - val_loss: 0.3662 - val_acc: 0.9258\n",
      "Epoch 29/100\n",
      "2787/2787 [==============================] - 1s 206us/step - loss: 0.3469 - acc: 0.9243 - val_loss: 0.3602 - val_acc: 0.9258\n",
      "Epoch 30/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3445 - acc: 0.9272 - val_loss: 0.3614 - val_acc: 0.9290\n",
      "Epoch 31/100\n",
      "2787/2787 [==============================] - 1s 197us/step - loss: 0.3451 - acc: 0.9236 - val_loss: 0.3604 - val_acc: 0.9258\n",
      "Epoch 32/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3404 - acc: 0.9182 - val_loss: 0.3551 - val_acc: 0.9258\n",
      "Epoch 33/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3255 - acc: 0.9343 - val_loss: 0.3536 - val_acc: 0.9323\n",
      "Epoch 34/100\n",
      "2787/2787 [==============================] - 1s 197us/step - loss: 0.3291 - acc: 0.9239 - val_loss: 0.3587 - val_acc: 0.9290\n",
      "Epoch 35/100\n",
      "2787/2787 [==============================] - 1s 200us/step - loss: 0.3291 - acc: 0.9261 - val_loss: 0.3501 - val_acc: 0.9258\n",
      "Epoch 36/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3377 - acc: 0.9214 - val_loss: 0.3436 - val_acc: 0.9290\n",
      "Epoch 37/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3293 - acc: 0.9196 - val_loss: 0.3439 - val_acc: 0.9258\n",
      "Epoch 38/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3328 - acc: 0.9186 - val_loss: 0.3424 - val_acc: 0.9161\n",
      "Epoch 39/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3345 - acc: 0.9178 - val_loss: 0.3353 - val_acc: 0.9194\n",
      "Epoch 40/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3225 - acc: 0.9207 - val_loss: 0.3386 - val_acc: 0.9226\n",
      "Epoch 41/100\n",
      "2787/2787 [==============================] - 1s 200us/step - loss: 0.3104 - acc: 0.9297 - val_loss: 0.3334 - val_acc: 0.9194\n",
      "Epoch 42/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3263 - acc: 0.9171 - val_loss: 0.3275 - val_acc: 0.9226\n",
      "Epoch 43/100\n",
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.2988 - acc: 0.9340 - val_loss: 0.3292 - val_acc: 0.9226\n",
      "Epoch 44/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.3062 - acc: 0.9293 - val_loss: 0.3294 - val_acc: 0.9226\n",
      "Epoch 45/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2787/2787 [==============================] - 1s 198us/step - loss: 0.3086 - acc: 0.9282 - val_loss: 0.3276 - val_acc: 0.9194\n",
      "Epoch 46/100\n",
      "2787/2787 [==============================] - 1s 205us/step - loss: 0.3085 - acc: 0.9257 - val_loss: 0.3289 - val_acc: 0.9226\n",
      "Epoch 47/100\n",
      "2787/2787 [==============================] - 1s 203us/step - loss: 0.2987 - acc: 0.9268 - val_loss: 0.3250 - val_acc: 0.9290\n",
      "Epoch 48/100\n",
      "2787/2787 [==============================] - 1s 206us/step - loss: 0.3058 - acc: 0.9318 - val_loss: 0.3296 - val_acc: 0.9258\n",
      "Epoch 49/100\n",
      "2787/2787 [==============================] - 1s 205us/step - loss: 0.3029 - acc: 0.9322 - val_loss: 0.3281 - val_acc: 0.9226\n",
      "Epoch 50/100\n",
      "2787/2787 [==============================] - 1s 201us/step - loss: 0.2888 - acc: 0.9368 - val_loss: 0.3263 - val_acc: 0.9258\n",
      "Epoch 51/100\n",
      "2787/2787 [==============================] - 1s 201us/step - loss: 0.2947 - acc: 0.9304 - val_loss: 0.3248 - val_acc: 0.9258\n",
      "Epoch 52/100\n",
      "2787/2787 [==============================] - 1s 207us/step - loss: 0.2926 - acc: 0.9333 - val_loss: 0.3268 - val_acc: 0.9258\n",
      "Epoch 53/100\n",
      "2787/2787 [==============================] - 1s 203us/step - loss: 0.3020 - acc: 0.9297 - val_loss: 0.3265 - val_acc: 0.9226\n",
      "Epoch 54/100\n",
      "2787/2787 [==============================] - 1s 232us/step - loss: 0.2834 - acc: 0.9368 - val_loss: 0.3277 - val_acc: 0.9226\n",
      "Epoch 55/100\n",
      "2787/2787 [==============================] - 1s 228us/step - loss: 0.3021 - acc: 0.9214 - val_loss: 0.3225 - val_acc: 0.9194\n",
      "Epoch 56/100\n",
      "2787/2787 [==============================] - 1s 236us/step - loss: 0.2869 - acc: 0.9333 - val_loss: 0.3280 - val_acc: 0.9129\n",
      "Epoch 57/100\n",
      "2787/2787 [==============================] - 1s 236us/step - loss: 0.3032 - acc: 0.9275 - val_loss: 0.3268 - val_acc: 0.9129\n",
      "Epoch 58/100\n",
      "2787/2787 [==============================] - 1s 234us/step - loss: 0.2988 - acc: 0.9268 - val_loss: 0.3265 - val_acc: 0.9161\n",
      "Epoch 59/100\n",
      "2787/2787 [==============================] - 1s 218us/step - loss: 0.2899 - acc: 0.9322 - val_loss: 0.3284 - val_acc: 0.9161\n",
      "Epoch 60/100\n",
      "2787/2787 [==============================] - 1s 218us/step - loss: 0.2775 - acc: 0.9347 - val_loss: 0.3262 - val_acc: 0.9194\n",
      "Epoch 61/100\n",
      "2787/2787 [==============================] - 1s 207us/step - loss: 0.2970 - acc: 0.9275 - val_loss: 0.3293 - val_acc: 0.9194\n",
      "Epoch 62/100\n",
      "2787/2787 [==============================] - 1s 226us/step - loss: 0.2902 - acc: 0.9290 - val_loss: 0.3280 - val_acc: 0.9194\n",
      "Epoch 63/100\n",
      "2787/2787 [==============================] - 1s 214us/step - loss: 0.2787 - acc: 0.9333 - val_loss: 0.3253 - val_acc: 0.9194\n",
      "Epoch 64/100\n",
      "2787/2787 [==============================] - 1s 212us/step - loss: 0.2929 - acc: 0.9261 - val_loss: 0.3268 - val_acc: 0.9161\n",
      "Epoch 65/100\n",
      "2787/2787 [==============================] - 1s 226us/step - loss: 0.2957 - acc: 0.9268 - val_loss: 0.3270 - val_acc: 0.9161\n",
      "Epoch 66/100\n",
      "2787/2787 [==============================] - 1s 227us/step - loss: 0.2938 - acc: 0.9293 - val_loss: 0.3275 - val_acc: 0.9161\n",
      "Epoch 67/100\n",
      "2787/2787 [==============================] - 1s 222us/step - loss: 0.2973 - acc: 0.9268 - val_loss: 0.3216 - val_acc: 0.9194\n",
      "Epoch 68/100\n",
      "2787/2787 [==============================] - 1s 222us/step - loss: 0.2785 - acc: 0.9351 - val_loss: 0.3261 - val_acc: 0.9194\n",
      "Epoch 69/100\n",
      "2787/2787 [==============================] - 1s 224us/step - loss: 0.2850 - acc: 0.9264 - val_loss: 0.3247 - val_acc: 0.9194\n",
      "Epoch 70/100\n",
      "2787/2787 [==============================] - 1s 213us/step - loss: 0.2771 - acc: 0.9383 - val_loss: 0.3252 - val_acc: 0.9194\n",
      "Epoch 71/100\n",
      "2787/2787 [==============================] - 1s 211us/step - loss: 0.2751 - acc: 0.9354 - val_loss: 0.3246 - val_acc: 0.9194\n",
      "Epoch 72/100\n",
      "2787/2787 [==============================] - 1s 214us/step - loss: 0.2862 - acc: 0.9343 - val_loss: 0.3243 - val_acc: 0.9194\n",
      "Epoch 73/100\n",
      "2787/2787 [==============================] - 1s 213us/step - loss: 0.2801 - acc: 0.9307 - val_loss: 0.3246 - val_acc: 0.9194\n",
      "Epoch 74/100\n",
      "2787/2787 [==============================] - 1s 199us/step - loss: 0.2820 - acc: 0.9361 - val_loss: 0.3232 - val_acc: 0.9226\n",
      "Epoch 75/100\n",
      "2787/2787 [==============================] - 1s 206us/step - loss: 0.2760 - acc: 0.9290 - val_loss: 0.3211 - val_acc: 0.9194\n",
      "Epoch 76/100\n",
      "2787/2787 [==============================] - 1s 207us/step - loss: 0.2899 - acc: 0.9297 - val_loss: 0.3206 - val_acc: 0.9226\n",
      "Epoch 77/100\n",
      "2787/2787 [==============================] - 1s 215us/step - loss: 0.2772 - acc: 0.9315 - val_loss: 0.3216 - val_acc: 0.9226\n",
      "Epoch 78/100\n",
      "2787/2787 [==============================] - 1s 214us/step - loss: 0.2713 - acc: 0.9379 - val_loss: 0.3224 - val_acc: 0.9226\n",
      "Epoch 79/100\n",
      "2787/2787 [==============================] - 1s 214us/step - loss: 0.2761 - acc: 0.9322 - val_loss: 0.3223 - val_acc: 0.9194\n",
      "Epoch 80/100\n",
      "2787/2787 [==============================] - 1s 217us/step - loss: 0.2703 - acc: 0.9376 - val_loss: 0.3224 - val_acc: 0.9194\n",
      "Epoch 81/100\n",
      "2787/2787 [==============================] - 1s 216us/step - loss: 0.2751 - acc: 0.9275 - val_loss: 0.3191 - val_acc: 0.9194\n",
      "Epoch 82/100\n",
      "2787/2787 [==============================] - 1s 229us/step - loss: 0.2729 - acc: 0.9343 - val_loss: 0.3170 - val_acc: 0.9226\n",
      "Epoch 83/100\n",
      "2787/2787 [==============================] - 1s 223us/step - loss: 0.2705 - acc: 0.9340 - val_loss: 0.3168 - val_acc: 0.9226\n",
      "Epoch 84/100\n",
      "2787/2787 [==============================] - 1s 229us/step - loss: 0.2772 - acc: 0.9311 - val_loss: 0.3161 - val_acc: 0.9226\n",
      "Epoch 85/100\n",
      "2787/2787 [==============================] - 1s 221us/step - loss: 0.2694 - acc: 0.9397 - val_loss: 0.3146 - val_acc: 0.9226\n",
      "Epoch 86/100\n",
      "2787/2787 [==============================] - 1s 227us/step - loss: 0.2744 - acc: 0.9354 - val_loss: 0.3157 - val_acc: 0.9226\n",
      "Epoch 87/100\n",
      "2787/2787 [==============================] - 1s 231us/step - loss: 0.2705 - acc: 0.9343 - val_loss: 0.3173 - val_acc: 0.9226\n",
      "Epoch 88/100\n",
      "2787/2787 [==============================] - 1s 220us/step - loss: 0.2873 - acc: 0.9261 - val_loss: 0.3191 - val_acc: 0.9226\n",
      "Epoch 89/100\n",
      "2787/2787 [==============================] - 1s 227us/step - loss: 0.2687 - acc: 0.9325 - val_loss: 0.3189 - val_acc: 0.9226\n",
      "Epoch 90/100\n",
      "2787/2787 [==============================] - 1s 227us/step - loss: 0.2751 - acc: 0.9372 - val_loss: 0.3177 - val_acc: 0.9226\n",
      "Epoch 91/100\n",
      "2787/2787 [==============================] - 1s 227us/step - loss: 0.2732 - acc: 0.9293 - val_loss: 0.3205 - val_acc: 0.9226\n",
      "Epoch 92/100\n",
      "2787/2787 [==============================] - 1s 224us/step - loss: 0.2708 - acc: 0.9318 - val_loss: 0.3230 - val_acc: 0.9194\n",
      "Epoch 93/100\n",
      "2787/2787 [==============================] - 1s 215us/step - loss: 0.2623 - acc: 0.9386 - val_loss: 0.3216 - val_acc: 0.9226\n",
      "Epoch 94/100\n",
      "2787/2787 [==============================] - 1s 220us/step - loss: 0.2741 - acc: 0.9322 - val_loss: 0.3210 - val_acc: 0.9194\n",
      "Epoch 95/100\n",
      "2787/2787 [==============================] - 1s 223us/step - loss: 0.2794 - acc: 0.9333 - val_loss: 0.3210 - val_acc: 0.9194\n",
      "Epoch 96/100\n",
      "2787/2787 [==============================] - 1s 218us/step - loss: 0.2728 - acc: 0.9307 - val_loss: 0.3216 - val_acc: 0.9194\n",
      "Epoch 97/100\n",
      "2787/2787 [==============================] - 1s 219us/step - loss: 0.2701 - acc: 0.9325 - val_loss: 0.3195 - val_acc: 0.9194\n",
      "Epoch 98/100\n",
      "2787/2787 [==============================] - 1s 220us/step - loss: 0.2662 - acc: 0.9379 - val_loss: 0.3182 - val_acc: 0.9194\n",
      "Epoch 99/100\n",
      "2787/2787 [==============================] - 1s 218us/step - loss: 0.2700 - acc: 0.9343 - val_loss: 0.3155 - val_acc: 0.9226\n",
      "Epoch 100/100\n",
      "2787/2787 [==============================] - 1s 220us/step - loss: 0.2737 - acc: 0.9354 - val_loss: 0.3148 - val_acc: 0.9226\n"
     ]
    }
   ],
   "source": [
    "# Train The Model\n",
    "\n",
    "print('Model Train 1')\n",
    "model_train1 = model.fit(train_X1, train_Y1, epochs=epochs, batch_size=32, \n",
    "                        validation_data=(valid_X1, valid_Y1), callbacks=callback_list1)\n",
    "model.load_weights(\"model.hdf5\")\n",
    "print('Model Train 2')\n",
    "model_train2 = model.fit(train_X2, train_Y2, epochs=epochs, batch_size=32, \n",
    "                        validation_data=(valid_X2, valid_Y2), callbacks=callback_list2)\n",
    "model.load_weights(\"model.hdf5\")\n",
    "print('Model Train 3')\n",
    "model_train3 = model.fit(train_X3, train_Y3, epochs=epochs, batch_size=32, \n",
    "                        validation_data=(valid_X3, valid_Y3), callbacks=callback_list3)\n",
    "model.load_weights(\"model.hdf5\")\n",
    "print('Model Train 4')\n",
    "model_train4 = model.fit(train_X4, train_Y4, epochs=epochs, batch_size=32, \n",
    "                        validation_data=(valid_X4, valid_Y4), callbacks=callback_list4)\n",
    "model.load_weights(\"model.hdf5\")\n",
    "print('Model Train 5')\n",
    "model_train5 = model.fit(train_X5, train_Y5, epochs=epochs, batch_size=32, \n",
    "                        validation_data=(valid_X5, valid_Y5), callbacks=callback_list5)\n",
    "model.load_weights(\"model.hdf5\")\n",
    "print('Model Train 6')\n",
    "model_train6 = model.fit(train_X6, train_Y6, epochs=epochs, batch_size=32, \n",
    "                        validation_data=(valid_X6, valid_Y6), callbacks=callback_list6)\n",
    "model.load_weights(\"model.hdf5\")\n",
    "print('Model Train 7')\n",
    "model_train7 = model.fit(train_X7, train_Y7, epochs=epochs, batch_size=32, \n",
    "                        validation_data=(valid_X7, valid_Y7), callbacks=callback_list7)\n",
    "model.load_weights(\"model.hdf5\")\n",
    "print('Model Train 8')\n",
    "model_train8 = model.fit(train_X8, train_Y8, epochs=epochs, batch_size=32, \n",
    "                        validation_data=(valid_X8, valid_Y8), callbacks=callback_list8)\n",
    "model.load_weights(\"model.hdf5\")\n",
    "print('Model Train 9')\n",
    "model_train9 = model.fit(train_X9, train_Y9, epochs=epochs, batch_size=32, \n",
    "                        validation_data=(valid_X9, valid_Y9), callbacks=callback_list9)\n",
    "model.load_weights(\"model.hdf5\")\n",
    "print('Model Train 10')\n",
    "model_train10 = model.fit(train_X10, train_Y10, epochs=epochs, batch_size=32, \n",
    "                        validation_data=(valid_X10, valid_Y10), callbacks=callback_list10)\n",
    "model.load_weights(\"model.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEICAYAAABfz4NwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJztnXl4FUXWh9/DGgKy4wYSYEQhLGGJgCLKog4yCoqAYlBRAccRFwb1Q3F3cMYZFxTRERlQFEFGRdERHVkUHTcCsgiooLIEEAMioCAIOd8f1Te5udylb3KTkOS8z9PP7a6urq7q7lu/qlObqCqGYRiGUaGkI2AYhmEcGZggGIZhGIAJgmEYhuFhgmAYhmEAJgiGYRiGhwmCYRiGAZggGEGISEUR+VlEGifSb0kiIieKSML7VovIWSKyPuj4KxHp5sdvAe41WURuL+j1huGXSiUdAaPgiMjPQYfJwH7gkHd8japOjyc8VT0E1Ei03/KAqp6ciHBEZBgwRFW7B4U9LBFhG0YsTBBKMaqamyF7JdBhqjovkn8RqaSqB4sjboYRC/sejzzMZFSGEZG/iMhLIjJDRPYAQ0TkVBH5RER+EpGtIvK4iFT2/FcSERWRJt7xC975uSKyR0Q+FpGm8fr1zp8rIl+LyC4RmSAi/xORoRHi7SeO14jIOhHZKSKPB11bUUQeFZEdIvIt0DvK8xkrIjND3CaKyCPe/jARWeOl5xuv9B4prCwR6e7tJ4vI817cVgEdQ/zeISLfeuGuEpG+nnsb4Amgm2eO2x70bO8Juv6PXtp3iMhrInKcn2cTz3MOxEdE5onIjyLyvYjcGnSfO71nsltEMkXk+HDmORH5MPCevee5yLvPj8AdItJcRBZ699juPbdaQdeneGnM9s4/JiJJXpxbBvk7TkT2iki9SOk1fKCqtpWBDVgPnBXi9hfgAHA+TvyrAacAnXG1w2bA18BIz38lQIEm3vELwHYgHagMvAS8UAC/RwN7gH7euT8DvwFDI6TFTxxfB2oBTYAfA2kHRgKrgEZAPWCR+8zD3qcZ8DNQPSjsH4B07/h8z48APYF9QFvv3FnA+qCwsoDu3v5DwHtAHSAFWB3idxBwnPdOLvXicIx3bhjwXkg8XwDu8fbP8eLYDkgCngQW+Hk2cT7nWsA24EagKlAT6OSduw1YDjT30tAOqAucGPqsgQ8D79lL20HgWqAi7ns8CegFVPG+k/8BDwWl5wvveVb3/Hf1zk0CxgXdZzQwu6T/h6V9K/EI2JagFxlZEBbEuO5m4N/efrhM/p9BfvsCXxTA71XAB0HnBNhKBEHwGccuQedfBW729hfhTGeBc31CM6mQsD8BLvX2zwW+iuL3TeA6bz+aIGwMfhfAn4L9hgn3C+AP3n4sQXgOeCDoXE1cu1GjWM8mzud8GbA4gr9vAvENcfcjCN/GiMOAwH2BbsD3QMUw/roC3wHiHS8D+if6f1XeNjMZlX02BR+ISAsR+Y9nAtgN3AfUj3L990H7e4nekBzJ7/HB8VD3D86KFIjPOPq6F7AhSnwBXgQGe/uXeseBeJwnIp965oyfcKXzaM8qwHHR4iAiQ0VkuWf2+Alo4TNccOnLDU9VdwM7gYZBfny9sxjP+QRcxh+OaOdiEfo9Hisis0RksxeHZ0PisF5dB4Z8qOr/cLWN00WkNdAY+E8B42R4mCCUfUK7XD6NK5GeqKo1gbtwJfaiZCuuBAuAiAj5M7BQChPHrbiMJECsbrGzgLNEpCHOpPWiF8dqwMvAX3HmnNrAf33G4/tIcRCRZsBTOLNJPS/cL4PCjdVFdgvODBUI7yicaWqzj3iFEu05bwJ+F+G6SOd+8eKUHOR2bIif0PQ9iOsd18aLw9CQOKSISMUI8ZgGDMHVZmap6v4I/gyfmCCUP44CdgG/eI1y1xTDPd8EOojI+SJSCWeXblBEcZwF3CQiDb0Gxv+L5llVv8eZNZ7FmYvWeqeq4uza2cAhETkPZ+v2G4fbRaS2uHEaI4PO1cBlitk4bRyOqyEE2AY0Cm7cDWEGcLWItBWRqjjB+kBVI9a4ohDtOc8BGovISBGpKiI1RaSTd24y8BcR+Z042olIXZwQfo/rvFBRREYQJF5R4vALsEtETsCZrQJ8DOwAHhDXUF9NRLoGnX8eZ2K6FCcORiExQSh/jAauwDXyPo1r/C1SVHUbcDHwCO4P/jvgc1zJMNFxfAqYD6wEFuNK+bF4EdcmkGsuUtWfgFHAbFzD7ACcsPnhblxNZT0wl6DMSlVXABOAzzw/JwOfBl37LrAW2CYiwaafwPVv40w7s73rGwMZPuMVSsTnrKq7gLOBi3Ai9TVwpnf6H8BruOe8G9fAm+SZAocDt+M6GJwYkrZw3A10wgnTHOCVoDgcBM4DWuJqCxtx7yFwfj3uPe9X1Y/iTLsRhkCDjGEUG54JYAswQFU/KOn4GKUXEZmGa6i+p6TjUhawgWlGsSAivXE9evbhui3+hislG0aB8Npj+gFtSjouZQUzGRnFxenAtzjb+e+BC60R0CgoIvJX3FiIB1R1Y0nHp6xgJiPDMAwDsBqCYRiG4VGq2hDq16+vTZo0KeloGIZhlCqWLFmyXVWjdfUGSpkgNGnShMzMzJKOhmEYRqlCRGKN2AfMZGQYhmF4mCAYhmEYgAmCYRiG4WGCYBiGYQAmCIZhGIaHL0EQkd4i8pW3LN+YMOdTRGS+iKwQkfdEJHiq40Misszb5gS5N/Xmml8nbpnHKolJkmEYpY3p06FJE6hQwf1On17SMSqfxBQEbyKyibjVpFKBwSKSGuLtIWCaqrbFzcT416Bz+1S1nbf1DXJ/EHhUVU/ELfBxdSHSYRhGKWX6dBgxAjZsAFX3O2JE4UXBRCZ+/NQQOgHrVPVbVT0AzMRNKBVMKrDA218Y5nw+vAVSepI3NfFzwAV+I20YRtlh7FjYuze/2969zr2gJFpkyou4+BGEhuRf9i6Lw1e7Wg709/YvBI7yFicBSBKRTBH5REQCmX494CdvvvNIYQIgIiO86zOzs7N9RNcwjNLExghT00Vy90M0kfGbuQf8icBll4UXl7ImFIkaqXwz8ISIDMUtcr4Zt/A3QIqqbvamql0gIitxi2H4QlUn4RbgID093WbiM4wyRuPGLpMN515QIolJIDMPiEXgGCAjaJmhQA0j4C90DtC9e+HGG2HfvthhlSb81BA2k3992EaErN+qqltUtb+qtgfGem4/eb+bvd9vgfeA9rhVs2p7yymGDdMwjPLBuHGQnJzfLTnZuQeXwOvXd5uf0ngkMalY0Z95KlwNI5QdO/yFVdA0lAR+BGEx0NzrFVQFuAS31F0uIlJfRAJh3QZM8dzreOu+IiL1ga7Aam+pvYXkLYd3BfB6YRNjGEbpIyMDJk2ClBRnnklJcceQvx1gxw63+WkTiCQyhw6F9x9aoyiMuSr42tC2jHjSUCKoaswN6INbU/UbYKzndh/Q19sfgFsH9mvcAtxVPffTcGueLvd+rw4Ksxluxax1wL8D10TbOnbsqIZhlA9SUlRd1hl9S0lRfeGFw69/4QV3TkS1Xj23RQsjnnsnJ0cOLxCfeOMfiGuk9BQGIFP95PV+PB0pmwmCYRQdicqUEhWOiL8MNZBBR7rPCy+48/FcG+6aQHwCaQrnJzlZ9dpro98vUhz8pqcgmCAYhuGbSJlbvJmSn4zUL35L2JFK+X7CiRYnP8IWzk+88a5YMb70FAQTBMMoQxS1SSFSJhZvpuTH1OI37rFK9qGbSPhwItU0wvlPxHOOt2YTb3oKggmCYXgUdWZa1PdLZKk7En4zzXB2+eB0+skMY4lMpHvE2w4QwK/YJaqWFE0U401Por4lEwTD0MT9yUvyfoksdcd7j1AzSLQMP1pDq9+Sr5/nF+8z9us/HuGIlkkXNA2h/sO1RRT0XZsgGIb6y+jiKX3FuiZRppdg/JS6K1aMXGr3QyIbSGPFN9qzKEim7CfNft61n1qSX3Ep6HcSToQT8S2ZIBiGxi7RxlP68pMZRLtfQav88TZSFrREmYgG0lBRCH0eseITj70/EOdElaL9ZMCJyqT9pDPeZxENEwSj2ChuG308RPoDR+vZESk9hckwQjPKwpbe490KWnKOp4HUz7O89trE1rAKm0GH1jSqVMkfjl/BF4nvf1Cc4qOqJghG8VDcNvp4iRS/aBlZpPQU1KQQ7T7xlt6DRSXerSC2+MLUTsI1SCe6faAwpehw96pcObr5KdLzqFcv8e0aifxvmSAYxUJR2MwTTTwjVqPVHOK1b/vJNONtsAz1Eym+fu5X0IbkwHEiR/8GpzXRJe1EXhspk442ajlaWOHSGW/7iB9MEIxiIVFV6EQT7t6xSu/RSt6B9MRTYvMjCgVpsAxNZ7zmJD9dROPp5uo33rFqNokaCOc3nILWLsJ9W4my9xdVjdsEwSgWElWFTiTxluJiiUFwSS9WH/l4BCg4g41Veo6VXr81ID/PoyAjZ/2Ivx+BLEjNsqAl6kTWbhMVVlHVuE0QjGIhkVXoRFEYu3e4LZxd2U8/8oC/WKYXv4O54hFTP2IUSbSjiWZh8BOnwtyjqMYnFMW9I5HInkXBmCAYxUZhq9CJNi0VpmdMuEwzXM8TP6XwaG0LBYljvBmMn7aMcCXqomwXKmxtKBoFbRNI1LeXiLCshmCCUGooisa+orCZxmvGilabSWQvm1jx87Ml0pwR6XkXR8+xorhHUZWuixNrQzBBKDDF2WibyOq4n14yhSkR+b13NPNPrK6mhcm8CxNmQTI3v20ZodcU9beV6HuUhh5vfiiKZ2+CUMYprv7/haniF6SnT6JKdfH+qSL5j6e24fdd+CmxJ7oNJtZ7LE2l6EgU13+iNGKCUMYpjtJQUTQCxmMuKeq+2X7wW9uIJ07hwgzt1llUmVtZKUVHoiS7Oh/JmCCUcRJhL4315ymKboLxmkv89MQp6lJgUWQyfsIsqvtaKbr8YYJQyiloZu23z3q4TDa0lFoUA4kixTveEbexahRGZKwUXf5IqCAAvYGvgHXAmDDnU4D5wArgPaCR594O+BhY5Z27OOiaZ4HvgGXe1i5WPMqLIBSVScGv/T4Qlp/FOxKRtkQ04Fpp1zAikzBBACoC3wDNgCrAciA1xM+/gSu8/Z7A897+SUBzb/94YCtQW/MEYYCfSAa28iIIfrsJxlvSK0h3x0jTFxemlBlvA67VFgyjcCRSEE4F3gk6vg24LcTPKuAEb1+A3RHCWh4kECYIEfA7ctUvfgYo+REFP7WTwgpFYad5ttqCYRxOIgVhADA56Pgy4IkQPy8CN3r7/QEF6oX46QSsASponiB85ZmSHgWqRrj/CCATyGzcuHHRP7kjAD+Zt9/G40RlssECFO/Ar3iEwk8bh9/5euIVTsMoqxS3IBwPvAp8DjwGZAVMQ97547zMv0uImwBVgeeAu2LFpbzUEPxm4okwE/mdT8fPSk6RtoJOdBdLRIqiW6xhlEWK1WQU4r8GkBV0XBNYGs08BHQH3owVl/IiCKr+570p6JKEoYISzwC0RNn6j/S5cQyjrOBXECoQm8VAcxFpKiJVgEuAOcEeRKS+iATCug2Y4rlXAWYD01T15ZBrjvN+BbgA+MJHXMoNGRmwfr3L1p5/HlJSwvvbuxfGjo0cTuPG4d1TUlz4GRn57/fCC5CcnN9vcjKMG5d3PG5ceD/16kVJUBg2bozPfzjiibdhGDHwoxpAH+BrXG+jsZ7bfUBfb38AsNbzMxmvPQAYAvxGXtfS3O6lwAJgJU4IXgBqxIpHaa4hJKLvdzyL0QTWrg1Xw/DTPbUgg6YiNTYX11TY1r/eMMKDDUw7cvA7ZqCgg9FizasTLArFMdVDPJPHGYZR9PgVBHF+Swfp6emamZlZ0tGImyZNYMOG8OdSUvLMGiNGOBNQgORkmDQpz6wzfXp4P9WqwY4dseMRMBOVBNOnO9PWxo3OjDVuXF66DMMoWkRkiaqmx/RnglD0VKjgysWRiJaph2bi4TLWyy6LHn4AEcjJiTv6hmGUcvwKgp9GZaOQRGrYDbB3b+QSfmjDa6ARNScnr1E4Vvh+42EYRvnGBKEImT49z1wkUrAw/GTi4Xr9hGI9bgzDiIUJQgEJZPYVKrjf6dMPPz9iRF7bgWr8ouA3E8/IcG0NKSnuHikpcO21+Y+D2yIMwzDCYW0IPgm23detC3v2wIEDeedDG4AjNSTXqwf79uVvGA5HoLHZMnHDMAqLtSEkkODSvqqz9weLARw+QCzSoKsff8wrzUcidNCYYRhGcWCC4IOxY2OX6MEJRsB8FMn237ixja41DOPIxATBB/FMsbBhg+sGGq4hOTSzD2f7N1u/YRglRaWSjkBpoHHjyAPLwhFolgk0JKtGbhPIyDABMAzjyMBqCD4I162zcmV/k7kFxMDaBAzDONIxQfBBONPO1KmwfXv0xuEAiZjV0zAMo6gxQYhC8FiDsWNdTSF4hDD4GxRmI4QNwygNmCBEILSr6YYN7jh0AFpw7QFiNyQbxpFAKRp+ZBQjJggRCNfVNNJiNOEWs7FeQ8aRyoEDkJ4Of/6zCYORH+tlFIFIdv9Y7QHWa8g40nn1VVi61G0NG8Lo0SUdI+NIwWoIEYg2sMwwSjOPPw4nnggDB8Itt8Brr5V0jIwjhXJdQ9iyBTZtgs6dDz83blz4xWisPeDI4IMPYNmyvOOzzoKWLfP7WbvWTTPSpYu/MA8cgPnz4fe/dx0J4mXBAmjXzs11VZwcOgRvv+2eQdWq0f1mZsLHH8P48e773rgRLr0U7r0XkpL837NVK+jZs3DxNo5A/CyrBvQGvgLWAWPCnE8B5gMrgPeARkHnrsCtt7wWuCLIvSNuTeV1wON4E+1F2xK5hOa2bapNm6pWqaL6zDN5yz7Wq+e20H1bo/fI4dAh1bp18y/JWaeO6pdf5vn55hvV+vVVK1VSXbDAX7iPPurCGjMm/jhNmeKuvfji+K8tLJMnu3tnZKjm5ET3e/nlqtWrq/70kzv+/nvV5s2jL78aaUnWV14p+rQZiYFErakMVAS+AZoBVYDlQGqIn38HMnugJ/C8t18X+Nb7rePt1/HOfQZ0AQSYC5wbKy6JEoR9+1RPPTVvneEqVSJ/+Lb275HHl1+6d/P446rbt6uuXKnaoIHq736nmp2t+uOPqi1aONFo0UK1dm3VNWtih3vWWXnfxL/+5T8+8+c74UlOdr+bNxc8bfGSk6Patm3emtV33x3Z77Zt7lu/7rr87r/95p6j323rVtUuXVSrVVP97LMiTZ6RIBIpCKcC7wQd3wbcFuJnFXCCty/Abm9/MPB0kL+nPbfjgC+D3PP5i7QlQhAOHXKlOFB98cW8DCDalpJS6NsaCeS559x7Wbkyz+2jj1SrVlXt2lW1Z0/VypVV339f9bvvVI8+WrVZM9Uffogc5p49LrO84QbVs892Gfu8ebHjsmaNE5xWrVSXLnXf0513FjqJvnn/ffcsJk1SHTrU7T//fHi/99/vzvsRx1hs26bapInqMceorl9f+PCMyOTkuHyrMCRSEAYAk4OOLwOeCPHzInCjt98fUKAecDNwR5C/Oz23dGBekHs34M1YcUmEIDzyiEv13/4WeFCxN5FC39Y3Eyao/vnPxXe/0sh116nWqKF68GB+95deyntnzz2X5/7JJ6pJSaqnneZqh+GYM8dd9+67zpzSqpUrdaemRt9q13aC8913LpzzznPHv/4aOx333hs53IsuUv3559hhXHSRM5f98ovq/v2qPXo4MQwXZnKy6jnnxA7TL6tWqdaq5cyqsZ7Taac5/0XFww/nv99ll7nnUZwcOKB61VUuLsHk5LhCwo03Rs/Y33rLvc/ffsvv/sEHzqwXXACKF7+CkKhG5ZuBJ0RkKLAI2AwcSkTAIjICGAHQOAFdfObPh9RUuPVWd1y3rlujIBrF1bPo55/dOIfdu12D38knF899SxuffgqnnAIVK+Z3HzQIfv3VNQ5ffnmee+fOMG2aO3/VVW5wYegAwrlzoXp16NbNNcy+9Rbcfbd7J9FIS3M9dZo0ccc33ADnnAOzZrlZbyMxYYILv1s3OOaY/OcOHYLZs93vyy8fns4AGze6HkKjR+eNln/lFbjttvBrdLduDf/3f9HTEw+pqe45Pf64i2s03n8f/vAH+OSTw9NbWHbvds+ycWMXp/373XigChXcFDMFXb42HlThuutgyhR3XK8eXHGF23/kEbj/frdfvXr4jimqcPvtrqPE66/DRRflnZswAbKzoWnTok2DF5HCm4xC/NcAsvQINRm1a6f6hz/kHT/2WPTaQXG2ITz5pLtnhQqqI0cWzz1LG/v2uRLw//1f/Nc+8IB7vqEmnZwcZ/44//zCxy8nx7VbpKdHbuB98033jvv1O7yWEyDwXY4eHfleY8a4cEqDyWbxYtfm0Lmz6t69iQ378cfdswpuz7jnHuf2l78k9l6R+Pvf3f1uuUW1Vy/3jS5cqDp7trMwDByoOmyY8zNlyuHXf/BB3n//zDPz3DdtUq1YMfp34Ad81hBiLqEpIpWAr4FeuJL/YuBSVV0V5Kc+8KOq5ojIOOCQqt4lInWBJUAHz+tSoKOq/iginwE3AJ8CbwETVPWtaHFJxBKaDRpA//7w9NN5biec4Caq278/r8vgjz+6EoffZSxXr86raVSsCB07QpUq+f1kZbkRzQHatYMaNdy+quvKl5zsSjmzZ8PmzVCzZoGTGpGtW+HYY4uu5HTgACxZErvUWL26ewbxxOOTT+DUU93gqgsvjC9eqjBsmCvFPfdcXi3iyy9dl9Unn3RrUReWJ590pcVp0w4v1W3fDkOGQIsWrtRcvXrkcK6/Hp54Av75T7jmmvzn9u1z3+0ZZ7hnURqYPduVfPv3h5tucm5JSdChw+HdfDdujDwItH37vOeWk+OeZb16rjttAFX3fl94wZWw27VLfHoCfPEF/OlPMGAAzJzpaiynneb+Z/v3Q9u2sHAhVKoEffrAe+/Bf/8LPXrkhTFoEMyb52qY994Ly5e76+64Ax54AL75pnA1BL9LaMZUDE8w+uBE4RtgrOd2H9BX89oZ1np+JgNVg669Cte1dB1wZZB7OvCFF+YTFEO3019/dSp8332u1B/oalqzpmtE3LOnYOGuXHl44/Qdd+T3s3evsy0H+zn5ZNcjRtXZrkH12WddaQpcKTHRvPaai+uECYkPO0Cg8dLP9sEH8YU9fry7rqA9eYLt7O+/79wC7UqBdoDCsmePa1uIlOZGjfzF/7ffVM8915UQ33kn/7nRo11YCxcmJs7FRaAkHbxdc03+2tSiRdF7/rVsqbpzp/M7d65zmz798Hv9+qtqt27+v8XCbF265K/5fPut6/nWpInr2htg507XxhHcTTpQC7j5ZtUdO1xNatgwVxtu0EC1b9/CP3cSVUM4kihsDeG776BZMxg+3NmRQ+cq+vOf4eGH4w93xAhXEnn5ZVcr+Pvf4fPP3aC3wGCfqVOd/XriRDjpJDcobtgw6NoV3nnHjRr9+GNXKkpKcqXgHTtc6bUgg6TCsWSJK1Hu3etKJwsWJCbcUDp1gt9+g3/8I7KfX3+F8893pZ/bbvMf9qWXukFpmzYVPH47d7oS3A8/uBrHyJHuua9ZU/AwQ/nmG/e9haN9e39raQDs2QOnn+5qlv/7n2sHePpp+OMfXbwnTEhYlIsFVTdlxs6d7njOHJeGhx5ybSHr1rmBhPXqubaJ0PaTrCz3/+3e3bVf9Ovn/msbNhxeIwf3nX30katJFBUVKrj/a7Vq+d23bXNxqlMnv/t337k0HnWU+/4efRT++te8WkAgPxk3zuVJ777rBh0WhoTWEI6UrbA1hEWLnJqHltQDW40a8YcZrOgB5s934U2d6o5zclTbt1dt3Tp/Sej5552/vn1dqX3s2LxzL77ozs2dW6CkHsbGjarHHafauLEbnFS5suru3YkJO5gffnBpue++2H5POsnZ0eOhWTPV/v0LFrdgAgPXTjzRdVcdNarwYRYVgXeXkqI6bZorTfbpc3hvlNLIoUPOvi7ixn6cdJLrtbR2beRrpk51/41+/dzvPfcUW3QTxscfu55vXbu67zD4f7BihUtXxYquNhRrsKEfSFS30yNpK6wgXHdd7KrfkCFuu/XWwxu/tm5Vve021wc7QKAKvHx5nltOjuu22L692w80GD399OFxuvvuvJeflZXnvn+/6rHHuj9IIE7B2223+e9Wt2+falqaM42tXOlG7oJr8ArmscdUP/zw8OtnzFB99VV/9wqI3OLFsf0OGeLSGOmDz8lxz/fzz91xdrYL+8EH/cUlFv/7nxMDUP3vfxMTZlGRmZk3+CwtrWjEvKTYu9c1NoMzFfkxI95+u/NfubL7X5ZGZs3Ky3dCx7x07+7cn3wyMfcyQQjhhRfcxxNNDCpVciXQpk3d8cUX5/Ub/vln1Y4dNZ+98OBBV2oL7hUQ4J//dH4//FB10CBnUw7Xrzwnx9kOw40wnTTJjb5t1iz/Fojf5Zf7Kz3861/5BWD/flcbuuaaPD+rV+dlNsFh/vSTm+qgbl1/vUMuvdTVwPwMpJkwwd1zw4bw5wPtKg0aOJvsf/7jjt97L3bYfnn1VdULL/Q3bqCkefNNN+hu48aSjkni+f57NyDw3//25//QIVer81MTPZKZONGNmQj9H7/3nuutVNB2zVBMEEJISYkuBqHdS//2N+d+xx3u47vwQtcl7KabnPugQW4uF1B9+eXD7/fzz04EundPTLexUAI1i/vvj+4vJ8d1tQ01V11wgTMfBdz+9Ke8Z7FoUZ6/QCMuuDlzonHwoKvuX3aZvzR89pkLN1Im0LevC69OHVd1vvFG9w4S9ScxjPKCCUII0aaoCDdxXU6O6tVXu/M9erjf8ePduQcfdMdHHaV6wgmRbbmBniAiroSbSHJynMkFXHtDJCKZq55+2rmvWpVXCxg0yGW+Awc6P4cOORv7qaeqtmlzeO0h4CfAJ5/Ejk8w+/c7E8HNNx9+7ptv8tpVFi7Mq921aeMvbMMw8jBBCCFSDSHaPEUHDrhqG7j2h0BmmJOTN8jkr3+NfH0gU0tEt7FwBLrVVa0aeXDSwIEukw81V23c6OL/0EOBtCPzAAAdn0lEQVR5s3wuWeIy54oV3fmAiWbGDGe+gryumocOqQ4e7NpKsrOd2913uxL89u3+09C5s0tDKKNH529XefZZd//hw/2HbRiGwwQhhBdeOLyW4GcU8q5dqjNnHl4LOHDAmYpi2Z7feqtobb4bNriM89ZbDz8X3L85HK1audpPs2ZurhlV1xe/QgXXaPf737veLQcOuLly6tRRHTDA+bv5Zs1tDO/a1T2HTp1c+0o83HCDew/Bzzdgbhs0KL/fN94om/ZzwyhqTBBCOHTIZV41a5a99Q0uusg1+v7yS373sWNd5h5pwFXApAVO9AJccIF7TpC/0e7WW90zvOuuvFpTYEK5fv3cc7333vjiPn26u37Zsjy34AZ5wzAKjwlCCN9/71JblCN0S4rAFMjPPJPntm/f4f2bQ5k3z113/PGuFhAgMI6iSpX8oyzXr3cCA24EbaBUP25cnrDEOz/+unWar43j0KH8XXYNwyg8fgWh3KypnJXlfhs1Ktl4FAXdurl5TyZMyDOIXXONmzdn1Kjo1zVu7GbrrFw5z71HDzeS8qqr8s9MmZICQ4e6cy+95OZmATfS+E9/cjN/duwYX9ybNXOjUj/91B3ffTesWuVGrRbHLJWGYeRRbtZU3rzZ/TZsWLLxKApE3ERow4fDokVuaodp09wkWWeeGfm6KlXyT7YXHF7wRGHBTJ6c5yfY/8SJTojizcRF3FQXn33mJpz7y1/clB6XXhpfOIZhFJ5yU0Moy4IALgOtW9eJwp13uhk177wz9nUi8WXi0fwXtETfubOrFQwfDr16udlCrXZgGMVPuRGErCw3UVaiF+c4UkhOdiXrtWudKWjy5NKTqXbu7GoXJ57oJggMNl8ZhlF8lCuT0XHHRV59qiwQaAsYNcqt+lVa6N7dtRmMHAm1a5d0bAyj/FKuBKGsmosC1K/vbPCljaQkN/2xYRglS7kyGZXFHkaGYRiJotwIQnmoIRiGYRSGciEIu3e7ladMEAzDMCJTLgQh0OXUTEaGYRiR8SUIItJbRL4SkXUiMibM+cYislBEPheRFSLSx3PPEJFlQVuOiLTzzr3nhRk4d3Rik5ZHWR+DYBiGkQhi9jISkYrAROBsIAtYLCJzVHV1kLc7gFmq+pSIpAJvAU1UdTow3QunDfCaqi4Lui5DVTMTlJaIBKatMEEwDMOIjJ8aQidgnap+q6oHgJlAvxA/CtT09msBW8KEM9i7ttixGoJhGEZs/AhCQ2BT0HGW5xbMPcAQEcnC1Q6uDxPOxcCMELepnrnoTpHw42pFZISIZIpIZnZ2to/oHs7mzW5ah2rVCnS5YRhGuSBRjcqDgWdVtRHQB3heRHLDFpHOwF5V/SLomgxVbQN087bLwgWsqpNUNV1V0xs0aFCgyNkYBMMwjNj4Gam8GTgh6LiR5xbM1UBvAFX9WESSgPrAD975SwipHajqZu93j4i8iDNNTYs3AX4YPtx1OzUMwzAi40cQFgPNRaQpTgguAUInJ94I9AKeFZGWQBKQDeDVFAbhagF4bpWA2qq6XUQqA+cB8wqZloicf35RhWwYhlF2iCkIqnpQREYC7wAVgSmqukpE7sOtwjMHGA08IyKjcA3MQ71VegDOADap6rdBwVYF3vHEoCJODJ5JWKoMwzCMuJG8fPvIJz09XTMzi7yXqmEYRplCRJaoanosf+VipLJhGIYRGxMEwzAMAzBBMAzDMDxMEAzDMAzABMEwDMPwMEEwDMMwABMEwzAMw8MEwTAMwwBMEAzDMAwPEwTDMAwDMEEwDMMwPEwQDMMwDMAEwTAMw/AwQTAMwzAAEwTDMAzDwwTBMAzDAEwQDMMwDA8TBMMwDAMwQTAMwzA8fAmCiPQWka9EZJ2IjAlzvrGILBSRz0VkhYj08dybiMg+EVnmbf8MuqajiKz0wnxcRCRxyTIMwzDiJaYgiEhFYCJwLpAKDBaR1BBvdwCzVLU9cAnwZNC5b1S1nbf9Mcj9KWA40Nzbehc8GYZhGEZh8VND6ASsU9VvVfUAMBPoF+JHgZrefi1gS7QAReQ4oKaqfqKqCkwDLogr5oZhGEZC8SMIDYFNQcdZnlsw9wBDRCQLeAu4PuhcU8+U9L6IdAsKMytGmACIyAgRyRSRzOzsbB/RNQzDMApCohqVBwPPqmojoA/wvIhUALYCjT1T0p+BF0WkZpRwDkNVJ6lquqqmN2jQIEHRNQzDMEKp5MPPZuCEoONGnlswV+O1AajqxyKSBNRX1R+A/Z77EhH5BjjJu75RjDANwzCMYsRPDWEx0FxEmopIFVyj8ZwQPxuBXgAi0hJIArJFpIHXKI2INMM1Hn+rqluB3SLSxetddDnwekJSZBiGYRSImDUEVT0oIiOBd4CKwBRVXSUi9wGZqjoHGA08IyKjcA3MQ1VVReQM4D4R+Q3IAf6oqj96Qf8JeBaoBsz1NsMwDKOEENfJp3SQnp6umZmZJR0NwzCMUoWILFHV9Fj+bKSyYRiGAZggGIZhGB4mCIZhGAZggmAYhmF4mCAYhmEYgAmCYRiG4WGCYBiGYQAmCIZhGIaHCYJhGIYBmCAYhmEYHiYIhmEYBmCCYBiGYXiYIBiGYRiACYJhGIbhYYJgGIZhACYIhmEYhocJgmEYhgGYIBiGYRgeJgiGYRgG4FMQRKS3iHwlIutEZEyY841FZKGIfC4iK0Skj+d+togsEZGV3m/PoGve88Jc5m1HJy5ZhmEYRrxUiuVBRCoCE4GzgSxgsYjMUdXVQd7uAGap6lMikgq8BTQBtgPnq+oWEWkNvAM0DLouQ1UzE5MUwzAMozD4qSF0Atap6reqegCYCfQL8aNATW+/FrAFQFU/V9UtnvsqoJqIVC18tA3DMIxE40cQGgKbgo6zyF/KB7gHGCIiWbjawfVhwrkIWKqq+4PcpnrmojtFRMLdXERGiEimiGRmZ2f7iK5hGIZREBLVqDwYeFZVGwF9gOdFJDdsEWkFPAhcE3RNhqq2Abp522XhAlbVSaqarqrpDRo0SFB0DcMwjFD8CMJm4ISg40aeWzBXA7MAVPVjIAmoDyAijYDZwOWq+k3gAlXd7P3uAV7EmaYMwzCMEsKPICwGmotIUxGpAlwCzAnxsxHoBSAiLXGCkC0itYH/AGNU9X8BzyJSSUQCglEZOA/4orCJMQzDMApOTEFQ1YPASFwPoTW43kSrROQ+EenreRsNDBeR5cAMYKiqqnfdicBdId1LqwLviMgKYBmuxvFMohNnGIZh+Edcvl06SE9P18xM66VqGIYRDyKyRFXTY/mzkcqGYRgGYIJgGIZheJggGIZhGIAJgmEYhuFhgmAYhmEAJgiGYRiGhwmCYRiGAZggGIZhGB4mCIZhGAZggmAYhmF4mCAYhmEYgAmCYRiG4WGCYBiGYQAmCIZhGIaHCYJhGIYBmCAYhmEYHiYIhmEYBmCCYBiGYXiYIBiGYRiAT0EQkd4i8pWIrBORMWHONxaRhSLyuYisEJE+Qedu8677SkR+7zdMwzAMo3iJKQgiUhGYCJwLpAKDRSQ1xNsdwCxVbQ9cAjzpXZvqHbcCegNPikhFn2EahmEYxYifGkInYJ2qfquqB4CZQL8QPwrU9PZrAVu8/X7ATFXdr6rfAeu88PyEaRiGYRQjfgShIbAp6DjLcwvmHmCIiGQBbwHXx7jWT5gAiMgIEckUkczs7Gwf0TUMwzAKQqIalQcDz6pqI6AP8LyIJCRsVZ2kqumqmt6gQYNEBGkYhmGEoZIPP5uBE4KOG3luwVyNayNAVT8WkSSgfoxrY4VpGIZhFCN+BGEx0FxEmuIy7UuAS0P8bAR6Ac+KSEsgCcgG5gAvisgjwPFAc+AzQHyEaRhGBH777TeysrL49ddfSzoqxhFEUlISjRo1onLlygW6PqYgqOpBERkJvANUBKao6ioRuQ/IVNU5wGjgGREZhWtgHqqqCqwSkVnAauAgcJ2qHgIIF2aBUmAY5ZCsrCyOOuoomjRpgoiUdHSMIwBVZceOHWRlZdG0adMChSEu3y4dpKena2ZmZklHwzBKnDVr1tCiRQsTAyMfqsqXX35Jy5Yt87mLyBJVTY91vY1UNoxSiomBEUphvwkTBMMwDAMwQTCMcsH06dCkCVSo4H6nTy9ceDt27KBdu3a0a9eOY489loYNG+YeHzhwwFcYV155JV999VVUPxMnTmR6YSNr+MZPLyPDMEox06fDiBGwd6873rDBHQNkZBQszHr16rFs2TIA7rnnHmrUqMHNN9+cz4+qoqpUqBC+3Dl16tSY97nuuusKFsES5ODBg1SqVDqzVqshGEYZZ+zYPDEIsHevc08069atIzU1lYyMDFq1asXWrVsZMWIE6enptGrVivvuuy/X7+mnn86yZcs4ePAgtWvXZsyYMaSlpXHqqafyww8/AHDHHXcwfvz4XP9jxoyhU6dOnHzyyXz00UcA/PLLL1x00UWkpqYyYMAA0tPTc8UqmLvvvptTTjmF1q1b88c//pFAh5qvv/6anj17kpaWRocOHVi/fj0ADzzwAG3atCEtLY2x3sMKxBng+++/58QTTwRg8uTJXHDBBfTo0YPf//737N69m549e9KhQwfatm3Lm2++mRuPqVOn0rZtW9LS0rjyyivZtWsXzZo14+DBgwDs3Lkz33FxYoJgGGWcjRvjcy8sX375JaNGjWL16tU0bNiQv/3tb2RmZrJ8+XLeffddVq9efdg1u3bt4swzz2T58uWceuqpTJkyJWzYqspnn33GP/7xj1xxmTBhAsceeyyrV6/mzjvv5PPPPw977Y033sjixYtZuXIlu3bt4u233wZg8ODBjBo1iuXLl/PRRx9x9NFH88YbbzB37lw+++wzli9fzujRo2Om+/PPP+fVV19l/vz5VKtWjddee42lS5cyb948Ro0aBcDy5ct58MEHee+991i+fDkPP/wwtWrVomvXrrnxmTFjBgMHDiyRWoYJgmGUcRo3js+9sPzud78jPT2vh+OMGTPo0KEDHTp0YM2aNWEFoVq1apx77rkAdOzYMbeUHkr//v0P8/Phhx9yySWXAJCWlkarVq3CXjt//nw6depEWloa77//PqtWrWLnzp1s376d888/H3ADu5KTk5k3bx5XXXUV1apVA6Bu3box033OOedQp04dwAnXmDFjaNu2Leeccw6bNm1i+/btLFiwgIsvvjg3vMDvsGHDck1oU6dO5corr4x5v6LABMEwyjjjxkFycn635GTnXhRUr149d3/t2rU89thjLFiwgBUrVtC7d++wo6urVKmSu1+xYsWI5pKqVavG9BOOvXv3MnLkSGbPns2KFSu46qqrCjTKu1KlSuTk5AAcdn1wuqdNm8auXbtYunQpy5Yto379+lHvd+aZZ/L111+zcOFCKleuTIsWLeKOWyIwQTCMMk5GBkyaBCkpIOJ+J00qeINyPOzevZujjjqKmjVrsnXrVt55552E36Nr167MmjULgJUrV4atgezbt48KFSpQv3599uzZwyuvvAJAnTp1aNCgAW+88QbgMvm9e/dy9tlnM2XKFPbt2wfAjz/+CECTJk1YsmQJAC+//HLEOO3atYujjz6aSpUq8e6777J5s5uqrWfPnrz00ku54QV+AYYMGUJGRkaJ1Q7ABMEwygUZGbB+PeTkuN/iEAOADh06kJqaSosWLbj88svp2rVrwu9x/fXXs3nzZlJTU7n33ntJTU2lVq1a+fzUq1ePK664gtTUVM4991w6d+6ce2769Ok8/PDDtG3bltNPP53s7GzOO+88evfuTXp6Ou3atePRRx8F4JZbbuGxxx6jQ4cO7Ny5M2KcLrvsMj766CPatGnDzJkzad68OeBMWrfeeitnnHEG7dq145Zbbsm9JiMjg127dnHxxRcn8vHEhU1dYRilkDVr1hw2PUF55eDBgxw8eJCkpCTWrl3LOeecw9q1a0td18+ZM2fyzjvv+OqOG41w34bfqStK1xMzDMMI4eeff6ZXr14cPHgQVeXpp58udWJw7bXXMm/evNyeRiVF6XpqhmEYIdSuXTvXrl9aeeqpp0o6CoC1IRiGYRgeJgiGYRgGYIJgGIZheJggGIZhGIAJgmEYBaBHjx6HDTIbP3481157bdTratSoAcCWLVsYMGBAWD/du3cnVvfy8ePHszdoxr4+ffrw008/+Ym6EQVfgiAivUXkKxFZJyJjwpx/VESWedvXIvKT594jyH2ZiPwqIhd4554Vke+CzrVLbNIMwygqBg8ezMyZM/O5zZw5k8GDB/u6/vjjj4860jcWoYLw1ltvUbt27QKHV9yoau4UGEcSMQVBRCoCE4FzgVRgsIikBvtR1VGq2k5V2wETgFc994VB7j2BvcB/gy69JXBeVQ+fr9YwjJjcdBN0757Y7aabot9zwIAB/Oc//8ldDGf9+vVs2bKFbt265Y4L6NChA23atOH1118/7Pr169fTunVrwE0rcckll9CyZUsuvPDC3OkiwPXPD0ydfffddwPw+OOPs2XLFnr06EGPHj0AN6XE9u3bAXjkkUdo3bo1rVu3zp06e/369bRs2ZLhw4fTqlUrzjnnnHz3CfDGG2/QuXNn2rdvz1lnncW2bdsAN9bhyiuvpE2bNrRt2zZ36ou3336bDh06kJaWRq9evQC3PsRDDz2UG2br1q1Zv34969ev5+STT+byyy+ndevWbNq0KWz6ABYvXsxpp51GWloanTp1Ys+ePZxxxhn5pvU+/fTTWb58efQXFSd+xiF0Atap6rcAIjIT6AccPmGIYzBwdxj3AcBcVd0b5pxhGKWIunXr0qlTJ+bOnUu/fv2YOXMmgwYNQkRISkpi9uzZ1KxZk+3bt9OlSxf69u0bcb3fp556iuTkZNasWcOKFSvo0KFD7rlx48ZRt25dDh06RK9evVixYgU33HADjzzyCAsXLqR+/fr5wlqyZAlTp07l008/RVXp3LkzZ555JnXq1GHt2rXMmDGDZ555hkGDBvHKK68wZMiQfNeffvrpfPLJJ4gIkydP5u9//zsPP/ww999/P7Vq1WLlypWAW7MgOzub4cOHs2jRIpo2bZpvXqJIrF27lueee44uXbpETF+LFi24+OKLeemllzjllFPYvXs31apV4+qrr+bZZ59l/PjxfP311/z666+kpaXF9d5i4UcQGgKbgo6zgM7hPIpICtAUWBDm9CXAIyFu40TkLmA+MEZV94cJcwQwAqBxUc3XaxilGK8QXOwEzEYBQfjXv/4FOHPI7bffzqJFi6hQoQKbN29m27ZtHHvssWHDWbRoETfccAMAbdu2pW3btrnnZs2axaRJkzh48CBbt25l9erV+c6H8uGHH3LhhRfmzjzav39/PvjgA/r27UvTpk1p185ZpiNNsZ2VlcXFF1/M1q1bOXDgAE2bNgVg3rx5+UxkderU4Y033uCMM87I9eNniuyUlJRcMYiUPhHhuOOO45RTTgGgZs2aAAwcOJD777+ff/zjH0yZMoWhQ4fGvF+8JLpR+RLgZVU9FOwoIscBbYDgVqjbgBbAKUBd4P/CBaiqk1Q1XVXTGzRoEHeEEr2WrGEYjn79+jF//nyWLl3K3r176dixI+Ami8vOzmbJkiUsW7aMY445pkBTTX/33Xc89NBDzJ8/nxUrVvCHP/yhQOEECEydDZGnz77++usZOXIkK1eu5Omnny70FNmQf5rs4Cmy401fcnIyZ599Nq+//jqzZs0iowhmKPQjCJuBE4KOG3lu4bgEmBHGfRAwW1V/Czio6lZ17Aem4kxTCSWwluyGDaCat5asiYJhFJ4aNWrQo0cPrrrqqnyNyYGpnytXrszChQvZsGFD1HDOOOMMXnzxRQC++OILVqxYAbips6tXr06tWrXYtm0bc+fOzb3mqKOOYs+ePYeF1a1bN1577TX27t3LL7/8wuzZs+nWrZvvNO3atYuGDRsC8Nxzz+W6n3322UycODH3eOfOnXTp0oVFixbx3XffAfmnyF66dCkAS5cuzT0fSqT0nXzyyWzdupXFixcDsGfPnlzxGjZsGDfccAOnnHJK7mI8icSPICwGmotIUxGpgsv054R6EpEWQB3g4zBhDCZEKLxaA+IMixcAX8QX9dgU51qyhlEeGTx4MMuXL88nCBkZGWRmZtKmTRumTZsWc7GXa6+9lp9//pmWLVty11135dY00tLSaN++PS1atODSSy/NN3X2iBEj6N27d26jcoAOHTowdOhQOnXqROfOnRk2bBjt27f3nZ577rmHgQMH0rFjx3ztE3fccQc7d+6kdevWpKWlsXDhQho0aMCkSZPo378/aWlpudNWX3TRRfz444+0atWKJ554gpNOOinsvSKlr0qVKrz00ktcf/31pKWlcfbZZ+fWHDp27EjNmjWLbM0EX9Nfi0gfYDxQEZiiquNE5D4gU1XneH7uAZJUdUzItU2A/wEnqGpOkPsCoAEgwDLgj6r6c7R4xDv9dYUKrmZweHrcvPCGUVqx6a/LJ1u2bKF79+58+eWXVKgQvjxf5NNfq+pbwFshbneFHN8T4dr1uIbpUPeefu5dGBo3dmaicO6GYRiliWnTpjF27FgeeeSRiGJQWMr0SOXiXkvWMAyjqLj88svZtGkTAwcOLLJ7lGlBKMm1ZA2jqClNqx0axUNhv4kyv0BORoYJgFH2SEpKYseOHdSrVy/igC+jfKGq7Nixg6SkpAKHUeYFwTDKIo0aNSIrK4vs7OySjopxBJGUlESjRo0KfL0JgmGUQipXrpw7QtYwEkWZbkMwDMMw/GOCYBiGYQAmCIZhGIaHr5HKRwoikg1EnxglMvWB7QmMTmmhPKa7PKYZyme6Lc3+SFHVmLODlipBKAwikuln6HZZozymuzymGcpnui3NicVMRoZhGAZggmAYhmF4lCdBmFTSESghymO6y2OaoXym29KcQMpNG4JhGIYRnfJUQzAMwzCiYIJgGIZhAOVEEESkt4h8JSLrRGRM7CtKHyJygogsFJHVIrJKRG703OuKyLsistb7TfxCrCWMiFQUkc9F5E3vuKmIfOq975e8pV/LFCJSW0ReFpEvRWSNiJxa1t+1iIzyvu0vRGSGiCSVxXctIlNE5AcR+SLILey7FcfjXvpXiEiHwty7zAuCiFQEJgLnAqnAYBFJLdlYFQkHgdGqmgp0Aa7z0jkGmK+qzYH53nFZ40ZgTdDxg8CjqnoisBO4ukRiVbQ8Brytqi2ANFz6y+y7FpGGwA1Auqq2xi3newll810/C/QOcYv0bs8FmnvbCOCpwty4zAsC0AlYp6rfquoBYCbQr4TjlHBUdauqLvX29+AyiIa4tD7neXsOuKBkYlg0iEgj4A/AZO9YgJ7Ay56XspjmWsAZwL8AVPWAqv5EGX/XuNmZq4lIJSAZ2EoZfNequgj4McQ50rvtB0xTxydAbRE5rqD3Lg+C0BDYFHScRZg1nssSItIEaA98Chyjqlu9U98Dx5RQtIqK8cCtQI53XA/4SVUPesdl8X03BbKBqZ6pbLKIVKcMv2tV3Qw8BGzECcEuYAll/10HiPRuE5q/lQdBKFeISA3gFeAmVd0dfE5dH+My089YRM4DflDVJSUdl2KmEtABeEpV2wO/EGIeKoPvug6uNNwUOB6ozuFmlXJBUb7b8iAIm4ETgo4beW5lDhGpjBOD6ar6que8LVCF9H5/KKn4FQFdgb4ish5nCuyJs63X9swKUDbfdxaQpaqfescv4wSiLL/rs4DvVDVbVX8DXsW9/7L+rgNEercJzd/KgyAsBpp7vRGq4Bqi5pRwnBKOZzv/F7BGVR8JOjUHuMLbvwJ4vbjjVlSo6m2q2khVm+De6wJVzQAWAgM8b2UqzQCq+j2wSURO9px6Aaspw+8aZyrqIiLJ3rceSHOZftdBRHq3c4DLvd5GXYBdQaal+FHVMr8BfYCvgW+AsSUdnyJK4+m4auQKYJm39cHZ1OcDa4F5QN2SjmsRpb878Ka33wz4DFgH/BuoWtLxK4L0tgMyvff9GlCnrL9r4F7gS+AL4Hmgall818AMXDvJb7ja4NWR3i0guF6U3wArcb2wCnxvm7rCMAzDAMqHycgwDMPwgQmCYRiGAZggGIZhGB4mCIZhGAZggmAYhmF4mCAYhmEYgAmCYRiG4fH/O3vN9IOKMIgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot The Training Accuracy\n",
    "\n",
    "check = model_train7\n",
    "accuracy = check.history['acc']\n",
    "val_accuracy = check.history['val_acc']\n",
    "loss = check.history['loss']\n",
    "val_loss = check.history['val_loss']\n",
    "epochs = range(len(accuracy))\n",
    "plt.plot(epochs, accuracy, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PELM_s Result\n",
      "Accuracy : 0.948220064724919\n",
      "AUC : 0.9466341709387345\n",
      "Sensitivity : 0.9754601226993865\n",
      "Specificity : 0.9178082191780822\n",
      "F1 : 0.9520958083832336\n",
      "MCC : 0.8970457113243496\n",
      "\n",
      "PELM_s Result\n",
      "Accuracy : 0.9129032258064517\n",
      "AUC : 0.9125582944703531\n",
      "Sensitivity : 0.930379746835443\n",
      "Specificity : 0.8947368421052632\n",
      "F1 : 0.9158878504672897\n",
      "MCC : 0.8260630310790035\n",
      "\n",
      "PELM_s Result\n",
      "Accuracy : 0.9193548387096774\n",
      "AUC : 0.9198842100878837\n",
      "Sensitivity : 0.89937106918239\n",
      "Specificity : 0.9403973509933775\n",
      "F1 : 0.9196141479099678\n",
      "MCC : 0.8396460266183494\n",
      "\n",
      "PELM_s Result\n",
      "Accuracy : 0.919093851132686\n",
      "AUC : 0.9187405731523379\n",
      "Sensitivity : 0.9551282051282052\n",
      "Specificity : 0.8823529411764706\n",
      "F1 : 0.9226006191950465\n",
      "MCC : 0.840196069677529\n",
      "\n",
      "PELM_s Result\n",
      "Accuracy : 0.9129032258064517\n",
      "AUC : 0.911484664644422\n",
      "Sensitivity : 0.8985507246376812\n",
      "Specificity : 0.9244186046511628\n",
      "F1 : 0.9018181818181817\n",
      "MCC : 0.8235767572551157\n",
      "\n",
      "PELM_s Result\n",
      "Accuracy : 0.9096774193548387\n",
      "AUC : 0.9094125973106865\n",
      "Sensitivity : 0.8888888888888888\n",
      "Specificity : 0.9299363057324841\n",
      "F1 : 0.9066666666666665\n",
      "MCC : 0.8198497510714674\n",
      "\n",
      "PELM_s Result\n",
      "Accuracy : 0.9029126213592233\n",
      "AUC : 0.902806870548806\n",
      "Sensitivity : 0.9354838709677419\n",
      "Specificity : 0.8701298701298701\n",
      "F1 : 0.90625\n",
      "MCC : 0.807476434315085\n",
      "\n",
      "PELM_s Result\n",
      "Accuracy : 0.9096774193548387\n",
      "AUC : 0.9091439040639573\n",
      "Sensitivity : 0.9367088607594937\n",
      "Specificity : 0.881578947368421\n",
      "F1 : 0.9135802469135801\n",
      "MCC : 0.8202025741277765\n",
      "\n",
      "PELM_s Result\n",
      "Accuracy : 0.932258064516129\n",
      "AUC : 0.9321702198534311\n",
      "Sensitivity : 0.9367088607594937\n",
      "Specificity : 0.9276315789473685\n",
      "F1 : 0.9337539432176657\n",
      "MCC : 0.8644664329205286\n",
      "\n",
      "PELM_s Result\n",
      "Accuracy : 0.9225806451612903\n",
      "AUC : 0.9224109224109224\n",
      "Sensitivity : 0.9487179487179487\n",
      "Specificity : 0.8961038961038961\n",
      "F1 : 0.9249999999999999\n",
      "MCC : 0.8462319974387332\n",
      "\n",
      "PELM_s Result\n",
      "Accuracy : 0.9189581375926504\n",
      "AUC : 0.9185246427481534\n",
      "Sensitivity : 0.9305398298576673\n",
      "Specificity : 0.9065094556386395\n",
      "F1 : 0.9197267464571631\n",
      "MCC : 0.8384754785827939\n"
     ]
    }
   ],
   "source": [
    "# Model Score Summary using Cross Validation\n",
    "\n",
    "model.load_weights(\"weight_best1.hdf5\")\n",
    "y_pred1 = np.argmax(model.predict(valid_X1), axis=1)\n",
    "y_true1 = np.argmax(valid_Y1, axis = 1)\n",
    "\n",
    "model.load_weights(\"weight_best2.hdf5\")\n",
    "y_pred2 = np.argmax(model.predict(valid_X2), axis=1)\n",
    "y_true2 = np.argmax(valid_Y2, axis = 1)\n",
    "\n",
    "model.load_weights(\"weight_best3.hdf5\")\n",
    "y_pred3 = np.argmax(model.predict(valid_X3), axis=1)\n",
    "y_true3 = np.argmax(valid_Y3, axis = 1)\n",
    "\n",
    "model.load_weights(\"weight_best4.hdf5\")\n",
    "y_pred4 = np.argmax(model.predict(valid_X4), axis=1)\n",
    "y_true4 = np.argmax(valid_Y4, axis = 1)\n",
    "\n",
    "model.load_weights(\"weight_best5.hdf5\")\n",
    "y_pred5 = np.argmax(model.predict(valid_X5), axis=1)\n",
    "y_true5 = np.argmax(valid_Y5, axis = 1)\n",
    "\n",
    "model.load_weights(\"weight_best6.hdf5\")\n",
    "y_pred6 = np.argmax(model.predict(valid_X6), axis=1)\n",
    "y_true6 = np.argmax(valid_Y6, axis = 1)\n",
    "\n",
    "model.load_weights(\"weight_best7.hdf5\")\n",
    "y_pred7 = np.argmax(model.predict(valid_X7), axis=1)\n",
    "y_true7 = np.argmax(valid_Y7, axis = 1)\n",
    "\n",
    "model.load_weights(\"weight_best8.hdf5\")\n",
    "y_pred8 = np.argmax(model.predict(valid_X8), axis=1)\n",
    "y_true8 = np.argmax(valid_Y8, axis = 1)\n",
    "\n",
    "model.load_weights(\"weight_best9.hdf5\")\n",
    "y_pred9 = np.argmax(model.predict(valid_X9), axis=1)\n",
    "y_true9 = np.argmax(valid_Y9, axis = 1)\n",
    "\n",
    "model.load_weights(\"weight_best10.hdf5\")\n",
    "y_pred10 = np.argmax(model.predict(valid_X10), axis=1)\n",
    "y_true10 = np.argmax(valid_Y10, axis = 1)\n",
    "\n",
    "def conf_matrix(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    f1 = f1_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
    "    auc = roc_auc_score(y_true, y_pred, average='macro', sample_weight=None, max_fpr=None)\n",
    "    sensi = tp/(tp+fn)\n",
    "    specificity = tn/(tn+fp)\n",
    "    accu = (tn + tp)/(tn + tp + fn + fp)\n",
    "    mcc = ((tp*tn)-(fp*fn))/np.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n",
    "    print('{} Result'.format(string_name))\n",
    "    print('Accuracy :', accu)\n",
    "    print('AUC :', auc)\n",
    "    print('Sensitivity :', sensi)\n",
    "    print('Specificity :', specificity)\n",
    "    print('F1 :', f1)\n",
    "    print('MCC :', mcc)\n",
    "    print()\n",
    "    return [auc, accu, mcc, f1, sensi, specificity]\n",
    "\n",
    "auc1, accu1, mcc1, f11, sen1, spec1 = conf_matrix(y_true1, y_pred1)\n",
    "auc2, accu2, mcc2, f12, sen2, spec2 = conf_matrix(y_true2, y_pred2)\n",
    "auc3, accu3, mcc3, f13, sen3, spec3 = conf_matrix(y_true3, y_pred3)\n",
    "auc4, accu4, mcc4, f14, sen4, spec4 = conf_matrix(y_true4, y_pred4)\n",
    "auc5, accu5, mcc5, f15, sen5, spec5 = conf_matrix(y_true5, y_pred5)\n",
    "auc6, accu6, mcc6, f16, sen6, spec6 = conf_matrix(y_true6, y_pred6)\n",
    "auc7, accu7, mcc7, f17, sen7, spec7 = conf_matrix(y_true7, y_pred7)\n",
    "auc8, accu8, mcc8, f18, sen8, spec8 = conf_matrix(y_true8, y_pred8)\n",
    "auc9, accu9, mcc9, f19, sen9, spec9 = conf_matrix(y_true9, y_pred9)\n",
    "auc10, accu10, mcc10, f110, sen10, spec10 = conf_matrix(y_true10, y_pred10)\n",
    "\n",
    "accu = (accu1 + accu2 + accu3 + accu4 + accu5 + accu6 + accu7 + accu8 + accu9 + accu10)/10\n",
    "auc = (auc1 + auc2 + auc3 + auc4 + auc5 + auc6 + auc7 + auc8 + auc9 + auc10)/10\n",
    "f1 = (f11 + f12 + f13 + f14 + f15 + f16 + f17 + f18 + f19 + f110)/10\n",
    "mcc = (mcc1 + mcc2 + mcc3 + mcc4 + mcc5 + mcc6 + mcc7 + mcc8 + mcc9 + mcc10)/10\n",
    "sen = (sen1 + sen2 + sen3 + sen4 + sen5 + sen6 + sen7 + sen8 + sen9 + sen10)/10\n",
    "spec = (spec1 + spec2 + spec3 + spec4 + spec5 + spec6 + spec7 + spec8 + spec9 + spec10)/10\n",
    "\n",
    "print('{} Result'.format(string_name))\n",
    "print('Accuracy :', accu)\n",
    "print('AUC :', auc)\n",
    "print('Sensitivity :', sen)\n",
    "print('Specificity :', spec)\n",
    "print('F1 :', f1)\n",
    "print('MCC :', mcc)\n",
    "    \n",
    "with open('results/summary_{}.csv'.format(string_name), mode='w') as summary_file:\n",
    "    employee_writer = csv.writer(summary_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    employee_writer.writerow(['Accuracy :', accu])\n",
    "    employee_writer.writerow(['AUC :', auc])\n",
    "    employee_writer.writerow(['Sensitivity :', sen])\n",
    "    employee_writer.writerow(['Specificity :', spec])\n",
    "    employee_writer.writerow(['F1 :', f1])\n",
    "    employee_writer.writerow(['MCC :', mcc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Testing\n",
    "\n",
    "# print('Model Final')\n",
    "# model_train = model.fit(main_X, main_Y, epochs=100, batch_size=32, \n",
    "#                         validation_data=(test_X, test_Y), callbacks=callback_list)\n",
    "# model.load_weights(\"model.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot The Training Accuracy\n",
    "\n",
    "# accuracy = model_train.history['acc']\n",
    "# val_accuracy = model_train.history['val_acc']\n",
    "# loss = model_train.history['loss']\n",
    "# val_loss = model_train.history['val_loss']\n",
    "# epochs = range(len(accuracy))\n",
    "# plt.plot(epochs, accuracy, 'bo', label='Training accuracy')\n",
    "# plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
    "# plt.title('Training and validation accuracy')\n",
    "# plt.legend()\n",
    "# plt.savefig('results/Acc_result_{}.png'.format(string_name))\n",
    "# plt.figure()\n",
    "# plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "# plt.title('Training and validation loss')\n",
    "# plt.legend()\n",
    "# plt.savefig('results/Loss_result_{}.png'.format(string_name))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights(\"weight_best.hdf5\")\n",
    "# y_pred_t = np.argmax(model.predict(test_X), axis=1)\n",
    "# y_true_t = np.argmax(test_Y, axis = 1)\n",
    "\n",
    "# def conf_matrix(y_true, y_pred):\n",
    "#     tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "#     f1 = f1_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
    "#     auc = roc_auc_score(y_true, y_pred, average='macro', sample_weight=None, max_fpr=None)\n",
    "#     sensi = tp/(tp+fn)\n",
    "#     specificity = tn/(tn+fp)\n",
    "#     accu = (tn + tp)/(tn + tp + fn + fp)\n",
    "#     mcc = ((tp*tn)-(fp*fn))/np.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n",
    "#     print('{} Result'.format(string_name))\n",
    "#     print('Accuracy :', accu)\n",
    "#     print('AUC :', auc)\n",
    "#     print('Sensitivity :', sensi)\n",
    "#     print('Specificity :', specificity)\n",
    "#     print('F1 :', f1)\n",
    "#     print('MCC :', mcc)\n",
    "#     print()\n",
    "#     return [auc, accu, mcc, f1, sensi, specificity]\n",
    "\n",
    "# auc_t, accu_t, mcc_t, f1_t, sen_t, spec_t = conf_matrix(y_true_t, y_pred_t)\n",
    "    \n",
    "# with open('results/final_summary_{}.csv'.format(string_name), mode='w') as summary_file:\n",
    "#     employee_writer = csv.writer(summary_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "#     employee_writer.writerow(['Accuracy :', accu_t])\n",
    "#     employee_writer.writerow(['AUC :', auc_t])\n",
    "#     employee_writer.writerow(['Sensitivity :', sen_t])\n",
    "#     employee_writer.writerow(['Specificity :', spec_t])\n",
    "#     employee_writer.writerow(['F1 :', f1_t])\n",
    "#     employee_writer.writerow(['MCC :', mcc_t])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
